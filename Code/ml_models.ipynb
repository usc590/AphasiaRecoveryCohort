{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T15:11:10.402671Z",
     "start_time": "2024-10-23T15:10:30.555079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv  # For saving hyperparameters, predictions, and metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(features_file, labels_file, labels_column):\n",
    "    features_df = pd.read_csv(features_file, index_col=0)\n",
    "    labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "\n",
    "    X = features_df\n",
    "    y = labels_df[labels_column]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y\n",
    "\n",
    "# Save the best hyperparameters to a CSV file\n",
    "def save_hyperparameters(model_name, best_params):\n",
    "    filename = f\"{model_name}_best_params.csv\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Parameter\", \"Value\"])\n",
    "        # Assuming best_params is a list of dictionaries, one for each fold\n",
    "        for params in best_params:\n",
    "            for param, value in params.items():\n",
    "                writer.writerow([param, value])\n",
    "    print(f\"Best hyperparameters for {model_name} saved to {filename}\")\n",
    "\n",
    "# Save the predictions, true labels, and metrics to a CSV file\n",
    "def save_predictions_and_metrics(model_name, y_true, y_pred, corr, rmse):\n",
    "    # Save predictions and true labels\n",
    "    predictions_filename = f\"../Results/{model_name}_predictions.csv\"\n",
    "    with open(predictions_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"True Label\", \"Predicted Label\"])\n",
    "        writer.writerows(zip(y_true, y_pred))\n",
    "    \n",
    "    # Save correlation and RMSE metrics\n",
    "    metrics_filename = f\"../Results/{model_name}_metrics.csv\"\n",
    "    with open(metrics_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Metric\", \"Value\"])\n",
    "        writer.writerow([\"Correlation\", corr])\n",
    "        writer.writerow([\"RMSE\", rmse])\n",
    "    \n",
    "    print(f\"Predictions for {model_name} saved to {predictions_filename}\")\n",
    "    print(f\"Metrics for {model_name} saved to {metrics_filename}\")\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation on a single model\n",
    "def run_loo_cv(model, param_grid, model_name, X, y):\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred, best_params = [], [], []\n",
    "\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        y_true.append(y_test.values[0])\n",
    "        \n",
    "        if param_grid:\n",
    "            # Add n_jobs=-1 for parallel processing during Grid Search\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            pred = best_model.predict(X_test)\n",
    "            best_params.append(grid_search.best_params_)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "        \n",
    "        y_pred.append(pred[0])\n",
    "    \n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    print(f\"{model_name}: Correlation (R) = {corr:.4f}, RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    # Save hyperparameters, predictions, and metrics\n",
    "    if param_grid:\n",
    "        print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "        save_hyperparameters(model_name, best_params)\n",
    "    \n",
    "    save_predictions_and_metrics(model_name, y_true, y_pred, corr, rmse)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Generic training and evaluation function\n",
    "def train_and_evaluate(model_class, X, y, num_epochs=200, learning_rate=0.001):\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Determine the number of input features\n",
    "    input_size = X.shape[1]\n",
    "    \n",
    "    # Initialize Leave-One-Out cross-validator\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        # Split the data into training and test for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Convert to PyTorch tensors and move to GPU if available\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Initialize the model, loss function, and optimizer\n",
    "        model = model_class(input_size=input_size).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on the left-out test sample\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(X_test_tensor).cpu().numpy()[0][0]  # Get prediction and move back to CPU\n",
    "\n",
    "        # Store the true value and predicted value\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "    # Calculate RMSE and Pearson correlation coefficient (R-value)\n",
    "    test_rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    corr = pearsonr(y_true, y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"\\nModel: {model_class.__name__} | Test RMSE: {test_rmse:.4f} | Test R value (correlation): {corr[0]:.4f}\")\n",
    "\n",
    "    # Save predictions and metrics using the existing function\n",
    "    model_name = model_class.__name__\n",
    "    save_predictions_and_metrics(model_name, y_true, y_pred, corr[0], test_rmse)\n",
    "\n",
    "# Choose and run individual models\n",
    "def run_linear_regression(X, y):\n",
    "    model_name = 'Linear_Regression'\n",
    "    model = LinearRegression()\n",
    "    best_params = run_loo_cv(model, None, model_name, X, y)\n",
    "\n",
    "def run_ridge(X, y):\n",
    "    model_name = 'Ridge'\n",
    "    model = Ridge()\n",
    "    best_params = run_loo_cv(model, param_grids[model_name], model_name, X, y)\n",
    "\n",
    "def run_lasso(X, y):\n",
    "    model_name = 'Lasso'\n",
    "    model = Lasso()\n",
    "    best_params = run_loo_cv(model, param_grids[model_name], model_name, X, y)\n",
    "\n",
    "def run_svr(X, y):\n",
    "    model_name = 'SVR'\n",
    "    model = SVR()\n",
    "    best_params = run_loo_cv(model, param_grids[model_name], model_name, X, y)\n",
    "\n",
    "def run_random_forest(X, y):\n",
    "    model_name = 'Random Forest'\n",
    "    model = RandomForestRegressor(n_jobs=-1)  # Enable parallel processing for RandomForest\n",
    "    best_params = run_loo_cv(model, param_grids[model_name], model_name, X, y)\n",
    "\n",
    "def run_xgboost(X, y):\n",
    "    model_name = 'XGBoost'\n",
    "    model = XGBRegressor(use_label_encoder=False, eval_metric='rmse', n_jobs=-1)  # Enable parallel processing for XGBoost\n",
    "    best_params = run_loo_cv(model, param_grids[model_name], model_name, X, y)\n",
    "\n",
    "# Define parameter grids for models\n",
    "param_grids = {\n",
    "    'Ridge': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n",
    "    'Lasso': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n",
    "    'SVR': {\n",
    "        # 'C': [0.1, 1.0, 10.0, 100.0],\n",
    "        # 'epsilon': [0.001, 0.01, 0.1, 1.0],\n",
    "        # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        # 'n_estimators': [100, 200],\n",
    "        # 'max_depth': [None, 10, 20],\n",
    "        # 'min_samples_split': [2, 5],\n",
    "        # 'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "class NeuralNetMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DeepNeuralNetMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeepNeuralNetMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Calculate the flattened size after convolution and pooling\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "\n",
    "    def _get_flattened_size(self, input_size):\n",
    "        # Create a dummy tensor with the same size as the input to calculate the output size after convolutions\n",
    "        dummy_input = torch.zeros(1, 1, input_size)  # Batch size of 1, 1 channel, input_size as width\n",
    "        output = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        output = self.pool(self.relu(self.conv2(output)))\n",
    "        flattened_size = output.numel()  # Calculate the number of elements in the output tensor\n",
    "        return flattened_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == '__main__':\n",
    "    features_file = '../Processed Data/rest_101_participants_40_regions.csv'\n",
    "    labels_file = '../Processed Data/101_participants_40_regions_target_variable.csv'\n",
    "    prediction_label = 'Aphasia quotient'\n",
    "    \n",
    "    X, y = load_data(features_file, labels_file, prediction_label)\n",
    "    \n",
    "    # Choose a model to run (uncomment the model you want to run)\n",
    "    #run_linear_regression(X, y)\n",
    "    # run_ridge(X, y)\n",
    "    # run_lasso(X, y)\n",
    "    #run_svr(X, y)\n",
    "    #run_random_forest(X, y)\n",
    "    # run_xgboost(X, y)\n",
    "    #train_and_evaluate(NeuralNetMLP, X, y)\n",
    "    #train_and_evaluate(DeepNeuralNetMLP, X, y)\n",
    "    train_and_evaluate(CNN, X, y)\n"
   ],
   "id": "48eb133d541e0321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [10/200], Loss: 1512.8531\n",
      "Epoch [20/200], Loss: 1060.5099\n",
      "Epoch [30/200], Loss: 786.2903\n",
      "Epoch [40/200], Loss: 648.0015\n",
      "Epoch [50/200], Loss: 612.6757\n",
      "Epoch [60/200], Loss: 575.7255\n",
      "Epoch [70/200], Loss: 533.0200\n",
      "Epoch [80/200], Loss: 482.3949\n",
      "Epoch [90/200], Loss: 426.4696\n",
      "Epoch [100/200], Loss: 366.3800\n",
      "Epoch [110/200], Loss: 305.9165\n",
      "Epoch [120/200], Loss: 253.4401\n",
      "Epoch [130/200], Loss: 212.5565\n",
      "Epoch [140/200], Loss: 176.9109\n",
      "Epoch [150/200], Loss: 144.1169\n",
      "Epoch [160/200], Loss: 115.8860\n",
      "Epoch [170/200], Loss: 92.2720\n",
      "Epoch [180/200], Loss: 73.1074\n",
      "Epoch [190/200], Loss: 58.1461\n",
      "Epoch [200/200], Loss: 46.9550\n",
      "Epoch [10/200], Loss: 700.6831\n",
      "Epoch [20/200], Loss: 657.5849\n",
      "Epoch [30/200], Loss: 684.2012\n",
      "Epoch [40/200], Loss: 608.9495\n",
      "Epoch [50/200], Loss: 543.2822\n",
      "Epoch [60/200], Loss: 488.8678\n",
      "Epoch [70/200], Loss: 433.9113\n",
      "Epoch [80/200], Loss: 371.4339\n",
      "Epoch [90/200], Loss: 308.7489\n",
      "Epoch [100/200], Loss: 250.1938\n",
      "Epoch [110/200], Loss: 201.5726\n",
      "Epoch [120/200], Loss: 161.0708\n",
      "Epoch [130/200], Loss: 124.2164\n",
      "Epoch [140/200], Loss: 92.7682\n",
      "Epoch [150/200], Loss: 68.0581\n",
      "Epoch [160/200], Loss: 50.2838\n",
      "Epoch [170/200], Loss: 38.4466\n",
      "Epoch [180/200], Loss: 31.2108\n",
      "Epoch [190/200], Loss: 27.4067\n",
      "Epoch [200/200], Loss: 25.7155\n",
      "Epoch [10/200], Loss: 814.1742\n",
      "Epoch [20/200], Loss: 745.3431\n",
      "Epoch [30/200], Loss: 748.5807\n",
      "Epoch [40/200], Loss: 640.9523\n",
      "Epoch [50/200], Loss: 564.5956\n",
      "Epoch [60/200], Loss: 505.4162\n",
      "Epoch [70/200], Loss: 442.2047\n",
      "Epoch [80/200], Loss: 360.8971\n",
      "Epoch [90/200], Loss: 279.4972\n",
      "Epoch [100/200], Loss: 220.4107\n",
      "Epoch [110/200], Loss: 174.5774\n",
      "Epoch [120/200], Loss: 131.8022\n",
      "Epoch [130/200], Loss: 96.4728\n",
      "Epoch [140/200], Loss: 67.7851\n",
      "Epoch [150/200], Loss: 47.3117\n",
      "Epoch [160/200], Loss: 34.8118\n",
      "Epoch [170/200], Loss: 28.5017\n",
      "Epoch [180/200], Loss: 25.9754\n",
      "Epoch [190/200], Loss: 25.1249\n",
      "Epoch [200/200], Loss: 24.7810\n",
      "Epoch [10/200], Loss: 1598.9495\n",
      "Epoch [20/200], Loss: 1014.4353\n",
      "Epoch [30/200], Loss: 750.5006\n",
      "Epoch [40/200], Loss: 611.8220\n",
      "Epoch [50/200], Loss: 576.8002\n",
      "Epoch [60/200], Loss: 538.3101\n",
      "Epoch [70/200], Loss: 487.0918\n",
      "Epoch [80/200], Loss: 423.1225\n",
      "Epoch [90/200], Loss: 346.7082\n",
      "Epoch [100/200], Loss: 273.2175\n",
      "Epoch [110/200], Loss: 218.7933\n",
      "Epoch [120/200], Loss: 174.8993\n",
      "Epoch [130/200], Loss: 130.8442\n",
      "Epoch [140/200], Loss: 92.0057\n",
      "Epoch [150/200], Loss: 61.8915\n",
      "Epoch [160/200], Loss: 42.1298\n",
      "Epoch [170/200], Loss: 31.5868\n",
      "Epoch [180/200], Loss: 27.1276\n",
      "Epoch [190/200], Loss: 25.5005\n",
      "Epoch [200/200], Loss: 24.9009\n",
      "Epoch [10/200], Loss: 900.5723\n",
      "Epoch [20/200], Loss: 769.1363\n",
      "Epoch [30/200], Loss: 756.8176\n",
      "Epoch [40/200], Loss: 637.1754\n",
      "Epoch [50/200], Loss: 567.6512\n",
      "Epoch [60/200], Loss: 501.5113\n",
      "Epoch [70/200], Loss: 426.1135\n",
      "Epoch [80/200], Loss: 340.6939\n",
      "Epoch [90/200], Loss: 263.4715\n",
      "Epoch [100/200], Loss: 209.4786\n",
      "Epoch [110/200], Loss: 167.5343\n",
      "Epoch [120/200], Loss: 125.6498\n",
      "Epoch [130/200], Loss: 89.7833\n",
      "Epoch [140/200], Loss: 62.3886\n",
      "Epoch [150/200], Loss: 43.7866\n",
      "Epoch [160/200], Loss: 33.0883\n",
      "Epoch [170/200], Loss: 28.1321\n",
      "Epoch [180/200], Loss: 26.0342\n",
      "Epoch [190/200], Loss: 25.1266\n",
      "Epoch [200/200], Loss: 24.7643\n",
      "Epoch [10/200], Loss: 1003.3722\n",
      "Epoch [20/200], Loss: 786.3134\n",
      "Epoch [30/200], Loss: 761.2387\n",
      "Epoch [40/200], Loss: 637.4433\n",
      "Epoch [50/200], Loss: 567.4454\n",
      "Epoch [60/200], Loss: 505.7313\n",
      "Epoch [70/200], Loss: 436.5659\n",
      "Epoch [80/200], Loss: 354.2388\n",
      "Epoch [90/200], Loss: 274.4482\n",
      "Epoch [100/200], Loss: 213.5416\n",
      "Epoch [110/200], Loss: 172.5699\n",
      "Epoch [120/200], Loss: 136.4600\n",
      "Epoch [130/200], Loss: 103.6350\n",
      "Epoch [140/200], Loss: 76.6169\n",
      "Epoch [150/200], Loss: 55.8932\n",
      "Epoch [160/200], Loss: 41.7809\n",
      "Epoch [170/200], Loss: 33.1785\n",
      "Epoch [180/200], Loss: 28.4462\n",
      "Epoch [190/200], Loss: 26.1156\n",
      "Epoch [200/200], Loss: 25.1067\n",
      "Epoch [10/200], Loss: 1145.9219\n",
      "Epoch [20/200], Loss: 962.4901\n",
      "Epoch [30/200], Loss: 879.2554\n",
      "Epoch [40/200], Loss: 722.0780\n",
      "Epoch [50/200], Loss: 622.7494\n",
      "Epoch [60/200], Loss: 546.2247\n",
      "Epoch [70/200], Loss: 470.6913\n",
      "Epoch [80/200], Loss: 391.5571\n",
      "Epoch [90/200], Loss: 318.8026\n",
      "Epoch [100/200], Loss: 262.6028\n",
      "Epoch [110/200], Loss: 221.1305\n",
      "Epoch [120/200], Loss: 184.1241\n",
      "Epoch [130/200], Loss: 148.7521\n",
      "Epoch [140/200], Loss: 116.2390\n",
      "Epoch [150/200], Loss: 87.6470\n",
      "Epoch [160/200], Loss: 64.5254\n",
      "Epoch [170/200], Loss: 47.8627\n",
      "Epoch [180/200], Loss: 37.2495\n",
      "Epoch [190/200], Loss: 31.1121\n",
      "Epoch [200/200], Loss: 27.7699\n",
      "Epoch [10/200], Loss: 1439.9495\n",
      "Epoch [20/200], Loss: 1015.7861\n",
      "Epoch [30/200], Loss: 810.5420\n",
      "Epoch [40/200], Loss: 642.5980\n",
      "Epoch [50/200], Loss: 578.9409\n",
      "Epoch [60/200], Loss: 527.6912\n",
      "Epoch [70/200], Loss: 477.7333\n",
      "Epoch [80/200], Loss: 428.5896\n",
      "Epoch [90/200], Loss: 378.8658\n",
      "Epoch [100/200], Loss: 327.8509\n",
      "Epoch [110/200], Loss: 278.5546\n",
      "Epoch [120/200], Loss: 235.6228\n",
      "Epoch [130/200], Loss: 200.5216\n",
      "Epoch [140/200], Loss: 169.8596\n",
      "Epoch [150/200], Loss: 141.7032\n",
      "Epoch [160/200], Loss: 115.4635\n",
      "Epoch [170/200], Loss: 91.7111\n",
      "Epoch [180/200], Loss: 71.7554\n",
      "Epoch [190/200], Loss: 55.8620\n",
      "Epoch [200/200], Loss: 43.4259\n",
      "Epoch [10/200], Loss: 850.8785\n",
      "Epoch [20/200], Loss: 744.9948\n",
      "Epoch [30/200], Loss: 747.0823\n",
      "Epoch [40/200], Loss: 637.8552\n",
      "Epoch [50/200], Loss: 556.3896\n",
      "Epoch [60/200], Loss: 499.2376\n",
      "Epoch [70/200], Loss: 445.5317\n",
      "Epoch [80/200], Loss: 387.5620\n",
      "Epoch [90/200], Loss: 324.2675\n",
      "Epoch [100/200], Loss: 263.4238\n",
      "Epoch [110/200], Loss: 211.1577\n",
      "Epoch [120/200], Loss: 166.2191\n",
      "Epoch [130/200], Loss: 126.2891\n",
      "Epoch [140/200], Loss: 93.2093\n",
      "Epoch [150/200], Loss: 67.9735\n",
      "Epoch [160/200], Loss: 49.6978\n",
      "Epoch [170/200], Loss: 37.4838\n",
      "Epoch [180/200], Loss: 30.2610\n",
      "Epoch [190/200], Loss: 26.8048\n",
      "Epoch [200/200], Loss: 25.4139\n",
      "Epoch [10/200], Loss: 1416.5348\n",
      "Epoch [20/200], Loss: 1000.9761\n",
      "Epoch [30/200], Loss: 721.9342\n",
      "Epoch [40/200], Loss: 590.3282\n",
      "Epoch [50/200], Loss: 566.3945\n",
      "Epoch [60/200], Loss: 539.7886\n",
      "Epoch [70/200], Loss: 507.8697\n",
      "Epoch [80/200], Loss: 475.0710\n",
      "Epoch [90/200], Loss: 440.9612\n",
      "Epoch [100/200], Loss: 406.0913\n",
      "Epoch [110/200], Loss: 370.8012\n",
      "Epoch [120/200], Loss: 335.9287\n",
      "Epoch [130/200], Loss: 300.8178\n",
      "Epoch [140/200], Loss: 263.8206\n",
      "Epoch [150/200], Loss: 229.2027\n",
      "Epoch [160/200], Loss: 197.8417\n",
      "Epoch [170/200], Loss: 166.1783\n",
      "Epoch [180/200], Loss: 135.8956\n",
      "Epoch [190/200], Loss: 108.2675\n",
      "Epoch [200/200], Loss: 83.6546\n",
      "Epoch [10/200], Loss: 994.3433\n",
      "Epoch [20/200], Loss: 834.2838\n",
      "Epoch [30/200], Loss: 810.0739\n",
      "Epoch [40/200], Loss: 677.6000\n",
      "Epoch [50/200], Loss: 589.8044\n",
      "Epoch [60/200], Loss: 511.0005\n",
      "Epoch [70/200], Loss: 428.3814\n",
      "Epoch [80/200], Loss: 347.4933\n",
      "Epoch [90/200], Loss: 277.1545\n",
      "Epoch [100/200], Loss: 229.6800\n",
      "Epoch [110/200], Loss: 191.7976\n",
      "Epoch [120/200], Loss: 154.0463\n",
      "Epoch [130/200], Loss: 120.6217\n",
      "Epoch [140/200], Loss: 92.2469\n",
      "Epoch [150/200], Loss: 69.3568\n",
      "Epoch [160/200], Loss: 52.2019\n",
      "Epoch [170/200], Loss: 40.5490\n",
      "Epoch [180/200], Loss: 33.3666\n",
      "Epoch [190/200], Loss: 29.2535\n",
      "Epoch [200/200], Loss: 26.9989\n",
      "Epoch [10/200], Loss: 1168.7745\n",
      "Epoch [20/200], Loss: 954.0437\n",
      "Epoch [30/200], Loss: 870.1613\n",
      "Epoch [40/200], Loss: 722.1884\n",
      "Epoch [50/200], Loss: 634.0076\n",
      "Epoch [60/200], Loss: 567.4415\n",
      "Epoch [70/200], Loss: 504.9515\n",
      "Epoch [80/200], Loss: 435.5580\n",
      "Epoch [90/200], Loss: 351.5490\n",
      "Epoch [100/200], Loss: 265.7904\n",
      "Epoch [110/200], Loss: 201.6327\n",
      "Epoch [120/200], Loss: 153.0744\n",
      "Epoch [130/200], Loss: 106.1143\n",
      "Epoch [140/200], Loss: 69.8979\n",
      "Epoch [150/200], Loss: 45.1112\n",
      "Epoch [160/200], Loss: 31.8835\n",
      "Epoch [170/200], Loss: 26.9608\n",
      "Epoch [180/200], Loss: 25.8145\n",
      "Epoch [190/200], Loss: 25.2057\n",
      "Epoch [200/200], Loss: 24.8769\n",
      "Epoch [10/200], Loss: 740.2817\n",
      "Epoch [20/200], Loss: 641.0657\n",
      "Epoch [30/200], Loss: 657.7087\n",
      "Epoch [40/200], Loss: 585.4702\n",
      "Epoch [50/200], Loss: 520.6015\n",
      "Epoch [60/200], Loss: 461.6745\n",
      "Epoch [70/200], Loss: 402.9258\n",
      "Epoch [80/200], Loss: 340.3971\n",
      "Epoch [90/200], Loss: 283.3839\n",
      "Epoch [100/200], Loss: 236.4235\n",
      "Epoch [110/200], Loss: 198.3944\n",
      "Epoch [120/200], Loss: 165.5125\n",
      "Epoch [130/200], Loss: 136.3388\n",
      "Epoch [140/200], Loss: 108.8632\n",
      "Epoch [150/200], Loss: 83.4336\n",
      "Epoch [160/200], Loss: 62.2384\n",
      "Epoch [170/200], Loss: 46.5742\n",
      "Epoch [180/200], Loss: 36.5688\n",
      "Epoch [190/200], Loss: 30.8369\n",
      "Epoch [200/200], Loss: 27.6489\n",
      "Epoch [10/200], Loss: 1584.2910\n",
      "Epoch [20/200], Loss: 1096.0094\n",
      "Epoch [30/200], Loss: 840.6402\n",
      "Epoch [40/200], Loss: 663.5214\n",
      "Epoch [50/200], Loss: 578.2615\n",
      "Epoch [60/200], Loss: 505.4284\n",
      "Epoch [70/200], Loss: 433.2682\n",
      "Epoch [80/200], Loss: 363.1046\n",
      "Epoch [90/200], Loss: 304.2441\n",
      "Epoch [100/200], Loss: 262.0342\n",
      "Epoch [110/200], Loss: 228.2988\n",
      "Epoch [120/200], Loss: 198.2873\n",
      "Epoch [130/200], Loss: 172.0293\n",
      "Epoch [140/200], Loss: 148.9681\n",
      "Epoch [150/200], Loss: 128.5142\n",
      "Epoch [160/200], Loss: 110.3225\n",
      "Epoch [170/200], Loss: 94.1980\n",
      "Epoch [180/200], Loss: 80.0391\n",
      "Epoch [190/200], Loss: 67.8785\n",
      "Epoch [200/200], Loss: 57.7516\n",
      "Epoch [10/200], Loss: 841.2661\n",
      "Epoch [20/200], Loss: 692.1136\n",
      "Epoch [30/200], Loss: 709.6389\n",
      "Epoch [40/200], Loss: 611.6997\n",
      "Epoch [50/200], Loss: 551.1306\n",
      "Epoch [60/200], Loss: 499.6744\n",
      "Epoch [70/200], Loss: 442.9388\n",
      "Epoch [80/200], Loss: 380.4871\n",
      "Epoch [90/200], Loss: 318.5315\n",
      "Epoch [100/200], Loss: 262.4737\n",
      "Epoch [110/200], Loss: 217.6213\n",
      "Epoch [120/200], Loss: 176.8001\n",
      "Epoch [130/200], Loss: 137.9936\n",
      "Epoch [140/200], Loss: 104.8387\n",
      "Epoch [150/200], Loss: 78.5105\n",
      "Epoch [160/200], Loss: 58.8284\n",
      "Epoch [170/200], Loss: 44.1386\n",
      "Epoch [180/200], Loss: 34.0393\n",
      "Epoch [190/200], Loss: 28.3021\n",
      "Epoch [200/200], Loss: 25.7514\n",
      "Epoch [10/200], Loss: 1141.9050\n",
      "Epoch [20/200], Loss: 936.3837\n",
      "Epoch [30/200], Loss: 862.7828\n",
      "Epoch [40/200], Loss: 709.7332\n",
      "Epoch [50/200], Loss: 604.1136\n",
      "Epoch [60/200], Loss: 526.1188\n",
      "Epoch [70/200], Loss: 457.5627\n",
      "Epoch [80/200], Loss: 389.9484\n",
      "Epoch [90/200], Loss: 318.6674\n",
      "Epoch [100/200], Loss: 255.6078\n",
      "Epoch [110/200], Loss: 207.0302\n",
      "Epoch [120/200], Loss: 165.7475\n",
      "Epoch [130/200], Loss: 127.9798\n",
      "Epoch [140/200], Loss: 95.7592\n",
      "Epoch [150/200], Loss: 70.5830\n",
      "Epoch [160/200], Loss: 52.2935\n",
      "Epoch [170/200], Loss: 40.0569\n",
      "Epoch [180/200], Loss: 32.5642\n",
      "Epoch [190/200], Loss: 28.4151\n",
      "Epoch [200/200], Loss: 26.3400\n",
      "Epoch [10/200], Loss: 1363.6324\n",
      "Epoch [20/200], Loss: 981.9022\n",
      "Epoch [30/200], Loss: 828.5031\n",
      "Epoch [40/200], Loss: 672.4233\n",
      "Epoch [50/200], Loss: 595.2743\n",
      "Epoch [60/200], Loss: 533.3007\n",
      "Epoch [70/200], Loss: 472.9776\n",
      "Epoch [80/200], Loss: 414.2397\n",
      "Epoch [90/200], Loss: 360.3202\n",
      "Epoch [100/200], Loss: 309.8774\n",
      "Epoch [110/200], Loss: 265.6015\n",
      "Epoch [120/200], Loss: 225.2779\n",
      "Epoch [130/200], Loss: 186.0549\n",
      "Epoch [140/200], Loss: 149.2096\n",
      "Epoch [150/200], Loss: 116.8728\n",
      "Epoch [160/200], Loss: 88.8912\n",
      "Epoch [170/200], Loss: 66.2014\n",
      "Epoch [180/200], Loss: 49.2968\n",
      "Epoch [190/200], Loss: 37.9152\n",
      "Epoch [200/200], Loss: 31.0979\n",
      "Epoch [10/200], Loss: 738.3971\n",
      "Epoch [20/200], Loss: 686.9383\n",
      "Epoch [30/200], Loss: 697.1784\n",
      "Epoch [40/200], Loss: 622.8940\n",
      "Epoch [50/200], Loss: 551.0850\n",
      "Epoch [60/200], Loss: 487.0508\n",
      "Epoch [70/200], Loss: 421.9046\n",
      "Epoch [80/200], Loss: 353.1867\n",
      "Epoch [90/200], Loss: 290.0444\n",
      "Epoch [100/200], Loss: 235.6181\n",
      "Epoch [110/200], Loss: 188.8347\n",
      "Epoch [120/200], Loss: 145.3609\n",
      "Epoch [130/200], Loss: 107.4126\n",
      "Epoch [140/200], Loss: 77.1841\n",
      "Epoch [150/200], Loss: 55.5629\n",
      "Epoch [160/200], Loss: 41.7690\n",
      "Epoch [170/200], Loss: 33.6285\n",
      "Epoch [180/200], Loss: 29.1338\n",
      "Epoch [190/200], Loss: 26.7799\n",
      "Epoch [200/200], Loss: 25.6003\n",
      "Epoch [10/200], Loss: 914.1418\n",
      "Epoch [20/200], Loss: 822.3972\n",
      "Epoch [30/200], Loss: 793.6731\n",
      "Epoch [40/200], Loss: 676.2266\n",
      "Epoch [50/200], Loss: 606.6800\n",
      "Epoch [60/200], Loss: 555.8828\n",
      "Epoch [70/200], Loss: 505.1715\n",
      "Epoch [80/200], Loss: 450.1873\n",
      "Epoch [90/200], Loss: 389.6030\n",
      "Epoch [100/200], Loss: 326.6842\n",
      "Epoch [110/200], Loss: 266.8467\n",
      "Epoch [120/200], Loss: 214.9157\n",
      "Epoch [130/200], Loss: 171.5726\n",
      "Epoch [140/200], Loss: 130.3642\n",
      "Epoch [150/200], Loss: 95.3124\n",
      "Epoch [160/200], Loss: 68.8319\n",
      "Epoch [170/200], Loss: 50.4039\n",
      "Epoch [180/200], Loss: 38.8805\n",
      "Epoch [190/200], Loss: 32.2045\n",
      "Epoch [200/200], Loss: 28.5757\n",
      "Epoch [10/200], Loss: 1732.0848\n",
      "Epoch [20/200], Loss: 1137.6105\n",
      "Epoch [30/200], Loss: 849.9619\n",
      "Epoch [40/200], Loss: 674.9808\n",
      "Epoch [50/200], Loss: 600.4571\n",
      "Epoch [60/200], Loss: 537.5941\n",
      "Epoch [70/200], Loss: 476.6562\n",
      "Epoch [80/200], Loss: 412.4071\n",
      "Epoch [90/200], Loss: 341.9617\n",
      "Epoch [100/200], Loss: 270.9892\n",
      "Epoch [110/200], Loss: 214.4350\n",
      "Epoch [120/200], Loss: 169.8057\n",
      "Epoch [130/200], Loss: 127.8288\n",
      "Epoch [140/200], Loss: 92.4768\n",
      "Epoch [150/200], Loss: 65.4390\n",
      "Epoch [160/200], Loss: 47.1528\n",
      "Epoch [170/200], Loss: 35.7229\n",
      "Epoch [180/200], Loss: 29.5267\n",
      "Epoch [190/200], Loss: 26.6386\n",
      "Epoch [200/200], Loss: 25.3744\n",
      "Epoch [10/200], Loss: 1489.5228\n",
      "Epoch [20/200], Loss: 998.2756\n",
      "Epoch [30/200], Loss: 808.1170\n",
      "Epoch [40/200], Loss: 645.8666\n",
      "Epoch [50/200], Loss: 578.6652\n",
      "Epoch [60/200], Loss: 530.2275\n",
      "Epoch [70/200], Loss: 484.6168\n",
      "Epoch [80/200], Loss: 435.7249\n",
      "Epoch [90/200], Loss: 382.9392\n",
      "Epoch [100/200], Loss: 327.7708\n",
      "Epoch [110/200], Loss: 275.5993\n",
      "Epoch [120/200], Loss: 232.8807\n",
      "Epoch [130/200], Loss: 198.9846\n",
      "Epoch [140/200], Loss: 168.2070\n",
      "Epoch [150/200], Loss: 139.9068\n",
      "Epoch [160/200], Loss: 114.9669\n",
      "Epoch [170/200], Loss: 93.7495\n",
      "Epoch [180/200], Loss: 76.1230\n",
      "Epoch [190/200], Loss: 61.9480\n",
      "Epoch [200/200], Loss: 50.9471\n",
      "Epoch [10/200], Loss: 747.1603\n",
      "Epoch [20/200], Loss: 683.8076\n",
      "Epoch [30/200], Loss: 661.4091\n",
      "Epoch [40/200], Loss: 595.3583\n",
      "Epoch [50/200], Loss: 530.5735\n",
      "Epoch [60/200], Loss: 464.0092\n",
      "Epoch [70/200], Loss: 393.0345\n",
      "Epoch [80/200], Loss: 316.3434\n",
      "Epoch [90/200], Loss: 251.9593\n",
      "Epoch [100/200], Loss: 204.8396\n",
      "Epoch [110/200], Loss: 165.3666\n",
      "Epoch [120/200], Loss: 130.1301\n",
      "Epoch [130/200], Loss: 100.4201\n",
      "Epoch [140/200], Loss: 76.8592\n",
      "Epoch [150/200], Loss: 59.0598\n",
      "Epoch [160/200], Loss: 46.3359\n",
      "Epoch [170/200], Loss: 37.6670\n",
      "Epoch [180/200], Loss: 32.2934\n",
      "Epoch [190/200], Loss: 29.2142\n",
      "Epoch [200/200], Loss: 27.1512\n",
      "Epoch [10/200], Loss: 777.7731\n",
      "Epoch [20/200], Loss: 728.7506\n",
      "Epoch [30/200], Loss: 739.6672\n",
      "Epoch [40/200], Loss: 658.3806\n",
      "Epoch [50/200], Loss: 581.2957\n",
      "Epoch [60/200], Loss: 522.2836\n",
      "Epoch [70/200], Loss: 462.3163\n",
      "Epoch [80/200], Loss: 399.4183\n",
      "Epoch [90/200], Loss: 331.4331\n",
      "Epoch [100/200], Loss: 267.1857\n",
      "Epoch [110/200], Loss: 217.4884\n",
      "Epoch [120/200], Loss: 175.8399\n",
      "Epoch [130/200], Loss: 137.0285\n",
      "Epoch [140/200], Loss: 103.9128\n",
      "Epoch [150/200], Loss: 77.1056\n",
      "Epoch [160/200], Loss: 56.9632\n",
      "Epoch [170/200], Loss: 43.1886\n",
      "Epoch [180/200], Loss: 34.5345\n",
      "Epoch [190/200], Loss: 29.5557\n",
      "Epoch [200/200], Loss: 26.9212\n",
      "Epoch [10/200], Loss: 901.7975\n",
      "Epoch [20/200], Loss: 819.9658\n",
      "Epoch [30/200], Loss: 776.8584\n",
      "Epoch [40/200], Loss: 665.3035\n",
      "Epoch [50/200], Loss: 576.6984\n",
      "Epoch [60/200], Loss: 508.9405\n",
      "Epoch [70/200], Loss: 444.1978\n",
      "Epoch [80/200], Loss: 380.8952\n",
      "Epoch [90/200], Loss: 314.0453\n",
      "Epoch [100/200], Loss: 258.0740\n",
      "Epoch [110/200], Loss: 211.7514\n",
      "Epoch [120/200], Loss: 167.3213\n",
      "Epoch [130/200], Loss: 127.8059\n",
      "Epoch [140/200], Loss: 94.4912\n",
      "Epoch [150/200], Loss: 67.2721\n",
      "Epoch [160/200], Loss: 47.6171\n",
      "Epoch [170/200], Loss: 35.0989\n",
      "Epoch [180/200], Loss: 28.8600\n",
      "Epoch [190/200], Loss: 26.1427\n",
      "Epoch [200/200], Loss: 25.2795\n",
      "Epoch [10/200], Loss: 903.9055\n",
      "Epoch [20/200], Loss: 776.9055\n",
      "Epoch [30/200], Loss: 759.3374\n",
      "Epoch [40/200], Loss: 640.1508\n",
      "Epoch [50/200], Loss: 577.6197\n",
      "Epoch [60/200], Loss: 518.9506\n",
      "Epoch [70/200], Loss: 444.6908\n",
      "Epoch [80/200], Loss: 360.4604\n",
      "Epoch [90/200], Loss: 289.1202\n",
      "Epoch [100/200], Loss: 237.3936\n",
      "Epoch [110/200], Loss: 188.6216\n",
      "Epoch [120/200], Loss: 141.7827\n",
      "Epoch [130/200], Loss: 102.4953\n",
      "Epoch [140/200], Loss: 70.9393\n",
      "Epoch [150/200], Loss: 48.7245\n",
      "Epoch [160/200], Loss: 35.5796\n",
      "Epoch [170/200], Loss: 29.0691\n",
      "Epoch [180/200], Loss: 26.3021\n",
      "Epoch [190/200], Loss: 25.3220\n",
      "Epoch [200/200], Loss: 24.8363\n",
      "Epoch [10/200], Loss: 1101.7528\n",
      "Epoch [20/200], Loss: 851.8578\n",
      "Epoch [30/200], Loss: 789.1472\n",
      "Epoch [40/200], Loss: 653.8333\n",
      "Epoch [50/200], Loss: 579.5695\n",
      "Epoch [60/200], Loss: 522.8288\n",
      "Epoch [70/200], Loss: 465.4497\n",
      "Epoch [80/200], Loss: 402.2784\n",
      "Epoch [90/200], Loss: 337.2367\n",
      "Epoch [100/200], Loss: 275.4466\n",
      "Epoch [110/200], Loss: 223.6687\n",
      "Epoch [120/200], Loss: 182.8269\n",
      "Epoch [130/200], Loss: 145.9905\n",
      "Epoch [140/200], Loss: 112.7245\n",
      "Epoch [150/200], Loss: 85.4645\n",
      "Epoch [160/200], Loss: 64.2100\n",
      "Epoch [170/200], Loss: 48.9716\n",
      "Epoch [180/200], Loss: 38.7047\n",
      "Epoch [190/200], Loss: 31.8885\n",
      "Epoch [200/200], Loss: 27.8344\n",
      "Epoch [10/200], Loss: 850.4130\n",
      "Epoch [20/200], Loss: 768.2643\n",
      "Epoch [30/200], Loss: 743.0316\n",
      "Epoch [40/200], Loss: 630.9302\n",
      "Epoch [50/200], Loss: 535.3915\n",
      "Epoch [60/200], Loss: 457.8235\n",
      "Epoch [70/200], Loss: 385.4589\n",
      "Epoch [80/200], Loss: 317.8855\n",
      "Epoch [90/200], Loss: 264.8109\n",
      "Epoch [100/200], Loss: 225.8157\n",
      "Epoch [110/200], Loss: 189.8162\n",
      "Epoch [120/200], Loss: 154.9601\n",
      "Epoch [130/200], Loss: 123.5082\n",
      "Epoch [140/200], Loss: 96.0112\n",
      "Epoch [150/200], Loss: 73.5539\n",
      "Epoch [160/200], Loss: 56.5922\n",
      "Epoch [170/200], Loss: 44.6265\n",
      "Epoch [180/200], Loss: 36.5998\n",
      "Epoch [190/200], Loss: 31.4719\n",
      "Epoch [200/200], Loss: 28.3775\n",
      "Epoch [10/200], Loss: 1388.4508\n",
      "Epoch [20/200], Loss: 1010.7166\n",
      "Epoch [30/200], Loss: 816.2202\n",
      "Epoch [40/200], Loss: 672.0851\n",
      "Epoch [50/200], Loss: 624.8633\n",
      "Epoch [60/200], Loss: 577.8144\n",
      "Epoch [70/200], Loss: 528.5555\n",
      "Epoch [80/200], Loss: 468.4211\n",
      "Epoch [90/200], Loss: 395.7082\n",
      "Epoch [100/200], Loss: 320.0648\n",
      "Epoch [110/200], Loss: 256.2719\n",
      "Epoch [120/200], Loss: 210.7321\n",
      "Epoch [130/200], Loss: 169.0495\n",
      "Epoch [140/200], Loss: 131.1430\n",
      "Epoch [150/200], Loss: 99.5050\n",
      "Epoch [160/200], Loss: 74.1613\n",
      "Epoch [170/200], Loss: 55.6134\n",
      "Epoch [180/200], Loss: 42.9979\n",
      "Epoch [190/200], Loss: 35.0608\n",
      "Epoch [200/200], Loss: 30.3768\n",
      "Epoch [10/200], Loss: 1006.1321\n",
      "Epoch [20/200], Loss: 797.9719\n",
      "Epoch [30/200], Loss: 764.2820\n",
      "Epoch [40/200], Loss: 641.0247\n",
      "Epoch [50/200], Loss: 581.6571\n",
      "Epoch [60/200], Loss: 543.6664\n",
      "Epoch [70/200], Loss: 503.9831\n",
      "Epoch [80/200], Loss: 451.0663\n",
      "Epoch [90/200], Loss: 382.4848\n",
      "Epoch [100/200], Loss: 307.6338\n",
      "Epoch [110/200], Loss: 245.9643\n",
      "Epoch [120/200], Loss: 203.0503\n",
      "Epoch [130/200], Loss: 164.6641\n",
      "Epoch [140/200], Loss: 129.9241\n",
      "Epoch [150/200], Loss: 100.4286\n",
      "Epoch [160/200], Loss: 76.4116\n",
      "Epoch [170/200], Loss: 57.8890\n",
      "Epoch [180/200], Loss: 44.5835\n",
      "Epoch [190/200], Loss: 35.8445\n",
      "Epoch [200/200], Loss: 30.5288\n",
      "Epoch [10/200], Loss: 1164.2712\n",
      "Epoch [20/200], Loss: 933.1282\n",
      "Epoch [30/200], Loss: 877.7729\n",
      "Epoch [40/200], Loss: 738.9159\n",
      "Epoch [50/200], Loss: 655.1722\n",
      "Epoch [60/200], Loss: 581.8236\n",
      "Epoch [70/200], Loss: 501.0814\n",
      "Epoch [80/200], Loss: 410.8772\n",
      "Epoch [90/200], Loss: 317.7700\n",
      "Epoch [100/200], Loss: 241.7703\n",
      "Epoch [110/200], Loss: 193.8312\n",
      "Epoch [120/200], Loss: 148.1935\n",
      "Epoch [130/200], Loss: 107.3619\n",
      "Epoch [140/200], Loss: 76.2985\n",
      "Epoch [150/200], Loss: 53.9064\n",
      "Epoch [160/200], Loss: 39.9733\n",
      "Epoch [170/200], Loss: 32.0827\n",
      "Epoch [180/200], Loss: 28.0351\n",
      "Epoch [190/200], Loss: 26.0952\n",
      "Epoch [200/200], Loss: 25.2047\n",
      "Epoch [10/200], Loss: 1451.5087\n",
      "Epoch [20/200], Loss: 1055.7737\n",
      "Epoch [30/200], Loss: 834.2987\n",
      "Epoch [40/200], Loss: 688.5862\n",
      "Epoch [50/200], Loss: 642.6293\n",
      "Epoch [60/200], Loss: 601.3547\n",
      "Epoch [70/200], Loss: 554.8015\n",
      "Epoch [80/200], Loss: 501.1798\n",
      "Epoch [90/200], Loss: 440.1514\n",
      "Epoch [100/200], Loss: 371.0214\n",
      "Epoch [110/200], Loss: 297.3135\n",
      "Epoch [120/200], Loss: 227.8462\n",
      "Epoch [130/200], Loss: 173.0487\n",
      "Epoch [140/200], Loss: 127.8067\n",
      "Epoch [150/200], Loss: 92.0479\n",
      "Epoch [160/200], Loss: 66.6184\n",
      "Epoch [170/200], Loss: 49.3052\n",
      "Epoch [180/200], Loss: 38.2412\n",
      "Epoch [190/200], Loss: 31.7447\n",
      "Epoch [200/200], Loss: 28.2399\n",
      "Epoch [10/200], Loss: 879.6522\n",
      "Epoch [20/200], Loss: 800.5311\n",
      "Epoch [30/200], Loss: 775.6802\n",
      "Epoch [40/200], Loss: 664.8641\n",
      "Epoch [50/200], Loss: 560.2252\n",
      "Epoch [60/200], Loss: 471.7402\n",
      "Epoch [70/200], Loss: 389.1809\n",
      "Epoch [80/200], Loss: 321.1725\n",
      "Epoch [90/200], Loss: 269.2661\n",
      "Epoch [100/200], Loss: 233.6283\n",
      "Epoch [110/200], Loss: 203.8881\n",
      "Epoch [120/200], Loss: 176.2878\n",
      "Epoch [130/200], Loss: 150.9908\n",
      "Epoch [140/200], Loss: 128.0204\n",
      "Epoch [150/200], Loss: 107.3130\n",
      "Epoch [160/200], Loss: 89.1326\n",
      "Epoch [170/200], Loss: 73.6409\n",
      "Epoch [180/200], Loss: 60.8887\n",
      "Epoch [190/200], Loss: 50.7969\n",
      "Epoch [200/200], Loss: 43.1308\n",
      "Epoch [10/200], Loss: 1477.2537\n",
      "Epoch [20/200], Loss: 1082.5751\n",
      "Epoch [30/200], Loss: 868.0481\n",
      "Epoch [40/200], Loss: 694.2747\n",
      "Epoch [50/200], Loss: 617.4495\n",
      "Epoch [60/200], Loss: 553.8580\n",
      "Epoch [70/200], Loss: 492.4766\n",
      "Epoch [80/200], Loss: 433.5730\n",
      "Epoch [90/200], Loss: 378.6795\n",
      "Epoch [100/200], Loss: 329.2426\n",
      "Epoch [110/200], Loss: 287.6711\n",
      "Epoch [120/200], Loss: 255.0028\n",
      "Epoch [130/200], Loss: 229.3191\n",
      "Epoch [140/200], Loss: 206.4131\n",
      "Epoch [150/200], Loss: 184.0549\n",
      "Epoch [160/200], Loss: 162.1997\n",
      "Epoch [170/200], Loss: 141.2492\n",
      "Epoch [180/200], Loss: 121.4738\n",
      "Epoch [190/200], Loss: 103.3074\n",
      "Epoch [200/200], Loss: 87.1317\n",
      "Epoch [10/200], Loss: 1035.3435\n",
      "Epoch [20/200], Loss: 853.2273\n",
      "Epoch [30/200], Loss: 775.0153\n",
      "Epoch [40/200], Loss: 630.0743\n",
      "Epoch [50/200], Loss: 565.2695\n",
      "Epoch [60/200], Loss: 516.4414\n",
      "Epoch [70/200], Loss: 471.1877\n",
      "Epoch [80/200], Loss: 426.7055\n",
      "Epoch [90/200], Loss: 382.5059\n",
      "Epoch [100/200], Loss: 339.3619\n",
      "Epoch [110/200], Loss: 298.7840\n",
      "Epoch [120/200], Loss: 261.9488\n",
      "Epoch [130/200], Loss: 228.5838\n",
      "Epoch [140/200], Loss: 197.0799\n",
      "Epoch [150/200], Loss: 164.7113\n",
      "Epoch [160/200], Loss: 133.6862\n",
      "Epoch [170/200], Loss: 105.7825\n",
      "Epoch [180/200], Loss: 81.3179\n",
      "Epoch [190/200], Loss: 61.1407\n",
      "Epoch [200/200], Loss: 45.9258\n",
      "Epoch [10/200], Loss: 980.8867\n",
      "Epoch [20/200], Loss: 822.4844\n",
      "Epoch [30/200], Loss: 761.5898\n",
      "Epoch [40/200], Loss: 647.9799\n",
      "Epoch [50/200], Loss: 562.7718\n",
      "Epoch [60/200], Loss: 506.4226\n",
      "Epoch [70/200], Loss: 459.0544\n",
      "Epoch [80/200], Loss: 412.7321\n",
      "Epoch [90/200], Loss: 365.8854\n",
      "Epoch [100/200], Loss: 319.1718\n",
      "Epoch [110/200], Loss: 274.4060\n",
      "Epoch [120/200], Loss: 233.8058\n",
      "Epoch [130/200], Loss: 199.0584\n",
      "Epoch [140/200], Loss: 168.9625\n",
      "Epoch [150/200], Loss: 141.7183\n",
      "Epoch [160/200], Loss: 117.2766\n",
      "Epoch [170/200], Loss: 96.0470\n",
      "Epoch [180/200], Loss: 78.0768\n",
      "Epoch [190/200], Loss: 63.3425\n",
      "Epoch [200/200], Loss: 51.8059\n",
      "Epoch [10/200], Loss: 1243.5309\n",
      "Epoch [20/200], Loss: 894.9561\n",
      "Epoch [30/200], Loss: 783.8661\n",
      "Epoch [40/200], Loss: 641.6870\n",
      "Epoch [50/200], Loss: 582.9641\n",
      "Epoch [60/200], Loss: 541.8209\n",
      "Epoch [70/200], Loss: 495.7601\n",
      "Epoch [80/200], Loss: 441.0212\n",
      "Epoch [90/200], Loss: 379.8965\n",
      "Epoch [100/200], Loss: 319.9600\n",
      "Epoch [110/200], Loss: 270.5340\n",
      "Epoch [120/200], Loss: 232.4807\n",
      "Epoch [130/200], Loss: 198.1723\n",
      "Epoch [140/200], Loss: 164.4134\n",
      "Epoch [150/200], Loss: 133.0860\n",
      "Epoch [160/200], Loss: 105.3110\n",
      "Epoch [170/200], Loss: 80.1783\n",
      "Epoch [180/200], Loss: 59.5534\n",
      "Epoch [190/200], Loss: 44.6059\n",
      "Epoch [200/200], Loss: 35.0592\n",
      "Epoch [10/200], Loss: 1159.7336\n",
      "Epoch [20/200], Loss: 896.3884\n",
      "Epoch [30/200], Loss: 793.2123\n",
      "Epoch [40/200], Loss: 647.5842\n",
      "Epoch [50/200], Loss: 587.6641\n",
      "Epoch [60/200], Loss: 539.0635\n",
      "Epoch [70/200], Loss: 479.0974\n",
      "Epoch [80/200], Loss: 410.3523\n",
      "Epoch [90/200], Loss: 336.4069\n",
      "Epoch [100/200], Loss: 271.8844\n",
      "Epoch [110/200], Loss: 225.8597\n",
      "Epoch [120/200], Loss: 185.3052\n",
      "Epoch [130/200], Loss: 148.2122\n",
      "Epoch [140/200], Loss: 116.4697\n",
      "Epoch [150/200], Loss: 89.5842\n",
      "Epoch [160/200], Loss: 68.1707\n",
      "Epoch [170/200], Loss: 52.1303\n",
      "Epoch [180/200], Loss: 40.8597\n",
      "Epoch [190/200], Loss: 33.4691\n",
      "Epoch [200/200], Loss: 29.0175\n",
      "Epoch [10/200], Loss: 1087.2899\n",
      "Epoch [20/200], Loss: 817.2130\n",
      "Epoch [30/200], Loss: 768.5918\n",
      "Epoch [40/200], Loss: 638.3620\n",
      "Epoch [50/200], Loss: 562.5705\n",
      "Epoch [60/200], Loss: 497.6451\n",
      "Epoch [70/200], Loss: 426.6296\n",
      "Epoch [80/200], Loss: 354.1079\n",
      "Epoch [90/200], Loss: 283.4979\n",
      "Epoch [100/200], Loss: 225.8831\n",
      "Epoch [110/200], Loss: 181.2039\n",
      "Epoch [120/200], Loss: 141.7921\n",
      "Epoch [130/200], Loss: 104.9787\n",
      "Epoch [140/200], Loss: 75.4056\n",
      "Epoch [150/200], Loss: 53.6959\n",
      "Epoch [160/200], Loss: 39.8439\n",
      "Epoch [170/200], Loss: 32.0149\n",
      "Epoch [180/200], Loss: 27.9838\n",
      "Epoch [190/200], Loss: 26.0500\n",
      "Epoch [200/200], Loss: 25.1824\n",
      "Epoch [10/200], Loss: 702.3592\n",
      "Epoch [20/200], Loss: 662.2258\n",
      "Epoch [30/200], Loss: 675.2698\n",
      "Epoch [40/200], Loss: 616.2906\n",
      "Epoch [50/200], Loss: 545.6367\n",
      "Epoch [60/200], Loss: 473.0227\n",
      "Epoch [70/200], Loss: 393.0820\n",
      "Epoch [80/200], Loss: 315.1671\n",
      "Epoch [90/200], Loss: 253.7278\n",
      "Epoch [100/200], Loss: 199.8618\n",
      "Epoch [110/200], Loss: 148.4829\n",
      "Epoch [120/200], Loss: 104.3925\n",
      "Epoch [130/200], Loss: 70.7932\n",
      "Epoch [140/200], Loss: 48.8087\n",
      "Epoch [150/200], Loss: 35.9035\n",
      "Epoch [160/200], Loss: 29.2561\n",
      "Epoch [170/200], Loss: 26.5101\n",
      "Epoch [180/200], Loss: 25.3575\n",
      "Epoch [190/200], Loss: 24.8709\n",
      "Epoch [200/200], Loss: 24.6860\n",
      "Epoch [10/200], Loss: 795.1109\n",
      "Epoch [20/200], Loss: 715.1072\n",
      "Epoch [30/200], Loss: 730.5416\n",
      "Epoch [40/200], Loss: 639.2693\n",
      "Epoch [50/200], Loss: 577.0435\n",
      "Epoch [60/200], Loss: 534.2381\n",
      "Epoch [70/200], Loss: 492.1542\n",
      "Epoch [80/200], Loss: 443.7204\n",
      "Epoch [90/200], Loss: 385.4477\n",
      "Epoch [100/200], Loss: 311.7000\n",
      "Epoch [110/200], Loss: 234.2546\n",
      "Epoch [120/200], Loss: 171.3461\n",
      "Epoch [130/200], Loss: 118.1896\n",
      "Epoch [140/200], Loss: 76.7283\n",
      "Epoch [150/200], Loss: 50.1643\n",
      "Epoch [160/200], Loss: 35.2633\n",
      "Epoch [170/200], Loss: 28.7456\n",
      "Epoch [180/200], Loss: 26.1529\n",
      "Epoch [190/200], Loss: 25.1510\n",
      "Epoch [200/200], Loss: 24.7822\n",
      "Epoch [10/200], Loss: 876.9989\n",
      "Epoch [20/200], Loss: 791.4006\n",
      "Epoch [30/200], Loss: 798.1181\n",
      "Epoch [40/200], Loss: 689.0054\n",
      "Epoch [50/200], Loss: 617.5568\n",
      "Epoch [60/200], Loss: 539.6586\n",
      "Epoch [70/200], Loss: 447.6464\n",
      "Epoch [80/200], Loss: 344.3086\n",
      "Epoch [90/200], Loss: 253.2100\n",
      "Epoch [100/200], Loss: 198.3363\n",
      "Epoch [110/200], Loss: 155.7505\n",
      "Epoch [120/200], Loss: 114.6941\n",
      "Epoch [130/200], Loss: 82.7135\n",
      "Epoch [140/200], Loss: 59.2337\n",
      "Epoch [150/200], Loss: 43.5405\n",
      "Epoch [160/200], Loss: 34.2161\n",
      "Epoch [170/200], Loss: 29.0400\n",
      "Epoch [180/200], Loss: 26.6401\n",
      "Epoch [190/200], Loss: 25.5047\n",
      "Epoch [200/200], Loss: 24.9361\n",
      "Epoch [10/200], Loss: 1070.6774\n",
      "Epoch [20/200], Loss: 863.8502\n",
      "Epoch [30/200], Loss: 777.8714\n",
      "Epoch [40/200], Loss: 633.4221\n",
      "Epoch [50/200], Loss: 558.4280\n",
      "Epoch [60/200], Loss: 508.3581\n",
      "Epoch [70/200], Loss: 465.7202\n",
      "Epoch [80/200], Loss: 424.5008\n",
      "Epoch [90/200], Loss: 380.6862\n",
      "Epoch [100/200], Loss: 333.5380\n",
      "Epoch [110/200], Loss: 284.8409\n",
      "Epoch [120/200], Loss: 234.5872\n",
      "Epoch [130/200], Loss: 188.9334\n",
      "Epoch [140/200], Loss: 146.6289\n",
      "Epoch [150/200], Loss: 107.7530\n",
      "Epoch [160/200], Loss: 76.7580\n",
      "Epoch [170/200], Loss: 54.1203\n",
      "Epoch [180/200], Loss: 39.5380\n",
      "Epoch [190/200], Loss: 31.3036\n",
      "Epoch [200/200], Loss: 27.4582\n",
      "Epoch [10/200], Loss: 1316.6066\n",
      "Epoch [20/200], Loss: 909.5945\n",
      "Epoch [30/200], Loss: 815.3767\n",
      "Epoch [40/200], Loss: 671.2419\n",
      "Epoch [50/200], Loss: 593.7757\n",
      "Epoch [60/200], Loss: 533.7908\n",
      "Epoch [70/200], Loss: 471.0086\n",
      "Epoch [80/200], Loss: 402.2632\n",
      "Epoch [90/200], Loss: 329.5863\n",
      "Epoch [100/200], Loss: 267.4012\n",
      "Epoch [110/200], Loss: 219.4270\n",
      "Epoch [120/200], Loss: 177.4059\n",
      "Epoch [130/200], Loss: 140.0991\n",
      "Epoch [140/200], Loss: 108.7974\n",
      "Epoch [150/200], Loss: 83.6947\n",
      "Epoch [160/200], Loss: 64.2623\n",
      "Epoch [170/200], Loss: 49.8882\n",
      "Epoch [180/200], Loss: 39.8645\n",
      "Epoch [190/200], Loss: 33.3663\n",
      "Epoch [200/200], Loss: 29.4576\n",
      "Epoch [10/200], Loss: 986.5927\n",
      "Epoch [20/200], Loss: 765.3381\n",
      "Epoch [30/200], Loss: 750.2590\n",
      "Epoch [40/200], Loss: 635.5405\n",
      "Epoch [50/200], Loss: 573.9634\n",
      "Epoch [60/200], Loss: 524.1143\n",
      "Epoch [70/200], Loss: 467.0643\n",
      "Epoch [80/200], Loss: 391.4427\n",
      "Epoch [90/200], Loss: 310.4047\n",
      "Epoch [100/200], Loss: 246.4466\n",
      "Epoch [110/200], Loss: 196.2783\n",
      "Epoch [120/200], Loss: 147.8719\n",
      "Epoch [130/200], Loss: 106.1645\n",
      "Epoch [140/200], Loss: 73.6883\n",
      "Epoch [150/200], Loss: 51.4153\n",
      "Epoch [160/200], Loss: 37.7364\n",
      "Epoch [170/200], Loss: 30.4300\n",
      "Epoch [180/200], Loss: 27.0415\n",
      "Epoch [190/200], Loss: 25.5831\n",
      "Epoch [200/200], Loss: 24.9744\n",
      "Epoch [10/200], Loss: 795.4175\n",
      "Epoch [20/200], Loss: 711.6189\n",
      "Epoch [30/200], Loss: 729.3157\n",
      "Epoch [40/200], Loss: 628.9186\n",
      "Epoch [50/200], Loss: 567.3864\n",
      "Epoch [60/200], Loss: 523.8834\n",
      "Epoch [70/200], Loss: 473.8723\n",
      "Epoch [80/200], Loss: 407.9542\n",
      "Epoch [90/200], Loss: 333.9972\n",
      "Epoch [100/200], Loss: 274.5013\n",
      "Epoch [110/200], Loss: 230.8232\n",
      "Epoch [120/200], Loss: 191.8486\n",
      "Epoch [130/200], Loss: 156.9612\n",
      "Epoch [140/200], Loss: 126.8286\n",
      "Epoch [150/200], Loss: 101.3134\n",
      "Epoch [160/200], Loss: 80.4259\n",
      "Epoch [170/200], Loss: 64.1088\n",
      "Epoch [180/200], Loss: 51.9153\n",
      "Epoch [190/200], Loss: 43.1105\n",
      "Epoch [200/200], Loss: 36.9337\n",
      "Epoch [10/200], Loss: 939.1960\n",
      "Epoch [20/200], Loss: 824.5439\n",
      "Epoch [30/200], Loss: 775.7919\n",
      "Epoch [40/200], Loss: 664.8108\n",
      "Epoch [50/200], Loss: 582.4993\n",
      "Epoch [60/200], Loss: 521.4119\n",
      "Epoch [70/200], Loss: 455.5227\n",
      "Epoch [80/200], Loss: 384.4290\n",
      "Epoch [90/200], Loss: 313.0572\n",
      "Epoch [100/200], Loss: 253.5805\n",
      "Epoch [110/200], Loss: 208.3381\n",
      "Epoch [120/200], Loss: 169.7831\n",
      "Epoch [130/200], Loss: 135.2256\n",
      "Epoch [140/200], Loss: 105.5136\n",
      "Epoch [150/200], Loss: 81.3418\n",
      "Epoch [160/200], Loss: 62.6399\n",
      "Epoch [170/200], Loss: 48.9862\n",
      "Epoch [180/200], Loss: 39.5945\n",
      "Epoch [190/200], Loss: 33.5363\n",
      "Epoch [200/200], Loss: 29.7872\n",
      "Epoch [10/200], Loss: 1160.7457\n",
      "Epoch [20/200], Loss: 843.9709\n",
      "Epoch [30/200], Loss: 758.6913\n",
      "Epoch [40/200], Loss: 624.4910\n",
      "Epoch [50/200], Loss: 571.7888\n",
      "Epoch [60/200], Loss: 529.9009\n",
      "Epoch [70/200], Loss: 485.5586\n",
      "Epoch [80/200], Loss: 434.0896\n",
      "Epoch [90/200], Loss: 378.2797\n",
      "Epoch [100/200], Loss: 320.9389\n",
      "Epoch [110/200], Loss: 267.8230\n",
      "Epoch [120/200], Loss: 225.1991\n",
      "Epoch [130/200], Loss: 190.0746\n",
      "Epoch [140/200], Loss: 157.5852\n",
      "Epoch [150/200], Loss: 127.9232\n",
      "Epoch [160/200], Loss: 102.1158\n",
      "Epoch [170/200], Loss: 80.4769\n",
      "Epoch [180/200], Loss: 63.1752\n",
      "Epoch [190/200], Loss: 50.0615\n",
      "Epoch [200/200], Loss: 40.6848\n",
      "Epoch [10/200], Loss: 830.0130\n",
      "Epoch [20/200], Loss: 752.4150\n",
      "Epoch [30/200], Loss: 733.5547\n",
      "Epoch [40/200], Loss: 634.9225\n",
      "Epoch [50/200], Loss: 549.0013\n",
      "Epoch [60/200], Loss: 485.4264\n",
      "Epoch [70/200], Loss: 424.7641\n",
      "Epoch [80/200], Loss: 365.9208\n",
      "Epoch [90/200], Loss: 311.1535\n",
      "Epoch [100/200], Loss: 262.7233\n",
      "Epoch [110/200], Loss: 224.7172\n",
      "Epoch [120/200], Loss: 195.4443\n",
      "Epoch [130/200], Loss: 169.9207\n",
      "Epoch [140/200], Loss: 146.1454\n",
      "Epoch [150/200], Loss: 124.4244\n",
      "Epoch [160/200], Loss: 105.0878\n",
      "Epoch [170/200], Loss: 88.1802\n",
      "Epoch [180/200], Loss: 73.6861\n",
      "Epoch [190/200], Loss: 61.6899\n",
      "Epoch [200/200], Loss: 52.0831\n",
      "Epoch [10/200], Loss: 1276.5543\n",
      "Epoch [20/200], Loss: 918.2592\n",
      "Epoch [30/200], Loss: 795.8253\n",
      "Epoch [40/200], Loss: 651.1940\n",
      "Epoch [50/200], Loss: 593.6303\n",
      "Epoch [60/200], Loss: 553.0624\n",
      "Epoch [70/200], Loss: 512.9583\n",
      "Epoch [80/200], Loss: 468.8963\n",
      "Epoch [90/200], Loss: 418.1153\n",
      "Epoch [100/200], Loss: 360.9464\n",
      "Epoch [110/200], Loss: 302.6630\n",
      "Epoch [120/200], Loss: 251.3881\n",
      "Epoch [130/200], Loss: 209.5298\n",
      "Epoch [140/200], Loss: 172.6604\n",
      "Epoch [150/200], Loss: 139.5580\n",
      "Epoch [160/200], Loss: 110.8159\n",
      "Epoch [170/200], Loss: 87.0368\n",
      "Epoch [180/200], Loss: 68.2242\n",
      "Epoch [190/200], Loss: 53.2856\n",
      "Epoch [200/200], Loss: 41.8338\n",
      "Epoch [10/200], Loss: 918.1281\n",
      "Epoch [20/200], Loss: 822.4987\n",
      "Epoch [30/200], Loss: 800.4223\n",
      "Epoch [40/200], Loss: 683.6253\n",
      "Epoch [50/200], Loss: 585.5352\n",
      "Epoch [60/200], Loss: 514.0554\n",
      "Epoch [70/200], Loss: 443.9723\n",
      "Epoch [80/200], Loss: 375.0416\n",
      "Epoch [90/200], Loss: 308.2262\n",
      "Epoch [100/200], Loss: 249.5817\n",
      "Epoch [110/200], Loss: 200.9996\n",
      "Epoch [120/200], Loss: 154.3578\n",
      "Epoch [130/200], Loss: 110.4788\n",
      "Epoch [140/200], Loss: 75.1088\n",
      "Epoch [150/200], Loss: 50.7512\n",
      "Epoch [160/200], Loss: 36.5717\n",
      "Epoch [170/200], Loss: 29.3381\n",
      "Epoch [180/200], Loss: 26.4807\n",
      "Epoch [190/200], Loss: 25.3454\n",
      "Epoch [200/200], Loss: 24.9029\n",
      "Epoch [10/200], Loss: 1708.7017\n",
      "Epoch [20/200], Loss: 1189.9570\n",
      "Epoch [30/200], Loss: 827.2913\n",
      "Epoch [40/200], Loss: 684.1731\n",
      "Epoch [50/200], Loss: 641.6505\n",
      "Epoch [60/200], Loss: 595.5740\n",
      "Epoch [70/200], Loss: 548.8105\n",
      "Epoch [80/200], Loss: 503.9318\n",
      "Epoch [90/200], Loss: 459.7923\n",
      "Epoch [100/200], Loss: 415.9038\n",
      "Epoch [110/200], Loss: 371.5209\n",
      "Epoch [120/200], Loss: 327.0274\n",
      "Epoch [130/200], Loss: 285.1672\n",
      "Epoch [140/200], Loss: 245.8038\n",
      "Epoch [150/200], Loss: 210.9400\n",
      "Epoch [160/200], Loss: 178.1746\n",
      "Epoch [170/200], Loss: 147.4926\n",
      "Epoch [180/200], Loss: 119.1255\n",
      "Epoch [190/200], Loss: 93.6917\n",
      "Epoch [200/200], Loss: 72.2185\n",
      "Epoch [10/200], Loss: 894.8081\n",
      "Epoch [20/200], Loss: 796.1717\n",
      "Epoch [30/200], Loss: 801.4934\n",
      "Epoch [40/200], Loss: 695.5773\n",
      "Epoch [50/200], Loss: 619.4662\n",
      "Epoch [60/200], Loss: 562.7538\n",
      "Epoch [70/200], Loss: 507.6317\n",
      "Epoch [80/200], Loss: 447.4519\n",
      "Epoch [90/200], Loss: 382.1200\n",
      "Epoch [100/200], Loss: 315.2924\n",
      "Epoch [110/200], Loss: 259.3543\n",
      "Epoch [120/200], Loss: 218.9739\n",
      "Epoch [130/200], Loss: 186.8106\n",
      "Epoch [140/200], Loss: 157.3251\n",
      "Epoch [150/200], Loss: 131.0310\n",
      "Epoch [160/200], Loss: 108.3512\n",
      "Epoch [170/200], Loss: 89.1812\n",
      "Epoch [180/200], Loss: 73.0139\n",
      "Epoch [190/200], Loss: 59.0338\n",
      "Epoch [200/200], Loss: 47.4958\n",
      "Epoch [10/200], Loss: 1496.1230\n",
      "Epoch [20/200], Loss: 1077.4949\n",
      "Epoch [30/200], Loss: 800.1159\n",
      "Epoch [40/200], Loss: 635.2350\n",
      "Epoch [50/200], Loss: 555.5248\n",
      "Epoch [60/200], Loss: 474.5534\n",
      "Epoch [70/200], Loss: 388.0788\n",
      "Epoch [80/200], Loss: 311.8764\n",
      "Epoch [90/200], Loss: 264.0295\n",
      "Epoch [100/200], Loss: 227.2418\n",
      "Epoch [110/200], Loss: 190.9932\n",
      "Epoch [120/200], Loss: 158.3862\n",
      "Epoch [130/200], Loss: 128.6678\n",
      "Epoch [140/200], Loss: 102.1147\n",
      "Epoch [150/200], Loss: 78.4137\n",
      "Epoch [160/200], Loss: 59.1425\n",
      "Epoch [170/200], Loss: 44.9142\n",
      "Epoch [180/200], Loss: 35.4367\n",
      "Epoch [190/200], Loss: 29.8462\n",
      "Epoch [200/200], Loss: 26.9418\n",
      "Epoch [10/200], Loss: 775.9545\n",
      "Epoch [20/200], Loss: 684.3134\n",
      "Epoch [30/200], Loss: 708.3984\n",
      "Epoch [40/200], Loss: 621.4128\n",
      "Epoch [50/200], Loss: 554.0934\n",
      "Epoch [60/200], Loss: 506.5555\n",
      "Epoch [70/200], Loss: 465.5836\n",
      "Epoch [80/200], Loss: 425.6194\n",
      "Epoch [90/200], Loss: 385.7527\n",
      "Epoch [100/200], Loss: 346.1703\n",
      "Epoch [110/200], Loss: 308.1717\n",
      "Epoch [120/200], Loss: 273.5811\n",
      "Epoch [130/200], Loss: 243.4466\n",
      "Epoch [140/200], Loss: 217.2411\n",
      "Epoch [150/200], Loss: 193.0078\n",
      "Epoch [160/200], Loss: 169.3987\n",
      "Epoch [170/200], Loss: 146.3822\n",
      "Epoch [180/200], Loss: 124.3207\n",
      "Epoch [190/200], Loss: 103.6503\n",
      "Epoch [200/200], Loss: 84.9436\n",
      "Epoch [10/200], Loss: 772.7488\n",
      "Epoch [20/200], Loss: 676.2250\n",
      "Epoch [30/200], Loss: 696.7478\n",
      "Epoch [40/200], Loss: 620.7666\n",
      "Epoch [50/200], Loss: 550.5413\n",
      "Epoch [60/200], Loss: 480.8028\n",
      "Epoch [70/200], Loss: 410.7785\n",
      "Epoch [80/200], Loss: 339.4987\n",
      "Epoch [90/200], Loss: 282.3992\n",
      "Epoch [100/200], Loss: 238.1702\n",
      "Epoch [110/200], Loss: 200.4780\n",
      "Epoch [120/200], Loss: 167.5311\n",
      "Epoch [130/200], Loss: 138.8623\n",
      "Epoch [140/200], Loss: 114.3932\n",
      "Epoch [150/200], Loss: 93.7004\n",
      "Epoch [160/200], Loss: 76.4655\n",
      "Epoch [170/200], Loss: 62.4254\n",
      "Epoch [180/200], Loss: 51.3820\n",
      "Epoch [190/200], Loss: 43.0133\n",
      "Epoch [200/200], Loss: 36.8857\n",
      "Epoch [10/200], Loss: 945.7751\n",
      "Epoch [20/200], Loss: 735.6307\n",
      "Epoch [30/200], Loss: 743.7309\n",
      "Epoch [40/200], Loss: 639.3719\n",
      "Epoch [50/200], Loss: 565.2056\n",
      "Epoch [60/200], Loss: 501.8935\n",
      "Epoch [70/200], Loss: 433.3947\n",
      "Epoch [80/200], Loss: 360.5096\n",
      "Epoch [90/200], Loss: 291.0888\n",
      "Epoch [100/200], Loss: 239.3000\n",
      "Epoch [110/200], Loss: 201.0927\n",
      "Epoch [120/200], Loss: 165.7791\n",
      "Epoch [130/200], Loss: 131.9104\n",
      "Epoch [140/200], Loss: 100.2342\n",
      "Epoch [150/200], Loss: 72.8552\n",
      "Epoch [160/200], Loss: 51.9824\n",
      "Epoch [170/200], Loss: 38.4224\n",
      "Epoch [180/200], Loss: 30.9490\n",
      "Epoch [190/200], Loss: 27.3817\n",
      "Epoch [200/200], Loss: 25.8078\n",
      "Epoch [10/200], Loss: 1236.3741\n",
      "Epoch [20/200], Loss: 923.0423\n",
      "Epoch [30/200], Loss: 775.4292\n",
      "Epoch [40/200], Loss: 613.1045\n",
      "Epoch [50/200], Loss: 534.1660\n",
      "Epoch [60/200], Loss: 479.4223\n",
      "Epoch [70/200], Loss: 434.1564\n",
      "Epoch [80/200], Loss: 391.9213\n",
      "Epoch [90/200], Loss: 355.3727\n",
      "Epoch [100/200], Loss: 323.7574\n",
      "Epoch [110/200], Loss: 296.1279\n",
      "Epoch [120/200], Loss: 268.1761\n",
      "Epoch [130/200], Loss: 238.9548\n",
      "Epoch [140/200], Loss: 209.7301\n",
      "Epoch [150/200], Loss: 181.1449\n",
      "Epoch [160/200], Loss: 154.0467\n",
      "Epoch [170/200], Loss: 129.1133\n",
      "Epoch [180/200], Loss: 106.6574\n",
      "Epoch [190/200], Loss: 87.0367\n",
      "Epoch [200/200], Loss: 70.5669\n",
      "Epoch [10/200], Loss: 836.7692\n",
      "Epoch [20/200], Loss: 743.7395\n",
      "Epoch [30/200], Loss: 732.0026\n",
      "Epoch [40/200], Loss: 629.0397\n",
      "Epoch [50/200], Loss: 553.0436\n",
      "Epoch [60/200], Loss: 491.4123\n",
      "Epoch [70/200], Loss: 427.2072\n",
      "Epoch [80/200], Loss: 358.2140\n",
      "Epoch [90/200], Loss: 293.6858\n",
      "Epoch [100/200], Loss: 240.1505\n",
      "Epoch [110/200], Loss: 197.6086\n",
      "Epoch [120/200], Loss: 156.9229\n",
      "Epoch [130/200], Loss: 117.1783\n",
      "Epoch [140/200], Loss: 83.0235\n",
      "Epoch [150/200], Loss: 57.2125\n",
      "Epoch [160/200], Loss: 40.4554\n",
      "Epoch [170/200], Loss: 31.3511\n",
      "Epoch [180/200], Loss: 27.1800\n",
      "Epoch [190/200], Loss: 25.5314\n",
      "Epoch [200/200], Loss: 24.9482\n",
      "Epoch [10/200], Loss: 875.5442\n",
      "Epoch [20/200], Loss: 733.2324\n",
      "Epoch [30/200], Loss: 724.3136\n",
      "Epoch [40/200], Loss: 611.5397\n",
      "Epoch [50/200], Loss: 539.5315\n",
      "Epoch [60/200], Loss: 477.9911\n",
      "Epoch [70/200], Loss: 404.4148\n",
      "Epoch [80/200], Loss: 321.6592\n",
      "Epoch [90/200], Loss: 249.1003\n",
      "Epoch [100/200], Loss: 200.8051\n",
      "Epoch [110/200], Loss: 158.0467\n",
      "Epoch [120/200], Loss: 120.0582\n",
      "Epoch [130/200], Loss: 89.1299\n",
      "Epoch [140/200], Loss: 65.0257\n",
      "Epoch [150/200], Loss: 47.8561\n",
      "Epoch [160/200], Loss: 36.7335\n",
      "Epoch [170/200], Loss: 30.3587\n",
      "Epoch [180/200], Loss: 27.1723\n",
      "Epoch [190/200], Loss: 25.7211\n",
      "Epoch [200/200], Loss: 25.0656\n",
      "Epoch [10/200], Loss: 817.2911\n",
      "Epoch [20/200], Loss: 704.8257\n",
      "Epoch [30/200], Loss: 713.3965\n",
      "Epoch [40/200], Loss: 613.2312\n",
      "Epoch [50/200], Loss: 557.3161\n",
      "Epoch [60/200], Loss: 517.6979\n",
      "Epoch [70/200], Loss: 470.7672\n",
      "Epoch [80/200], Loss: 409.5549\n",
      "Epoch [90/200], Loss: 339.8961\n",
      "Epoch [100/200], Loss: 271.5089\n",
      "Epoch [110/200], Loss: 214.3112\n",
      "Epoch [120/200], Loss: 168.6995\n",
      "Epoch [130/200], Loss: 128.9324\n",
      "Epoch [140/200], Loss: 95.3470\n",
      "Epoch [150/200], Loss: 69.8768\n",
      "Epoch [160/200], Loss: 51.6725\n",
      "Epoch [170/200], Loss: 39.6010\n",
      "Epoch [180/200], Loss: 32.2646\n",
      "Epoch [190/200], Loss: 28.1978\n",
      "Epoch [200/200], Loss: 26.1630\n",
      "Epoch [10/200], Loss: 1504.4154\n",
      "Epoch [20/200], Loss: 1072.6635\n",
      "Epoch [30/200], Loss: 815.0453\n",
      "Epoch [40/200], Loss: 652.8179\n",
      "Epoch [50/200], Loss: 598.7025\n",
      "Epoch [60/200], Loss: 555.1986\n",
      "Epoch [70/200], Loss: 512.3326\n",
      "Epoch [80/200], Loss: 470.1342\n",
      "Epoch [90/200], Loss: 427.1411\n",
      "Epoch [100/200], Loss: 382.6479\n",
      "Epoch [110/200], Loss: 336.7538\n",
      "Epoch [120/200], Loss: 291.4691\n",
      "Epoch [130/200], Loss: 250.2454\n",
      "Epoch [140/200], Loss: 216.1696\n",
      "Epoch [150/200], Loss: 187.9807\n",
      "Epoch [160/200], Loss: 162.6161\n",
      "Epoch [170/200], Loss: 138.8830\n",
      "Epoch [180/200], Loss: 117.1454\n",
      "Epoch [190/200], Loss: 97.8638\n",
      "Epoch [200/200], Loss: 81.2397\n",
      "Epoch [10/200], Loss: 821.2545\n",
      "Epoch [20/200], Loss: 705.3563\n",
      "Epoch [30/200], Loss: 683.3130\n",
      "Epoch [40/200], Loss: 629.8238\n",
      "Epoch [50/200], Loss: 561.4044\n",
      "Epoch [60/200], Loss: 490.9851\n",
      "Epoch [70/200], Loss: 414.3622\n",
      "Epoch [80/200], Loss: 332.0080\n",
      "Epoch [90/200], Loss: 265.2337\n",
      "Epoch [100/200], Loss: 217.4363\n",
      "Epoch [110/200], Loss: 173.8316\n",
      "Epoch [120/200], Loss: 133.3481\n",
      "Epoch [130/200], Loss: 98.6275\n",
      "Epoch [140/200], Loss: 70.0703\n",
      "Epoch [150/200], Loss: 49.2329\n",
      "Epoch [160/200], Loss: 36.0704\n",
      "Epoch [170/200], Loss: 29.2545\n",
      "Epoch [180/200], Loss: 26.3350\n",
      "Epoch [190/200], Loss: 25.2163\n",
      "Epoch [200/200], Loss: 24.8029\n",
      "Epoch [10/200], Loss: 783.3409\n",
      "Epoch [20/200], Loss: 737.9064\n",
      "Epoch [30/200], Loss: 766.2766\n",
      "Epoch [40/200], Loss: 664.8090\n",
      "Epoch [50/200], Loss: 570.8984\n",
      "Epoch [60/200], Loss: 476.8305\n",
      "Epoch [70/200], Loss: 379.6665\n",
      "Epoch [80/200], Loss: 295.4388\n",
      "Epoch [90/200], Loss: 239.8250\n",
      "Epoch [100/200], Loss: 202.2734\n",
      "Epoch [110/200], Loss: 165.3183\n",
      "Epoch [120/200], Loss: 132.7783\n",
      "Epoch [130/200], Loss: 104.9665\n",
      "Epoch [140/200], Loss: 81.9186\n",
      "Epoch [150/200], Loss: 63.7824\n",
      "Epoch [160/200], Loss: 50.2082\n",
      "Epoch [170/200], Loss: 40.5988\n",
      "Epoch [180/200], Loss: 34.1667\n",
      "Epoch [190/200], Loss: 30.0809\n",
      "Epoch [200/200], Loss: 27.6213\n",
      "Epoch [10/200], Loss: 1647.3365\n",
      "Epoch [20/200], Loss: 1115.3295\n",
      "Epoch [30/200], Loss: 796.0642\n",
      "Epoch [40/200], Loss: 643.9842\n",
      "Epoch [50/200], Loss: 602.0466\n",
      "Epoch [60/200], Loss: 561.8222\n",
      "Epoch [70/200], Loss: 517.5742\n",
      "Epoch [80/200], Loss: 469.8857\n",
      "Epoch [90/200], Loss: 416.6519\n",
      "Epoch [100/200], Loss: 360.9555\n",
      "Epoch [110/200], Loss: 308.1559\n",
      "Epoch [120/200], Loss: 264.1295\n",
      "Epoch [130/200], Loss: 229.9017\n",
      "Epoch [140/200], Loss: 198.4098\n",
      "Epoch [150/200], Loss: 167.1039\n",
      "Epoch [160/200], Loss: 137.2060\n",
      "Epoch [170/200], Loss: 109.9338\n",
      "Epoch [180/200], Loss: 86.0518\n",
      "Epoch [190/200], Loss: 66.2412\n",
      "Epoch [200/200], Loss: 51.0329\n",
      "Epoch [10/200], Loss: 1064.0500\n",
      "Epoch [20/200], Loss: 815.5134\n",
      "Epoch [30/200], Loss: 757.1467\n",
      "Epoch [40/200], Loss: 634.6304\n",
      "Epoch [50/200], Loss: 576.8211\n",
      "Epoch [60/200], Loss: 539.3541\n",
      "Epoch [70/200], Loss: 503.9340\n",
      "Epoch [80/200], Loss: 463.8119\n",
      "Epoch [90/200], Loss: 414.5918\n",
      "Epoch [100/200], Loss: 357.8859\n",
      "Epoch [110/200], Loss: 301.3714\n",
      "Epoch [120/200], Loss: 256.2247\n",
      "Epoch [130/200], Loss: 220.1100\n",
      "Epoch [140/200], Loss: 185.7345\n",
      "Epoch [150/200], Loss: 153.9284\n",
      "Epoch [160/200], Loss: 126.0453\n",
      "Epoch [170/200], Loss: 101.8924\n",
      "Epoch [180/200], Loss: 81.8693\n",
      "Epoch [190/200], Loss: 65.7410\n",
      "Epoch [200/200], Loss: 53.1454\n",
      "Epoch [10/200], Loss: 1162.6716\n",
      "Epoch [20/200], Loss: 907.1666\n",
      "Epoch [30/200], Loss: 819.9269\n",
      "Epoch [40/200], Loss: 680.5590\n",
      "Epoch [50/200], Loss: 615.6251\n",
      "Epoch [60/200], Loss: 556.1715\n",
      "Epoch [70/200], Loss: 490.3245\n",
      "Epoch [80/200], Loss: 420.8618\n",
      "Epoch [90/200], Loss: 349.8209\n",
      "Epoch [100/200], Loss: 287.9999\n",
      "Epoch [110/200], Loss: 240.3108\n",
      "Epoch [120/200], Loss: 199.3939\n",
      "Epoch [130/200], Loss: 163.5645\n",
      "Epoch [140/200], Loss: 132.3304\n",
      "Epoch [150/200], Loss: 105.3966\n",
      "Epoch [160/200], Loss: 82.6335\n",
      "Epoch [170/200], Loss: 63.2522\n",
      "Epoch [180/200], Loss: 47.1191\n",
      "Epoch [190/200], Loss: 35.5477\n",
      "Epoch [200/200], Loss: 29.2081\n",
      "Epoch [10/200], Loss: 860.5186\n",
      "Epoch [20/200], Loss: 733.4090\n",
      "Epoch [30/200], Loss: 722.7139\n",
      "Epoch [40/200], Loss: 633.3063\n",
      "Epoch [50/200], Loss: 549.8240\n",
      "Epoch [60/200], Loss: 470.1051\n",
      "Epoch [70/200], Loss: 391.4488\n",
      "Epoch [80/200], Loss: 318.8192\n",
      "Epoch [90/200], Loss: 264.6190\n",
      "Epoch [100/200], Loss: 224.3812\n",
      "Epoch [110/200], Loss: 187.1892\n",
      "Epoch [120/200], Loss: 152.6567\n",
      "Epoch [130/200], Loss: 121.6495\n",
      "Epoch [140/200], Loss: 94.2100\n",
      "Epoch [150/200], Loss: 71.9616\n",
      "Epoch [160/200], Loss: 55.3347\n",
      "Epoch [170/200], Loss: 43.7079\n",
      "Epoch [180/200], Loss: 36.0428\n",
      "Epoch [190/200], Loss: 31.3044\n",
      "Epoch [200/200], Loss: 28.5224\n",
      "Epoch [10/200], Loss: 1471.0912\n",
      "Epoch [20/200], Loss: 981.1329\n",
      "Epoch [30/200], Loss: 841.2061\n",
      "Epoch [40/200], Loss: 680.5905\n",
      "Epoch [50/200], Loss: 592.8096\n",
      "Epoch [60/200], Loss: 512.4676\n",
      "Epoch [70/200], Loss: 429.8703\n",
      "Epoch [80/200], Loss: 349.0005\n",
      "Epoch [90/200], Loss: 285.7339\n",
      "Epoch [100/200], Loss: 244.2853\n",
      "Epoch [110/200], Loss: 214.0840\n",
      "Epoch [120/200], Loss: 186.0607\n",
      "Epoch [130/200], Loss: 160.2447\n",
      "Epoch [140/200], Loss: 136.9044\n",
      "Epoch [150/200], Loss: 115.2266\n",
      "Epoch [160/200], Loss: 94.7209\n",
      "Epoch [170/200], Loss: 76.3185\n",
      "Epoch [180/200], Loss: 60.8721\n",
      "Epoch [190/200], Loss: 48.8040\n",
      "Epoch [200/200], Loss: 39.9353\n",
      "Epoch [10/200], Loss: 900.3951\n",
      "Epoch [20/200], Loss: 778.3367\n",
      "Epoch [30/200], Loss: 756.3423\n",
      "Epoch [40/200], Loss: 637.1352\n",
      "Epoch [50/200], Loss: 553.9886\n",
      "Epoch [60/200], Loss: 493.3681\n",
      "Epoch [70/200], Loss: 432.5849\n",
      "Epoch [80/200], Loss: 362.7412\n",
      "Epoch [90/200], Loss: 293.5583\n",
      "Epoch [100/200], Loss: 241.2008\n",
      "Epoch [110/200], Loss: 202.3819\n",
      "Epoch [120/200], Loss: 166.9037\n",
      "Epoch [130/200], Loss: 135.9026\n",
      "Epoch [140/200], Loss: 108.9678\n",
      "Epoch [150/200], Loss: 86.4049\n",
      "Epoch [160/200], Loss: 67.9755\n",
      "Epoch [170/200], Loss: 53.6565\n",
      "Epoch [180/200], Loss: 43.1353\n",
      "Epoch [190/200], Loss: 35.8883\n",
      "Epoch [200/200], Loss: 31.2303\n",
      "Epoch [10/200], Loss: 1004.3647\n",
      "Epoch [20/200], Loss: 871.7666\n",
      "Epoch [30/200], Loss: 812.6868\n",
      "Epoch [40/200], Loss: 677.3764\n",
      "Epoch [50/200], Loss: 596.0217\n",
      "Epoch [60/200], Loss: 517.1694\n",
      "Epoch [70/200], Loss: 428.2529\n",
      "Epoch [80/200], Loss: 330.8215\n",
      "Epoch [90/200], Loss: 256.0131\n",
      "Epoch [100/200], Loss: 210.1964\n",
      "Epoch [110/200], Loss: 165.9370\n",
      "Epoch [120/200], Loss: 126.1102\n",
      "Epoch [130/200], Loss: 92.3619\n",
      "Epoch [140/200], Loss: 65.7393\n",
      "Epoch [150/200], Loss: 47.2475\n",
      "Epoch [160/200], Loss: 35.6848\n",
      "Epoch [170/200], Loss: 29.5381\n",
      "Epoch [180/200], Loss: 26.7850\n",
      "Epoch [190/200], Loss: 25.5706\n",
      "Epoch [200/200], Loss: 25.0503\n",
      "Epoch [10/200], Loss: 924.3323\n",
      "Epoch [20/200], Loss: 808.7448\n",
      "Epoch [30/200], Loss: 803.1038\n",
      "Epoch [40/200], Loss: 672.0273\n",
      "Epoch [50/200], Loss: 571.6123\n",
      "Epoch [60/200], Loss: 483.9777\n",
      "Epoch [70/200], Loss: 408.5857\n",
      "Epoch [80/200], Loss: 347.5310\n",
      "Epoch [90/200], Loss: 297.2473\n",
      "Epoch [100/200], Loss: 258.0282\n",
      "Epoch [110/200], Loss: 223.3462\n",
      "Epoch [120/200], Loss: 190.7266\n",
      "Epoch [130/200], Loss: 160.4062\n",
      "Epoch [140/200], Loss: 131.3330\n",
      "Epoch [150/200], Loss: 102.4921\n",
      "Epoch [160/200], Loss: 77.1844\n",
      "Epoch [170/200], Loss: 57.7231\n",
      "Epoch [180/200], Loss: 44.2115\n",
      "Epoch [190/200], Loss: 35.4331\n",
      "Epoch [200/200], Loss: 30.1871\n",
      "Epoch [10/200], Loss: 1059.2704\n",
      "Epoch [20/200], Loss: 771.2480\n",
      "Epoch [30/200], Loss: 751.4153\n",
      "Epoch [40/200], Loss: 636.0283\n",
      "Epoch [50/200], Loss: 569.5692\n",
      "Epoch [60/200], Loss: 518.2046\n",
      "Epoch [70/200], Loss: 468.4369\n",
      "Epoch [80/200], Loss: 416.9000\n",
      "Epoch [90/200], Loss: 359.2154\n",
      "Epoch [100/200], Loss: 300.4764\n",
      "Epoch [110/200], Loss: 249.1590\n",
      "Epoch [120/200], Loss: 207.0775\n",
      "Epoch [130/200], Loss: 170.7659\n",
      "Epoch [140/200], Loss: 138.8048\n",
      "Epoch [150/200], Loss: 111.5825\n",
      "Epoch [160/200], Loss: 88.6766\n",
      "Epoch [170/200], Loss: 68.0103\n",
      "Epoch [180/200], Loss: 50.5139\n",
      "Epoch [190/200], Loss: 37.9001\n",
      "Epoch [200/200], Loss: 30.4532\n",
      "Epoch [10/200], Loss: 801.1667\n",
      "Epoch [20/200], Loss: 731.8717\n",
      "Epoch [30/200], Loss: 725.7047\n",
      "Epoch [40/200], Loss: 618.3026\n",
      "Epoch [50/200], Loss: 543.4520\n",
      "Epoch [60/200], Loss: 489.9203\n",
      "Epoch [70/200], Loss: 436.4554\n",
      "Epoch [80/200], Loss: 380.4073\n",
      "Epoch [90/200], Loss: 324.7480\n",
      "Epoch [100/200], Loss: 274.8015\n",
      "Epoch [110/200], Loss: 234.7945\n",
      "Epoch [120/200], Loss: 201.9452\n",
      "Epoch [130/200], Loss: 166.3174\n",
      "Epoch [140/200], Loss: 129.1556\n",
      "Epoch [150/200], Loss: 93.6720\n",
      "Epoch [160/200], Loss: 64.0486\n",
      "Epoch [170/200], Loss: 43.0936\n",
      "Epoch [180/200], Loss: 31.4520\n",
      "Epoch [190/200], Loss: 26.9650\n",
      "Epoch [200/200], Loss: 25.5955\n",
      "Epoch [10/200], Loss: 1164.2977\n",
      "Epoch [20/200], Loss: 896.8523\n",
      "Epoch [30/200], Loss: 806.7727\n",
      "Epoch [40/200], Loss: 666.4713\n",
      "Epoch [50/200], Loss: 572.3582\n",
      "Epoch [60/200], Loss: 504.7619\n",
      "Epoch [70/200], Loss: 438.4135\n",
      "Epoch [80/200], Loss: 367.3583\n",
      "Epoch [90/200], Loss: 299.6140\n",
      "Epoch [100/200], Loss: 242.1405\n",
      "Epoch [110/200], Loss: 198.5709\n",
      "Epoch [120/200], Loss: 161.7195\n",
      "Epoch [130/200], Loss: 127.5739\n",
      "Epoch [140/200], Loss: 98.2006\n",
      "Epoch [150/200], Loss: 74.7193\n",
      "Epoch [160/200], Loss: 57.2457\n",
      "Epoch [170/200], Loss: 45.0750\n",
      "Epoch [180/200], Loss: 37.0609\n",
      "Epoch [190/200], Loss: 32.0310\n",
      "Epoch [200/200], Loss: 28.9891\n",
      "Epoch [10/200], Loss: 998.0469\n",
      "Epoch [20/200], Loss: 790.2523\n",
      "Epoch [30/200], Loss: 729.0383\n",
      "Epoch [40/200], Loss: 622.3630\n",
      "Epoch [50/200], Loss: 549.3859\n",
      "Epoch [60/200], Loss: 494.9198\n",
      "Epoch [70/200], Loss: 443.0809\n",
      "Epoch [80/200], Loss: 389.4494\n",
      "Epoch [90/200], Loss: 335.3913\n",
      "Epoch [100/200], Loss: 284.7519\n",
      "Epoch [110/200], Loss: 237.7424\n",
      "Epoch [120/200], Loss: 192.2994\n",
      "Epoch [130/200], Loss: 147.1250\n",
      "Epoch [140/200], Loss: 105.6642\n",
      "Epoch [150/200], Loss: 73.1240\n",
      "Epoch [160/200], Loss: 49.9531\n",
      "Epoch [170/200], Loss: 35.8627\n",
      "Epoch [180/200], Loss: 29.0368\n",
      "Epoch [190/200], Loss: 26.2422\n",
      "Epoch [200/200], Loss: 25.1620\n",
      "Epoch [10/200], Loss: 1082.0448\n",
      "Epoch [20/200], Loss: 824.4445\n",
      "Epoch [30/200], Loss: 744.7131\n",
      "Epoch [40/200], Loss: 605.4418\n",
      "Epoch [50/200], Loss: 544.8778\n",
      "Epoch [60/200], Loss: 486.5249\n",
      "Epoch [70/200], Loss: 422.6436\n",
      "Epoch [80/200], Loss: 359.7520\n",
      "Epoch [90/200], Loss: 305.9526\n",
      "Epoch [100/200], Loss: 261.6318\n",
      "Epoch [110/200], Loss: 218.3989\n",
      "Epoch [120/200], Loss: 175.5367\n",
      "Epoch [130/200], Loss: 135.4166\n",
      "Epoch [140/200], Loss: 99.8646\n",
      "Epoch [150/200], Loss: 71.2731\n",
      "Epoch [160/200], Loss: 51.0596\n",
      "Epoch [170/200], Loss: 37.9984\n",
      "Epoch [180/200], Loss: 30.3555\n",
      "Epoch [190/200], Loss: 26.5503\n",
      "Epoch [200/200], Loss: 25.1769\n",
      "Epoch [10/200], Loss: 1622.1094\n",
      "Epoch [20/200], Loss: 1089.1273\n",
      "Epoch [30/200], Loss: 807.1486\n",
      "Epoch [40/200], Loss: 659.7407\n",
      "Epoch [50/200], Loss: 620.7413\n",
      "Epoch [60/200], Loss: 578.7705\n",
      "Epoch [70/200], Loss: 519.6354\n",
      "Epoch [80/200], Loss: 448.6833\n",
      "Epoch [90/200], Loss: 374.3087\n",
      "Epoch [100/200], Loss: 311.0851\n",
      "Epoch [110/200], Loss: 267.6866\n",
      "Epoch [120/200], Loss: 235.4518\n",
      "Epoch [130/200], Loss: 203.1618\n",
      "Epoch [140/200], Loss: 171.6214\n",
      "Epoch [150/200], Loss: 141.5337\n",
      "Epoch [160/200], Loss: 114.0824\n",
      "Epoch [170/200], Loss: 90.2697\n",
      "Epoch [180/200], Loss: 70.6479\n",
      "Epoch [190/200], Loss: 55.3668\n",
      "Epoch [200/200], Loss: 44.1532\n",
      "Epoch [10/200], Loss: 1141.6293\n",
      "Epoch [20/200], Loss: 870.8391\n",
      "Epoch [30/200], Loss: 819.9289\n",
      "Epoch [40/200], Loss: 687.8998\n",
      "Epoch [50/200], Loss: 618.8206\n",
      "Epoch [60/200], Loss: 561.4443\n",
      "Epoch [70/200], Loss: 500.6563\n",
      "Epoch [80/200], Loss: 433.7231\n",
      "Epoch [90/200], Loss: 368.4077\n",
      "Epoch [100/200], Loss: 305.0518\n",
      "Epoch [110/200], Loss: 246.2861\n",
      "Epoch [120/200], Loss: 196.3567\n",
      "Epoch [130/200], Loss: 152.5426\n",
      "Epoch [140/200], Loss: 113.4408\n",
      "Epoch [150/200], Loss: 81.6130\n",
      "Epoch [160/200], Loss: 57.0888\n",
      "Epoch [170/200], Loss: 39.9450\n",
      "Epoch [180/200], Loss: 30.0582\n",
      "Epoch [190/200], Loss: 26.2444\n",
      "Epoch [200/200], Loss: 25.2361\n",
      "Epoch [10/200], Loss: 740.1923\n",
      "Epoch [20/200], Loss: 685.0402\n",
      "Epoch [30/200], Loss: 722.3270\n",
      "Epoch [40/200], Loss: 651.3881\n",
      "Epoch [50/200], Loss: 586.1406\n",
      "Epoch [60/200], Loss: 525.0904\n",
      "Epoch [70/200], Loss: 458.6920\n",
      "Epoch [80/200], Loss: 382.7484\n",
      "Epoch [90/200], Loss: 304.8101\n",
      "Epoch [100/200], Loss: 243.9350\n",
      "Epoch [110/200], Loss: 196.4622\n",
      "Epoch [120/200], Loss: 151.5544\n",
      "Epoch [130/200], Loss: 112.4048\n",
      "Epoch [140/200], Loss: 80.9204\n",
      "Epoch [150/200], Loss: 56.9440\n",
      "Epoch [160/200], Loss: 41.3155\n",
      "Epoch [170/200], Loss: 32.2113\n",
      "Epoch [180/200], Loss: 27.9451\n",
      "Epoch [190/200], Loss: 25.8364\n",
      "Epoch [200/200], Loss: 25.1521\n",
      "Epoch [10/200], Loss: 1407.8929\n",
      "Epoch [20/200], Loss: 968.5981\n",
      "Epoch [30/200], Loss: 799.9570\n",
      "Epoch [40/200], Loss: 648.7694\n",
      "Epoch [50/200], Loss: 575.2842\n",
      "Epoch [60/200], Loss: 520.2231\n",
      "Epoch [70/200], Loss: 471.5665\n",
      "Epoch [80/200], Loss: 423.1547\n",
      "Epoch [90/200], Loss: 376.5318\n",
      "Epoch [100/200], Loss: 332.4969\n",
      "Epoch [110/200], Loss: 290.1093\n",
      "Epoch [120/200], Loss: 248.1907\n",
      "Epoch [130/200], Loss: 209.3375\n",
      "Epoch [140/200], Loss: 172.2362\n",
      "Epoch [150/200], Loss: 138.7535\n",
      "Epoch [160/200], Loss: 109.3171\n",
      "Epoch [170/200], Loss: 84.3763\n",
      "Epoch [180/200], Loss: 64.8705\n",
      "Epoch [190/200], Loss: 50.5705\n",
      "Epoch [200/200], Loss: 40.6971\n",
      "Epoch [10/200], Loss: 818.3771\n",
      "Epoch [20/200], Loss: 733.0388\n",
      "Epoch [30/200], Loss: 717.4434\n",
      "Epoch [40/200], Loss: 635.2858\n",
      "Epoch [50/200], Loss: 559.5452\n",
      "Epoch [60/200], Loss: 502.6659\n",
      "Epoch [70/200], Loss: 448.7524\n",
      "Epoch [80/200], Loss: 395.5631\n",
      "Epoch [90/200], Loss: 342.8644\n",
      "Epoch [100/200], Loss: 289.7527\n",
      "Epoch [110/200], Loss: 238.7757\n",
      "Epoch [120/200], Loss: 193.7000\n",
      "Epoch [130/200], Loss: 154.1433\n",
      "Epoch [140/200], Loss: 119.2780\n",
      "Epoch [150/200], Loss: 90.1716\n",
      "Epoch [160/200], Loss: 67.5416\n",
      "Epoch [170/200], Loss: 51.2109\n",
      "Epoch [180/200], Loss: 40.3654\n",
      "Epoch [190/200], Loss: 33.6664\n",
      "Epoch [200/200], Loss: 29.7002\n",
      "Epoch [10/200], Loss: 1202.4532\n",
      "Epoch [20/200], Loss: 960.7914\n",
      "Epoch [30/200], Loss: 810.8234\n",
      "Epoch [40/200], Loss: 660.4658\n",
      "Epoch [50/200], Loss: 593.7184\n",
      "Epoch [60/200], Loss: 540.3625\n",
      "Epoch [70/200], Loss: 487.7199\n",
      "Epoch [80/200], Loss: 427.5762\n",
      "Epoch [90/200], Loss: 355.6755\n",
      "Epoch [100/200], Loss: 283.8569\n",
      "Epoch [110/200], Loss: 229.1315\n",
      "Epoch [120/200], Loss: 183.7945\n",
      "Epoch [130/200], Loss: 139.2997\n",
      "Epoch [140/200], Loss: 100.4876\n",
      "Epoch [150/200], Loss: 69.8482\n",
      "Epoch [160/200], Loss: 49.1809\n",
      "Epoch [170/200], Loss: 36.8898\n",
      "Epoch [180/200], Loss: 30.3701\n",
      "Epoch [190/200], Loss: 27.2013\n",
      "Epoch [200/200], Loss: 25.7501\n",
      "Epoch [10/200], Loss: 736.4017\n",
      "Epoch [20/200], Loss: 668.5740\n",
      "Epoch [30/200], Loss: 671.8279\n",
      "Epoch [40/200], Loss: 608.0971\n",
      "Epoch [50/200], Loss: 541.8162\n",
      "Epoch [60/200], Loss: 476.6315\n",
      "Epoch [70/200], Loss: 409.4272\n",
      "Epoch [80/200], Loss: 335.0109\n",
      "Epoch [90/200], Loss: 264.0991\n",
      "Epoch [100/200], Loss: 210.6784\n",
      "Epoch [110/200], Loss: 164.1314\n",
      "Epoch [120/200], Loss: 122.3325\n",
      "Epoch [130/200], Loss: 88.7816\n",
      "Epoch [140/200], Loss: 63.0437\n",
      "Epoch [150/200], Loss: 44.7705\n",
      "Epoch [160/200], Loss: 33.6081\n",
      "Epoch [170/200], Loss: 28.1380\n",
      "Epoch [180/200], Loss: 25.9206\n",
      "Epoch [190/200], Loss: 25.1728\n",
      "Epoch [200/200], Loss: 24.8364\n",
      "Epoch [10/200], Loss: 965.9406\n",
      "Epoch [20/200], Loss: 803.0814\n",
      "Epoch [30/200], Loss: 778.4501\n",
      "Epoch [40/200], Loss: 660.6275\n",
      "Epoch [50/200], Loss: 590.0774\n",
      "Epoch [60/200], Loss: 540.0613\n",
      "Epoch [70/200], Loss: 491.4789\n",
      "Epoch [80/200], Loss: 435.0380\n",
      "Epoch [90/200], Loss: 361.3482\n",
      "Epoch [100/200], Loss: 274.7779\n",
      "Epoch [110/200], Loss: 202.1136\n",
      "Epoch [120/200], Loss: 144.0054\n",
      "Epoch [130/200], Loss: 94.8875\n",
      "Epoch [140/200], Loss: 61.8305\n",
      "Epoch [150/200], Loss: 42.3166\n",
      "Epoch [160/200], Loss: 32.4227\n",
      "Epoch [170/200], Loss: 27.8211\n",
      "Epoch [180/200], Loss: 25.7993\n",
      "Epoch [190/200], Loss: 25.0127\n",
      "Epoch [200/200], Loss: 24.7458\n",
      "Epoch [10/200], Loss: 1690.0425\n",
      "Epoch [20/200], Loss: 1074.5059\n",
      "Epoch [30/200], Loss: 765.8482\n",
      "Epoch [40/200], Loss: 618.5809\n",
      "Epoch [50/200], Loss: 562.2104\n",
      "Epoch [60/200], Loss: 496.0344\n",
      "Epoch [70/200], Loss: 413.2655\n",
      "Epoch [80/200], Loss: 327.3288\n",
      "Epoch [90/200], Loss: 265.2217\n",
      "Epoch [100/200], Loss: 217.9238\n",
      "Epoch [110/200], Loss: 171.7193\n",
      "Epoch [120/200], Loss: 131.4814\n",
      "Epoch [130/200], Loss: 96.4710\n",
      "Epoch [140/200], Loss: 67.5279\n",
      "Epoch [150/200], Loss: 46.7554\n",
      "Epoch [160/200], Loss: 34.2960\n",
      "Epoch [170/200], Loss: 28.5505\n",
      "Epoch [180/200], Loss: 26.0078\n",
      "Epoch [190/200], Loss: 25.1491\n",
      "Epoch [200/200], Loss: 24.7577\n",
      "Epoch [10/200], Loss: 818.1976\n",
      "Epoch [20/200], Loss: 729.2651\n",
      "Epoch [30/200], Loss: 740.2559\n",
      "Epoch [40/200], Loss: 643.3804\n",
      "Epoch [50/200], Loss: 574.9061\n",
      "Epoch [60/200], Loss: 522.6661\n",
      "Epoch [70/200], Loss: 472.0668\n",
      "Epoch [80/200], Loss: 417.6579\n",
      "Epoch [90/200], Loss: 360.6577\n",
      "Epoch [100/200], Loss: 304.9885\n",
      "Epoch [110/200], Loss: 255.7787\n",
      "Epoch [120/200], Loss: 214.4292\n",
      "Epoch [130/200], Loss: 177.3728\n",
      "Epoch [140/200], Loss: 142.8779\n",
      "Epoch [150/200], Loss: 109.8975\n",
      "Epoch [160/200], Loss: 81.7106\n",
      "Epoch [170/200], Loss: 59.5919\n",
      "Epoch [180/200], Loss: 43.1075\n",
      "Epoch [190/200], Loss: 32.7115\n",
      "Epoch [200/200], Loss: 27.5646\n",
      "Epoch [10/200], Loss: 1334.7502\n",
      "Epoch [20/200], Loss: 991.6779\n",
      "Epoch [30/200], Loss: 782.4688\n",
      "Epoch [40/200], Loss: 642.1112\n",
      "Epoch [50/200], Loss: 582.3644\n",
      "Epoch [60/200], Loss: 525.7490\n",
      "Epoch [70/200], Loss: 461.9702\n",
      "Epoch [80/200], Loss: 391.1939\n",
      "Epoch [90/200], Loss: 322.4754\n",
      "Epoch [100/200], Loss: 263.9033\n",
      "Epoch [110/200], Loss: 210.1344\n",
      "Epoch [120/200], Loss: 158.9126\n",
      "Epoch [130/200], Loss: 115.3952\n",
      "Epoch [140/200], Loss: 80.0412\n",
      "Epoch [150/200], Loss: 54.4883\n",
      "Epoch [160/200], Loss: 38.6614\n",
      "Epoch [170/200], Loss: 30.6128\n",
      "Epoch [180/200], Loss: 27.2618\n",
      "Epoch [190/200], Loss: 25.7550\n",
      "Epoch [200/200], Loss: 25.0770\n",
      "Epoch [10/200], Loss: 1164.1512\n",
      "Epoch [20/200], Loss: 912.2645\n",
      "Epoch [30/200], Loss: 789.6063\n",
      "Epoch [40/200], Loss: 636.9619\n",
      "Epoch [50/200], Loss: 575.9838\n",
      "Epoch [60/200], Loss: 527.2486\n",
      "Epoch [70/200], Loss: 474.2530\n",
      "Epoch [80/200], Loss: 415.7230\n",
      "Epoch [90/200], Loss: 353.4778\n",
      "Epoch [100/200], Loss: 294.1253\n",
      "Epoch [110/200], Loss: 241.0880\n",
      "Epoch [120/200], Loss: 194.3838\n",
      "Epoch [130/200], Loss: 149.9981\n",
      "Epoch [140/200], Loss: 108.9498\n",
      "Epoch [150/200], Loss: 75.9338\n",
      "Epoch [160/200], Loss: 52.8867\n",
      "Epoch [170/200], Loss: 38.8624\n",
      "Epoch [180/200], Loss: 31.0660\n",
      "Epoch [190/200], Loss: 27.1662\n",
      "Epoch [200/200], Loss: 25.5266\n",
      "Epoch [10/200], Loss: 1343.9395\n",
      "Epoch [20/200], Loss: 931.1250\n",
      "Epoch [30/200], Loss: 801.8856\n",
      "Epoch [40/200], Loss: 634.9518\n",
      "Epoch [50/200], Loss: 549.4473\n",
      "Epoch [60/200], Loss: 477.0387\n",
      "Epoch [70/200], Loss: 404.0368\n",
      "Epoch [80/200], Loss: 331.6530\n",
      "Epoch [90/200], Loss: 268.6092\n",
      "Epoch [100/200], Loss: 223.8693\n",
      "Epoch [110/200], Loss: 189.1322\n",
      "Epoch [120/200], Loss: 156.1507\n",
      "Epoch [130/200], Loss: 126.0255\n",
      "Epoch [140/200], Loss: 99.7997\n",
      "Epoch [150/200], Loss: 77.5437\n",
      "Epoch [160/200], Loss: 59.9397\n",
      "Epoch [170/200], Loss: 46.9579\n",
      "Epoch [180/200], Loss: 37.9718\n",
      "Epoch [190/200], Loss: 32.1441\n",
      "Epoch [200/200], Loss: 28.6543\n",
      "Epoch [10/200], Loss: 1365.9894\n",
      "Epoch [20/200], Loss: 984.5752\n",
      "Epoch [30/200], Loss: 802.6414\n",
      "Epoch [40/200], Loss: 635.3327\n",
      "Epoch [50/200], Loss: 556.4814\n",
      "Epoch [60/200], Loss: 480.3101\n",
      "Epoch [70/200], Loss: 392.7504\n",
      "Epoch [80/200], Loss: 306.9090\n",
      "Epoch [90/200], Loss: 245.9414\n",
      "Epoch [100/200], Loss: 205.5843\n",
      "Epoch [110/200], Loss: 166.9571\n",
      "Epoch [120/200], Loss: 128.4659\n",
      "Epoch [130/200], Loss: 92.5680\n",
      "Epoch [140/200], Loss: 60.9337\n",
      "Epoch [150/200], Loss: 36.5045\n",
      "Epoch [160/200], Loss: 20.1355\n",
      "Epoch [170/200], Loss: 10.3353\n",
      "Epoch [180/200], Loss: 4.9859\n",
      "Epoch [190/200], Loss: 2.2975\n",
      "Epoch [200/200], Loss: 1.0221\n",
      "Epoch [10/200], Loss: 910.6897\n",
      "Epoch [20/200], Loss: 707.0217\n",
      "Epoch [30/200], Loss: 720.5561\n",
      "Epoch [40/200], Loss: 619.2445\n",
      "Epoch [50/200], Loss: 552.2176\n",
      "Epoch [60/200], Loss: 499.1647\n",
      "Epoch [70/200], Loss: 446.3398\n",
      "Epoch [80/200], Loss: 383.4480\n",
      "Epoch [90/200], Loss: 317.1956\n",
      "Epoch [100/200], Loss: 252.9017\n",
      "Epoch [110/200], Loss: 199.2816\n",
      "Epoch [120/200], Loss: 154.9124\n",
      "Epoch [130/200], Loss: 114.0278\n",
      "Epoch [140/200], Loss: 77.5398\n",
      "Epoch [150/200], Loss: 48.4575\n",
      "Epoch [160/200], Loss: 27.7436\n",
      "Epoch [170/200], Loss: 14.5889\n",
      "Epoch [180/200], Loss: 7.1153\n",
      "Epoch [190/200], Loss: 3.2474\n",
      "Epoch [200/200], Loss: 1.3956\n",
      "Epoch [10/200], Loss: 1130.3551\n",
      "Epoch [20/200], Loss: 903.6794\n",
      "Epoch [30/200], Loss: 842.9356\n",
      "Epoch [40/200], Loss: 707.7461\n",
      "Epoch [50/200], Loss: 633.3361\n",
      "Epoch [60/200], Loss: 578.8270\n",
      "Epoch [70/200], Loss: 524.2024\n",
      "Epoch [80/200], Loss: 463.1767\n",
      "Epoch [90/200], Loss: 397.6158\n",
      "Epoch [100/200], Loss: 335.4268\n",
      "Epoch [110/200], Loss: 285.0518\n",
      "Epoch [120/200], Loss: 245.6688\n",
      "Epoch [130/200], Loss: 209.8443\n",
      "Epoch [140/200], Loss: 173.7243\n",
      "Epoch [150/200], Loss: 140.1720\n",
      "Epoch [160/200], Loss: 110.6338\n",
      "Epoch [170/200], Loss: 85.8881\n",
      "Epoch [180/200], Loss: 66.5019\n",
      "Epoch [190/200], Loss: 52.1734\n",
      "Epoch [200/200], Loss: 42.0906\n",
      "Epoch [10/200], Loss: 1749.7737\n",
      "Epoch [20/200], Loss: 1217.6089\n",
      "Epoch [30/200], Loss: 857.2978\n",
      "Epoch [40/200], Loss: 712.3969\n",
      "Epoch [50/200], Loss: 635.3945\n",
      "Epoch [60/200], Loss: 526.7656\n",
      "Epoch [70/200], Loss: 413.4489\n",
      "Epoch [80/200], Loss: 324.9461\n",
      "Epoch [90/200], Loss: 258.7833\n",
      "Epoch [100/200], Loss: 202.6861\n",
      "Epoch [110/200], Loss: 149.0076\n",
      "Epoch [120/200], Loss: 105.5723\n",
      "Epoch [130/200], Loss: 72.2127\n",
      "Epoch [140/200], Loss: 48.8930\n",
      "Epoch [150/200], Loss: 35.3022\n",
      "Epoch [160/200], Loss: 28.5125\n",
      "Epoch [170/200], Loss: 25.7351\n",
      "Epoch [180/200], Loss: 24.8783\n",
      "Epoch [190/200], Loss: 24.6751\n",
      "Epoch [200/200], Loss: 24.6261\n",
      "Epoch [10/200], Loss: 1723.3746\n",
      "Epoch [20/200], Loss: 1196.6721\n",
      "Epoch [30/200], Loss: 871.4261\n",
      "Epoch [40/200], Loss: 701.2327\n",
      "Epoch [50/200], Loss: 642.4749\n",
      "Epoch [60/200], Loss: 586.5024\n",
      "Epoch [70/200], Loss: 523.6752\n",
      "Epoch [80/200], Loss: 451.8738\n",
      "Epoch [90/200], Loss: 375.9754\n",
      "Epoch [100/200], Loss: 306.3908\n",
      "Epoch [110/200], Loss: 253.8824\n",
      "Epoch [120/200], Loss: 216.5723\n",
      "Epoch [130/200], Loss: 183.5319\n",
      "Epoch [140/200], Loss: 152.4704\n",
      "Epoch [150/200], Loss: 124.9709\n",
      "Epoch [160/200], Loss: 101.1529\n",
      "Epoch [170/200], Loss: 81.1912\n",
      "Epoch [180/200], Loss: 65.0408\n",
      "Epoch [190/200], Loss: 52.4901\n",
      "Epoch [200/200], Loss: 43.1359\n",
      "Epoch [10/200], Loss: 1305.2498\n",
      "Epoch [20/200], Loss: 950.7389\n",
      "Epoch [30/200], Loss: 816.1927\n",
      "Epoch [40/200], Loss: 666.4586\n",
      "Epoch [50/200], Loss: 590.5977\n",
      "Epoch [60/200], Loss: 518.3176\n",
      "Epoch [70/200], Loss: 440.4017\n",
      "Epoch [80/200], Loss: 361.9671\n",
      "Epoch [90/200], Loss: 304.4182\n",
      "Epoch [100/200], Loss: 267.8489\n",
      "Epoch [110/200], Loss: 238.9640\n",
      "Epoch [120/200], Loss: 212.3836\n",
      "Epoch [130/200], Loss: 188.6448\n",
      "Epoch [140/200], Loss: 167.0212\n",
      "Epoch [150/200], Loss: 147.1473\n",
      "Epoch [160/200], Loss: 128.9123\n",
      "Epoch [170/200], Loss: 112.1623\n",
      "Epoch [180/200], Loss: 96.9160\n",
      "Epoch [190/200], Loss: 83.2073\n",
      "Epoch [200/200], Loss: 71.1451\n",
      "Epoch [10/200], Loss: 910.4741\n",
      "Epoch [20/200], Loss: 812.1456\n",
      "Epoch [30/200], Loss: 741.2751\n",
      "Epoch [40/200], Loss: 649.0900\n",
      "Epoch [50/200], Loss: 563.0953\n",
      "Epoch [60/200], Loss: 495.7271\n",
      "Epoch [70/200], Loss: 433.1857\n",
      "Epoch [80/200], Loss: 372.9378\n",
      "Epoch [90/200], Loss: 315.0829\n",
      "Epoch [100/200], Loss: 266.1487\n",
      "Epoch [110/200], Loss: 228.8983\n",
      "Epoch [120/200], Loss: 196.7435\n",
      "Epoch [130/200], Loss: 166.1012\n",
      "Epoch [140/200], Loss: 138.2118\n",
      "Epoch [150/200], Loss: 111.2725\n",
      "Epoch [160/200], Loss: 86.3502\n",
      "Epoch [170/200], Loss: 65.4922\n",
      "Epoch [180/200], Loss: 48.8907\n",
      "Epoch [190/200], Loss: 37.0447\n",
      "Epoch [200/200], Loss: 29.9741\n",
      "Epoch [10/200], Loss: 1063.9326\n",
      "Epoch [20/200], Loss: 826.7302\n",
      "Epoch [30/200], Loss: 780.3636\n",
      "Epoch [40/200], Loss: 659.9462\n",
      "Epoch [50/200], Loss: 602.7649\n",
      "Epoch [60/200], Loss: 561.4919\n",
      "Epoch [70/200], Loss: 517.8553\n",
      "Epoch [80/200], Loss: 463.2704\n",
      "Epoch [90/200], Loss: 397.1073\n",
      "Epoch [100/200], Loss: 316.9911\n",
      "Epoch [110/200], Loss: 246.9144\n",
      "Epoch [120/200], Loss: 195.6438\n",
      "Epoch [130/200], Loss: 152.2504\n",
      "Epoch [140/200], Loss: 114.6831\n",
      "Epoch [150/200], Loss: 85.3976\n",
      "Epoch [160/200], Loss: 63.3718\n",
      "Epoch [170/200], Loss: 47.8787\n",
      "Epoch [180/200], Loss: 37.6670\n",
      "Epoch [190/200], Loss: 31.4049\n",
      "Epoch [200/200], Loss: 27.8575\n",
      "Epoch [10/200], Loss: 960.8405\n",
      "Epoch [20/200], Loss: 852.0876\n",
      "Epoch [30/200], Loss: 835.2668\n",
      "Epoch [40/200], Loss: 708.9348\n",
      "Epoch [50/200], Loss: 635.0013\n",
      "Epoch [60/200], Loss: 582.5001\n",
      "Epoch [70/200], Loss: 531.1379\n",
      "Epoch [80/200], Loss: 468.7277\n",
      "Epoch [90/200], Loss: 394.9272\n",
      "Epoch [100/200], Loss: 318.2825\n",
      "Epoch [110/200], Loss: 256.6678\n",
      "Epoch [120/200], Loss: 213.8534\n",
      "Epoch [130/200], Loss: 178.5345\n",
      "Epoch [140/200], Loss: 145.1285\n",
      "Epoch [150/200], Loss: 115.7013\n",
      "Epoch [160/200], Loss: 90.7616\n",
      "Epoch [170/200], Loss: 69.3090\n",
      "Epoch [180/200], Loss: 52.1886\n",
      "Epoch [190/200], Loss: 39.3682\n",
      "Epoch [200/200], Loss: 31.3950\n",
      "Epoch [10/200], Loss: 858.6304\n",
      "Epoch [20/200], Loss: 770.5047\n",
      "Epoch [30/200], Loss: 794.9492\n",
      "Epoch [40/200], Loss: 707.6401\n",
      "Epoch [50/200], Loss: 633.8107\n",
      "Epoch [60/200], Loss: 565.1570\n",
      "Epoch [70/200], Loss: 492.8542\n",
      "Epoch [80/200], Loss: 410.6363\n",
      "Epoch [90/200], Loss: 329.2235\n",
      "Epoch [100/200], Loss: 256.0313\n",
      "Epoch [110/200], Loss: 201.0120\n",
      "Epoch [120/200], Loss: 157.3743\n",
      "Epoch [130/200], Loss: 119.9218\n",
      "Epoch [140/200], Loss: 89.9708\n",
      "Epoch [150/200], Loss: 67.1854\n",
      "Epoch [160/200], Loss: 50.9322\n",
      "Epoch [170/200], Loss: 40.1842\n",
      "Epoch [180/200], Loss: 33.5563\n",
      "Epoch [190/200], Loss: 29.6299\n",
      "Epoch [200/200], Loss: 27.3274\n",
      "Epoch [10/200], Loss: 913.5824\n",
      "Epoch [20/200], Loss: 749.0954\n",
      "Epoch [30/200], Loss: 748.7405\n",
      "Epoch [40/200], Loss: 646.9427\n",
      "Epoch [50/200], Loss: 565.7097\n",
      "Epoch [60/200], Loss: 502.4949\n",
      "Epoch [70/200], Loss: 435.3930\n",
      "Epoch [80/200], Loss: 367.9187\n",
      "Epoch [90/200], Loss: 306.0905\n",
      "Epoch [100/200], Loss: 257.9828\n",
      "Epoch [110/200], Loss: 218.5614\n",
      "Epoch [120/200], Loss: 181.8087\n",
      "Epoch [130/200], Loss: 147.9409\n",
      "Epoch [140/200], Loss: 118.0132\n",
      "Epoch [150/200], Loss: 92.3740\n",
      "Epoch [160/200], Loss: 71.2127\n",
      "Epoch [170/200], Loss: 54.8242\n",
      "Epoch [180/200], Loss: 43.1539\n",
      "Epoch [190/200], Loss: 35.5014\n",
      "Epoch [200/200], Loss: 30.7950\n",
      "Epoch [10/200], Loss: 720.8765\n",
      "Epoch [20/200], Loss: 670.2737\n",
      "Epoch [30/200], Loss: 670.4746\n",
      "Epoch [40/200], Loss: 614.8136\n",
      "Epoch [50/200], Loss: 552.8595\n",
      "Epoch [60/200], Loss: 494.4954\n",
      "Epoch [70/200], Loss: 432.6774\n",
      "Epoch [80/200], Loss: 363.3477\n",
      "Epoch [90/200], Loss: 295.9802\n",
      "Epoch [100/200], Loss: 240.0386\n",
      "Epoch [110/200], Loss: 197.9214\n",
      "Epoch [120/200], Loss: 162.2217\n",
      "Epoch [130/200], Loss: 130.3255\n",
      "Epoch [140/200], Loss: 103.1739\n",
      "Epoch [150/200], Loss: 80.4929\n",
      "Epoch [160/200], Loss: 61.4539\n",
      "Epoch [170/200], Loss: 46.7288\n",
      "Epoch [180/200], Loss: 36.4987\n",
      "Epoch [190/200], Loss: 30.2637\n",
      "Epoch [200/200], Loss: 26.9524\n",
      "\n",
      "Model: CNN | Test RMSE: 24.8942 | Test R value (correlation): 0.3517\n",
      "Predictions for CNN saved to ../Results/CNN_predictions.csv\n",
      "Metrics for CNN saved to ../Results/CNN_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:35:45.873589Z",
     "start_time": "2024-10-23T18:16:42.509609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "def load_data_3d(input_folder, labels_file, labels_column, normalization='minmax'):\n",
    "    labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "    y = labels_df[labels_column]\n",
    "    data_dict = {}  \n",
    "    scaler = MinMaxScaler()  # Create a scaler instance\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Files look like this 'M10011_rest_jhu.csv', extract the number\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) > 0:\n",
    "                participant_id = ''.join(filter(str.isdigit, parts[0]))\n",
    "                csv_path = os.path.join(input_folder, filename)\n",
    "                matrix = pd.read_csv(csv_path, index_col=None, header=None).values\n",
    "\n",
    "                # Normalize the matrix based on the chosen method\n",
    "                if normalization == 'minmax':\n",
    "                    matrix = scaler.fit_transform(matrix)  # Min-Max scale between 0 and 1\n",
    "                elif normalization == 'standard':\n",
    "                    matrix = (matrix - np.mean(matrix)) / np.std(matrix)  # Standardize to have mean 0, std 1\n",
    "                \n",
    "                data_dict[participant_id] = matrix\n",
    "\n",
    "    return data_dict, y\n",
    "\n",
    "def convert_dict_to_array(data_dict):\n",
    "    matrix_list = [data_dict[participant_id] for participant_id in data_dict]\n",
    "    \n",
    "    # Convert the list of matrices to a 3D numpy array\n",
    "    X_array = np.array(matrix_list)\n",
    "    return X_array\n",
    "\n",
    "def get_shape_3d(data_dict):\n",
    "    num_participants = len(data_dict)\n",
    "    matrix_shape = next(iter(data_dict.values())).shape if num_participants > 0 else (0, 0)\n",
    "    #print(f\"Number of Participants: {num_participants}, Matrix Shape per Participant: {matrix_shape}\")\n",
    "    return  num_participants, matrix_shape[0]\n",
    "\n",
    "def train_and_evaluate_3d(model_class, X_dict, y, num_epochs=200, learning_rate=0.001):\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Convert the dictionary to a 3D array\n",
    "    X_array = convert_dict_to_array(X_dict)\n",
    "\n",
    "    # Get the shape of each 2D matrix\n",
    "    num_samples, num_regions, _ = X_array.shape\n",
    "    \n",
    "    # Initialize Leave-One-Out cross-validator\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for train_index, test_index in loo.split(X_array):\n",
    "        # Split the data into training and test for this fold\n",
    "        X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # For CNN, reshape input into (batch_size, 1, num_regions, num_regions)\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        # For MLP, flatten the 2D matrices into vectors of size (batch_size, num_regions * num_regions)\n",
    "        if model_class.__name__ in [\"NeuralNetMLP\", \"DeepNeuralNetMLP\"]:\n",
    "            X_train_tensor = X_train_tensor.view(X_train_tensor.size(0), -1)\n",
    "            X_test_tensor = X_test_tensor.view(X_test_tensor.size(0), -1)\n",
    "        \n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Initialize the model, loss function, and optimizer\n",
    "        input_size = X_train_tensor.size(1) if model_class.__name__ in [\"NeuralNetMLP\", \"DeepNeuralNetMLP\"] else num_regions\n",
    "        model = model_class(input_size=input_size).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on the left-out test sample\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(X_test_tensor).cpu().numpy()[0][0]  # Get prediction and move back to CPU\n",
    "\n",
    "        # Store the true value and predicted value\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "    # Calculate RMSE and Pearson correlation coefficient (R-value)\n",
    "    test_rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    corr = pearsonr(y_true, y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"\\nModel: {model_class.__name__} | Test RMSE: {test_rmse:.4f} | Test R value (correlation): {corr[0]:.4f}\")\n",
    "\n",
    "    # Save predictions and metrics using the existing function\n",
    "    model_name = model_class.__name__\n",
    "    save_predictions_and_metrics(model_name, y_true, y_pred, corr[0], test_rmse)\n",
    "\n",
    "class CNN_2d(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CNN_2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Calculate the flattened size after convolution and pooling\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def _get_flattened_size(self, input_size):\n",
    "        # Create a dummy tensor with the same size as the input to calculate the output size after convolutions\n",
    "        dummy_input = torch.zeros(1, 1, input_size, input_size)  # Batch size of 1, 1 channel, input_size as height and width\n",
    "        output = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        output = self.pool(self.relu(self.conv2(output)))\n",
    "        flattened_size = output.numel()  # Calculate the number of elements in the output tensor\n",
    "        return flattened_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def convert_to_graph(matrix):\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Create an edge index based on non-zero correlations (can be thresholded if needed)\n",
    "    edge_index = torch.nonzero(torch.tensor(matrix), as_tuple=False).t().contiguous()\n",
    "    edge_weight = torch.tensor(matrix[edge_index[0], edge_index[1]], dtype=torch.float)\n",
    "\n",
    "    # Node features can be set as identity matrix or something else relevant\n",
    "    x = torch.eye(num_nodes, dtype=torch.float)  # Using an identity matrix as node features\n",
    "\n",
    "    # Create a Data object for PyTorch Geometric\n",
    "    graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight)\n",
    "    return graph_data\n",
    "\n",
    "def prepare_graph_data(data_dict, y):\n",
    "    \"\"\"\n",
    "    Convert the entire dataset into a list of PyTorch Geometric Data objects.\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "    for i, (participant_id, matrix) in enumerate(data_dict.items()):\n",
    "        graph = convert_to_graph(matrix)\n",
    "        graph.y = torch.tensor([y.iloc[i]], dtype=torch.float)  # Attach the target variable as the label\n",
    "        graph_data_list.append(graph)\n",
    "    return graph_data_list\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 16)\n",
    "        self.fc1 = torch.nn.Linear(16, 1)  # Final fully connected layer for regression\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Extract data from the PyTorch Geometric Data object\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # GCN layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "\n",
    "        # Global pooling layer to summarize the graph\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Final fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_evaluate_gnn(model_class, X_dict, y, num_epochs=200, learning_rate=0.001):\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Convert the dictionary to a list of graph data objects\n",
    "    graph_data_list = prepare_graph_data(X_dict, y)\n",
    "\n",
    "    # Move all graph data objects to the same device\n",
    "    for graph_data in graph_data_list:\n",
    "        graph_data.to(device)\n",
    "\n",
    "    # Initialize Leave-One-Out cross-validator\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for train_index, test_index in loo.split(graph_data_list):\n",
    "        # Split the data into training and test for this fold\n",
    "        train_data = [graph_data_list[i] for i in train_index]\n",
    "        test_data = graph_data_list[test_index[0]].to(device)\n",
    "\n",
    "        # Create DataLoader for the training data\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Initialize the model, loss function, and optimizer\n",
    "        num_node_features = train_data[0].x.shape[1]  # Assuming all graphs have the same number of features\n",
    "        model = model_class(num_node_features=num_node_features, hidden_channels=32).to(device)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                # Ensure the entire batch is moved to the device\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs, batch.y.view(-1, 1))  \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Print loss every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluate on the left-out test sample\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(test_data).cpu().numpy()[0][0]  # Get prediction and move back to CPU\n",
    "\n",
    "        # Store the true value and predicted value\n",
    "        y_true.append(test_data.y.cpu().numpy()[0])\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "    # Calculate RMSE and Pearson correlation coefficient (R-value)\n",
    "    test_rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    corr = pearsonr(y_true, y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"\\nModel: {model_class.__name__} | Test RMSE: {test_rmse:.4f} | Test R value (correlation): {corr[0]:.4f}\")\n",
    "\n",
    "    # Save predictions and metrics using the existing function\n",
    "    model_name = model_class.__name__\n",
    "    save_predictions_and_metrics(model_name, y_true, y_pred, corr[0], test_rmse)\n",
    "\n",
    "\n",
    "\n",
    "features_file = '../Processed Data/rest_jhu_first_visit/'\n",
    "labels_file = '../Processed Data/101_participants_40_regions_target_variable.csv'\n",
    "prediction_label = 'Aphasia quotient'\n",
    "\n",
    "X, y = load_data_3d(features_file, labels_file, prediction_label)\n",
    "\n",
    "# Train the GNN model\n",
    "train_and_evaluate_gnn(GCN, X, y)\n",
    "#train_and_evaluate_3d(NeuralNetMLP, X, y)\n"
   ],
   "id": "986a73474bb62acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [10/200], Loss: 4410.0552\n",
      "Epoch [20/200], Loss: 4492.1182\n",
      "Epoch [30/200], Loss: 4334.7894\n",
      "Epoch [40/200], Loss: 4101.3366\n",
      "Epoch [50/200], Loss: 3777.4244\n",
      "Epoch [60/200], Loss: 2397.9063\n",
      "Epoch [70/200], Loss: 2024.5803\n",
      "Epoch [80/200], Loss: 1335.1375\n",
      "Epoch [90/200], Loss: 821.1103\n",
      "Epoch [100/200], Loss: 635.0001\n",
      "Epoch [110/200], Loss: 669.1812\n",
      "Epoch [120/200], Loss: 601.0932\n",
      "Epoch [130/200], Loss: 688.0505\n",
      "Epoch [140/200], Loss: 714.9279\n",
      "Epoch [150/200], Loss: 604.3989\n",
      "Epoch [160/200], Loss: 711.4239\n",
      "Epoch [170/200], Loss: 551.0873\n",
      "Epoch [180/200], Loss: 676.0183\n",
      "Epoch [190/200], Loss: 683.0985\n",
      "Epoch [200/200], Loss: 536.3320\n",
      "Epoch [10/200], Loss: 4333.7787\n",
      "Epoch [20/200], Loss: 4569.3975\n",
      "Epoch [30/200], Loss: 4519.8047\n",
      "Epoch [40/200], Loss: 3677.5983\n",
      "Epoch [50/200], Loss: 2978.6760\n",
      "Epoch [60/200], Loss: 2181.7789\n",
      "Epoch [70/200], Loss: 1538.6178\n",
      "Epoch [80/200], Loss: 832.6339\n",
      "Epoch [90/200], Loss: 604.3848\n",
      "Epoch [100/200], Loss: 688.4220\n",
      "Epoch [110/200], Loss: 507.2969\n",
      "Epoch [120/200], Loss: 589.3279\n",
      "Epoch [130/200], Loss: 581.0775\n",
      "Epoch [140/200], Loss: 547.0613\n",
      "Epoch [150/200], Loss: 693.8443\n",
      "Epoch [160/200], Loss: 750.0436\n",
      "Epoch [170/200], Loss: 737.8865\n",
      "Epoch [180/200], Loss: 599.4626\n",
      "Epoch [190/200], Loss: 585.4084\n",
      "Epoch [200/200], Loss: 588.6738\n",
      "Epoch [10/200], Loss: 4366.1580\n",
      "Epoch [20/200], Loss: 4057.0465\n",
      "Epoch [30/200], Loss: 4424.2231\n",
      "Epoch [40/200], Loss: 4023.5541\n",
      "Epoch [50/200], Loss: 3667.3787\n",
      "Epoch [60/200], Loss: 2479.4393\n",
      "Epoch [70/200], Loss: 1758.8863\n",
      "Epoch [80/200], Loss: 1640.8270\n",
      "Epoch [90/200], Loss: 795.9102\n",
      "Epoch [100/200], Loss: 608.0708\n",
      "Epoch [110/200], Loss: 558.8592\n",
      "Epoch [120/200], Loss: 633.0333\n",
      "Epoch [130/200], Loss: 554.3125\n",
      "Epoch [140/200], Loss: 630.4669\n",
      "Epoch [150/200], Loss: 527.4948\n",
      "Epoch [160/200], Loss: 564.6651\n",
      "Epoch [170/200], Loss: 661.7856\n",
      "Epoch [180/200], Loss: 603.8126\n",
      "Epoch [190/200], Loss: 658.8763\n",
      "Epoch [200/200], Loss: 669.8996\n",
      "Epoch [10/200], Loss: 4042.6698\n",
      "Epoch [20/200], Loss: 4273.9880\n",
      "Epoch [30/200], Loss: 3785.5125\n",
      "Epoch [40/200], Loss: 3923.9343\n",
      "Epoch [50/200], Loss: 3490.3585\n",
      "Epoch [60/200], Loss: 2517.8654\n",
      "Epoch [70/200], Loss: 1819.7704\n",
      "Epoch [80/200], Loss: 1187.1844\n",
      "Epoch [90/200], Loss: 739.6438\n",
      "Epoch [100/200], Loss: 709.6208\n",
      "Epoch [110/200], Loss: 587.4549\n",
      "Epoch [120/200], Loss: 642.4487\n",
      "Epoch [130/200], Loss: 574.2110\n",
      "Epoch [140/200], Loss: 502.6233\n",
      "Epoch [150/200], Loss: 556.8891\n",
      "Epoch [160/200], Loss: 696.2024\n",
      "Epoch [170/200], Loss: 624.5182\n",
      "Epoch [180/200], Loss: 530.2099\n",
      "Epoch [190/200], Loss: 638.8652\n",
      "Epoch [200/200], Loss: 612.1362\n",
      "Epoch [10/200], Loss: 4290.4911\n",
      "Epoch [20/200], Loss: 4339.9641\n",
      "Epoch [30/200], Loss: 4174.0226\n",
      "Epoch [40/200], Loss: 4127.0452\n",
      "Epoch [50/200], Loss: 2975.0490\n",
      "Epoch [60/200], Loss: 2975.8918\n",
      "Epoch [70/200], Loss: 2130.1924\n",
      "Epoch [80/200], Loss: 1274.6055\n",
      "Epoch [90/200], Loss: 878.0884\n",
      "Epoch [100/200], Loss: 627.6893\n",
      "Epoch [110/200], Loss: 623.2059\n",
      "Epoch [120/200], Loss: 617.1902\n",
      "Epoch [130/200], Loss: 665.0732\n",
      "Epoch [140/200], Loss: 688.4602\n",
      "Epoch [150/200], Loss: 623.6246\n",
      "Epoch [160/200], Loss: 572.0149\n",
      "Epoch [170/200], Loss: 667.2505\n",
      "Epoch [180/200], Loss: 819.7394\n",
      "Epoch [190/200], Loss: 627.1589\n",
      "Epoch [200/200], Loss: 588.5920\n",
      "Epoch [10/200], Loss: 4635.7057\n",
      "Epoch [20/200], Loss: 4086.3954\n",
      "Epoch [30/200], Loss: 4019.6066\n",
      "Epoch [40/200], Loss: 3319.7698\n",
      "Epoch [50/200], Loss: 2797.7939\n",
      "Epoch [60/200], Loss: 1949.4683\n",
      "Epoch [70/200], Loss: 1143.4366\n",
      "Epoch [80/200], Loss: 685.6170\n",
      "Epoch [90/200], Loss: 617.0904\n",
      "Epoch [100/200], Loss: 688.5228\n",
      "Epoch [110/200], Loss: 668.5251\n",
      "Epoch [120/200], Loss: 621.3516\n",
      "Epoch [130/200], Loss: 598.7370\n",
      "Epoch [140/200], Loss: 660.2528\n",
      "Epoch [150/200], Loss: 756.1040\n",
      "Epoch [160/200], Loss: 589.6664\n",
      "Epoch [170/200], Loss: 634.2692\n",
      "Epoch [180/200], Loss: 620.9710\n",
      "Epoch [190/200], Loss: 579.9924\n",
      "Epoch [200/200], Loss: 605.4840\n",
      "Epoch [10/200], Loss: 4183.4553\n",
      "Epoch [20/200], Loss: 3914.2925\n",
      "Epoch [30/200], Loss: 3805.1133\n",
      "Epoch [40/200], Loss: 4033.7114\n",
      "Epoch [50/200], Loss: 3877.4792\n",
      "Epoch [60/200], Loss: 2304.0563\n",
      "Epoch [70/200], Loss: 2001.8897\n",
      "Epoch [80/200], Loss: 1228.0207\n",
      "Epoch [90/200], Loss: 973.7413\n",
      "Epoch [100/200], Loss: 588.5705\n",
      "Epoch [110/200], Loss: 682.8072\n",
      "Epoch [120/200], Loss: 665.9625\n",
      "Epoch [130/200], Loss: 572.9978\n",
      "Epoch [140/200], Loss: 607.6599\n",
      "Epoch [150/200], Loss: 606.8587\n",
      "Epoch [160/200], Loss: 638.6931\n",
      "Epoch [170/200], Loss: 617.9292\n",
      "Epoch [180/200], Loss: 693.3779\n",
      "Epoch [190/200], Loss: 622.5118\n",
      "Epoch [200/200], Loss: 670.3164\n",
      "Epoch [10/200], Loss: 4428.6516\n",
      "Epoch [20/200], Loss: 4151.3398\n",
      "Epoch [30/200], Loss: 4219.9074\n",
      "Epoch [40/200], Loss: 3707.7947\n",
      "Epoch [50/200], Loss: 3048.1464\n",
      "Epoch [60/200], Loss: 2192.8417\n",
      "Epoch [70/200], Loss: 1304.0366\n",
      "Epoch [80/200], Loss: 853.8415\n",
      "Epoch [90/200], Loss: 718.2180\n",
      "Epoch [100/200], Loss: 506.1505\n",
      "Epoch [110/200], Loss: 573.7851\n",
      "Epoch [120/200], Loss: 505.3938\n",
      "Epoch [130/200], Loss: 598.1464\n",
      "Epoch [140/200], Loss: 665.2724\n",
      "Epoch [150/200], Loss: 503.0609\n",
      "Epoch [160/200], Loss: 757.2033\n",
      "Epoch [170/200], Loss: 547.6298\n",
      "Epoch [180/200], Loss: 537.8939\n",
      "Epoch [190/200], Loss: 571.0167\n",
      "Epoch [200/200], Loss: 620.2035\n",
      "Epoch [10/200], Loss: 5051.0657\n",
      "Epoch [20/200], Loss: 4915.9410\n",
      "Epoch [30/200], Loss: 4177.8215\n",
      "Epoch [40/200], Loss: 4013.0107\n",
      "Epoch [50/200], Loss: 2849.0297\n",
      "Epoch [60/200], Loss: 1836.0281\n",
      "Epoch [70/200], Loss: 1378.0476\n",
      "Epoch [80/200], Loss: 916.7202\n",
      "Epoch [90/200], Loss: 604.5560\n",
      "Epoch [100/200], Loss: 646.1635\n",
      "Epoch [110/200], Loss: 686.2507\n",
      "Epoch [120/200], Loss: 580.4241\n",
      "Epoch [130/200], Loss: 623.3565\n",
      "Epoch [140/200], Loss: 680.1420\n",
      "Epoch [150/200], Loss: 675.3627\n",
      "Epoch [160/200], Loss: 638.4464\n",
      "Epoch [170/200], Loss: 624.9360\n",
      "Epoch [180/200], Loss: 638.0378\n",
      "Epoch [190/200], Loss: 692.6289\n",
      "Epoch [200/200], Loss: 622.1325\n",
      "Epoch [10/200], Loss: 4665.7314\n",
      "Epoch [20/200], Loss: 4732.5432\n",
      "Epoch [30/200], Loss: 4714.7076\n",
      "Epoch [40/200], Loss: 4297.9108\n",
      "Epoch [50/200], Loss: 4145.7485\n",
      "Epoch [60/200], Loss: 3421.7606\n",
      "Epoch [70/200], Loss: 2613.0173\n",
      "Epoch [80/200], Loss: 1766.7649\n",
      "Epoch [90/200], Loss: 1435.2236\n",
      "Epoch [100/200], Loss: 923.9301\n",
      "Epoch [110/200], Loss: 621.9771\n",
      "Epoch [120/200], Loss: 600.3210\n",
      "Epoch [130/200], Loss: 604.5493\n",
      "Epoch [140/200], Loss: 630.4360\n",
      "Epoch [150/200], Loss: 547.7521\n",
      "Epoch [160/200], Loss: 688.7768\n",
      "Epoch [170/200], Loss: 588.2184\n",
      "Epoch [180/200], Loss: 611.0709\n",
      "Epoch [190/200], Loss: 571.0957\n",
      "Epoch [200/200], Loss: 619.5776\n",
      "Epoch [10/200], Loss: 4699.6309\n",
      "Epoch [20/200], Loss: 4493.1340\n",
      "Epoch [30/200], Loss: 4658.6796\n",
      "Epoch [40/200], Loss: 3986.3670\n",
      "Epoch [50/200], Loss: 3096.6805\n",
      "Epoch [60/200], Loss: 3174.6768\n",
      "Epoch [70/200], Loss: 2023.1597\n",
      "Epoch [80/200], Loss: 1377.6393\n",
      "Epoch [90/200], Loss: 901.0067\n",
      "Epoch [100/200], Loss: 750.8202\n",
      "Epoch [110/200], Loss: 633.1490\n",
      "Epoch [120/200], Loss: 650.0165\n",
      "Epoch [130/200], Loss: 576.6106\n",
      "Epoch [140/200], Loss: 638.7978\n",
      "Epoch [150/200], Loss: 612.3276\n",
      "Epoch [160/200], Loss: 581.7356\n",
      "Epoch [170/200], Loss: 694.9239\n",
      "Epoch [180/200], Loss: 789.0820\n",
      "Epoch [190/200], Loss: 671.9025\n",
      "Epoch [200/200], Loss: 617.8576\n",
      "Epoch [10/200], Loss: 4126.8755\n",
      "Epoch [20/200], Loss: 4308.1157\n",
      "Epoch [30/200], Loss: 4392.3024\n",
      "Epoch [40/200], Loss: 3741.5366\n",
      "Epoch [50/200], Loss: 3734.2182\n",
      "Epoch [60/200], Loss: 1815.4892\n",
      "Epoch [70/200], Loss: 1665.8074\n",
      "Epoch [80/200], Loss: 926.6724\n",
      "Epoch [90/200], Loss: 803.4525\n",
      "Epoch [100/200], Loss: 570.4671\n",
      "Epoch [110/200], Loss: 630.9965\n",
      "Epoch [120/200], Loss: 616.3764\n",
      "Epoch [130/200], Loss: 731.4117\n",
      "Epoch [140/200], Loss: 602.0692\n",
      "Epoch [150/200], Loss: 567.8714\n",
      "Epoch [160/200], Loss: 534.6390\n",
      "Epoch [170/200], Loss: 578.0169\n",
      "Epoch [180/200], Loss: 504.6248\n",
      "Epoch [190/200], Loss: 646.9313\n",
      "Epoch [200/200], Loss: 566.9181\n",
      "Epoch [10/200], Loss: 4790.4846\n",
      "Epoch [20/200], Loss: 4154.2903\n",
      "Epoch [30/200], Loss: 4451.8954\n",
      "Epoch [40/200], Loss: 3939.4647\n",
      "Epoch [50/200], Loss: 3702.6304\n",
      "Epoch [60/200], Loss: 3126.4690\n",
      "Epoch [70/200], Loss: 2172.4211\n",
      "Epoch [80/200], Loss: 1461.5493\n",
      "Epoch [90/200], Loss: 976.1367\n",
      "Epoch [100/200], Loss: 641.3185\n",
      "Epoch [110/200], Loss: 646.9040\n",
      "Epoch [120/200], Loss: 700.8739\n",
      "Epoch [130/200], Loss: 537.2939\n",
      "Epoch [140/200], Loss: 533.3265\n",
      "Epoch [150/200], Loss: 578.7689\n",
      "Epoch [160/200], Loss: 583.0789\n",
      "Epoch [170/200], Loss: 736.5665\n",
      "Epoch [180/200], Loss: 600.6580\n",
      "Epoch [190/200], Loss: 658.0842\n",
      "Epoch [200/200], Loss: 534.5997\n",
      "Epoch [10/200], Loss: 4830.5507\n",
      "Epoch [20/200], Loss: 4791.6849\n",
      "Epoch [30/200], Loss: 4566.9607\n",
      "Epoch [40/200], Loss: 4087.5000\n",
      "Epoch [50/200], Loss: 3483.0721\n",
      "Epoch [60/200], Loss: 2885.9780\n",
      "Epoch [70/200], Loss: 2069.5775\n",
      "Epoch [80/200], Loss: 1252.3750\n",
      "Epoch [90/200], Loss: 884.0316\n",
      "Epoch [100/200], Loss: 831.1823\n",
      "Epoch [110/200], Loss: 588.7670\n",
      "Epoch [120/200], Loss: 618.3960\n",
      "Epoch [130/200], Loss: 566.2316\n",
      "Epoch [140/200], Loss: 553.8429\n",
      "Epoch [150/200], Loss: 583.2001\n",
      "Epoch [160/200], Loss: 740.7095\n",
      "Epoch [170/200], Loss: 541.1243\n",
      "Epoch [180/200], Loss: 572.2412\n",
      "Epoch [190/200], Loss: 622.5987\n",
      "Epoch [200/200], Loss: 600.6882\n",
      "Epoch [10/200], Loss: 4527.6859\n",
      "Epoch [20/200], Loss: 4337.2531\n",
      "Epoch [30/200], Loss: 4176.4525\n",
      "Epoch [40/200], Loss: 3712.5010\n",
      "Epoch [50/200], Loss: 3706.1546\n",
      "Epoch [60/200], Loss: 2196.2519\n",
      "Epoch [70/200], Loss: 1702.4058\n",
      "Epoch [80/200], Loss: 917.9389\n",
      "Epoch [90/200], Loss: 722.7667\n",
      "Epoch [100/200], Loss: 597.7571\n",
      "Epoch [110/200], Loss: 584.4191\n",
      "Epoch [120/200], Loss: 690.9480\n",
      "Epoch [130/200], Loss: 582.0047\n",
      "Epoch [140/200], Loss: 574.6690\n",
      "Epoch [150/200], Loss: 599.4034\n",
      "Epoch [160/200], Loss: 523.6242\n",
      "Epoch [170/200], Loss: 582.3833\n",
      "Epoch [180/200], Loss: 547.5836\n",
      "Epoch [190/200], Loss: 651.7984\n",
      "Epoch [200/200], Loss: 596.8400\n",
      "Epoch [10/200], Loss: 4560.2299\n",
      "Epoch [20/200], Loss: 4953.3965\n",
      "Epoch [30/200], Loss: 4422.1196\n",
      "Epoch [40/200], Loss: 3406.5812\n",
      "Epoch [50/200], Loss: 2916.8760\n",
      "Epoch [60/200], Loss: 2629.0378\n",
      "Epoch [70/200], Loss: 1460.9804\n",
      "Epoch [80/200], Loss: 842.6956\n",
      "Epoch [90/200], Loss: 686.8709\n",
      "Epoch [100/200], Loss: 678.4045\n",
      "Epoch [110/200], Loss: 670.4032\n",
      "Epoch [120/200], Loss: 607.7296\n",
      "Epoch [130/200], Loss: 579.8546\n",
      "Epoch [140/200], Loss: 690.5639\n",
      "Epoch [150/200], Loss: 622.8855\n",
      "Epoch [160/200], Loss: 620.1830\n",
      "Epoch [170/200], Loss: 551.9505\n",
      "Epoch [180/200], Loss: 618.7048\n",
      "Epoch [190/200], Loss: 669.3335\n",
      "Epoch [200/200], Loss: 595.5929\n",
      "Epoch [10/200], Loss: 4602.2123\n",
      "Epoch [20/200], Loss: 4569.0089\n",
      "Epoch [30/200], Loss: 4169.0095\n",
      "Epoch [40/200], Loss: 3324.9013\n",
      "Epoch [50/200], Loss: 2826.6376\n",
      "Epoch [60/200], Loss: 1943.2219\n",
      "Epoch [70/200], Loss: 1356.2760\n",
      "Epoch [80/200], Loss: 802.7793\n",
      "Epoch [90/200], Loss: 796.2545\n",
      "Epoch [100/200], Loss: 578.8668\n",
      "Epoch [110/200], Loss: 682.0502\n",
      "Epoch [120/200], Loss: 569.7205\n",
      "Epoch [130/200], Loss: 550.1234\n",
      "Epoch [140/200], Loss: 549.0246\n",
      "Epoch [150/200], Loss: 661.5883\n",
      "Epoch [160/200], Loss: 583.5864\n",
      "Epoch [170/200], Loss: 555.3367\n",
      "Epoch [180/200], Loss: 523.4779\n",
      "Epoch [190/200], Loss: 523.6963\n",
      "Epoch [200/200], Loss: 495.0104\n",
      "Epoch [10/200], Loss: 4811.2531\n",
      "Epoch [20/200], Loss: 4537.4200\n",
      "Epoch [30/200], Loss: 3872.1248\n",
      "Epoch [40/200], Loss: 3513.7872\n",
      "Epoch [50/200], Loss: 2939.7097\n",
      "Epoch [60/200], Loss: 2002.8841\n",
      "Epoch [70/200], Loss: 1029.4848\n",
      "Epoch [80/200], Loss: 775.5049\n",
      "Epoch [90/200], Loss: 668.0798\n",
      "Epoch [100/200], Loss: 668.9098\n",
      "Epoch [110/200], Loss: 657.3410\n",
      "Epoch [120/200], Loss: 585.4404\n",
      "Epoch [130/200], Loss: 644.2049\n",
      "Epoch [140/200], Loss: 608.1199\n",
      "Epoch [150/200], Loss: 665.9262\n",
      "Epoch [160/200], Loss: 651.3904\n",
      "Epoch [170/200], Loss: 586.9724\n",
      "Epoch [180/200], Loss: 688.9306\n",
      "Epoch [190/200], Loss: 674.4330\n",
      "Epoch [200/200], Loss: 625.1631\n",
      "Epoch [10/200], Loss: 4927.8961\n",
      "Epoch [20/200], Loss: 4242.0435\n",
      "Epoch [30/200], Loss: 4129.6109\n",
      "Epoch [40/200], Loss: 3777.5326\n",
      "Epoch [50/200], Loss: 3337.7263\n",
      "Epoch [60/200], Loss: 2763.0083\n",
      "Epoch [70/200], Loss: 1686.3051\n",
      "Epoch [80/200], Loss: 1245.5967\n",
      "Epoch [90/200], Loss: 906.2961\n",
      "Epoch [100/200], Loss: 748.6308\n",
      "Epoch [110/200], Loss: 635.2620\n",
      "Epoch [120/200], Loss: 583.8196\n",
      "Epoch [130/200], Loss: 545.0991\n",
      "Epoch [140/200], Loss: 588.4172\n",
      "Epoch [150/200], Loss: 711.4771\n",
      "Epoch [160/200], Loss: 640.2619\n",
      "Epoch [170/200], Loss: 727.3582\n",
      "Epoch [180/200], Loss: 583.0009\n",
      "Epoch [190/200], Loss: 690.3062\n",
      "Epoch [200/200], Loss: 622.5521\n",
      "Epoch [10/200], Loss: 4594.7969\n",
      "Epoch [20/200], Loss: 4328.3473\n",
      "Epoch [30/200], Loss: 3700.2438\n",
      "Epoch [40/200], Loss: 3791.0464\n",
      "Epoch [50/200], Loss: 3392.2534\n",
      "Epoch [60/200], Loss: 2409.2000\n",
      "Epoch [70/200], Loss: 1300.1646\n",
      "Epoch [80/200], Loss: 793.3206\n",
      "Epoch [90/200], Loss: 678.0891\n",
      "Epoch [100/200], Loss: 570.6450\n",
      "Epoch [110/200], Loss: 698.1469\n",
      "Epoch [120/200], Loss: 560.4841\n",
      "Epoch [130/200], Loss: 548.6876\n",
      "Epoch [140/200], Loss: 628.8114\n",
      "Epoch [150/200], Loss: 493.4488\n",
      "Epoch [160/200], Loss: 636.8959\n",
      "Epoch [170/200], Loss: 583.7272\n",
      "Epoch [180/200], Loss: 598.4423\n",
      "Epoch [190/200], Loss: 659.1749\n",
      "Epoch [200/200], Loss: 604.8773\n",
      "Epoch [10/200], Loss: 4736.4968\n",
      "Epoch [20/200], Loss: 4470.3026\n",
      "Epoch [30/200], Loss: 4025.8554\n",
      "Epoch [40/200], Loss: 4088.9997\n",
      "Epoch [50/200], Loss: 3795.9249\n",
      "Epoch [60/200], Loss: 3075.5325\n",
      "Epoch [70/200], Loss: 2291.3715\n",
      "Epoch [80/200], Loss: 1494.3236\n",
      "Epoch [90/200], Loss: 1036.4206\n",
      "Epoch [100/200], Loss: 771.0528\n",
      "Epoch [110/200], Loss: 700.1267\n",
      "Epoch [120/200], Loss: 626.5538\n",
      "Epoch [130/200], Loss: 590.6481\n",
      "Epoch [140/200], Loss: 584.7549\n",
      "Epoch [150/200], Loss: 590.8839\n",
      "Epoch [160/200], Loss: 598.2485\n",
      "Epoch [170/200], Loss: 661.3477\n",
      "Epoch [180/200], Loss: 663.3923\n",
      "Epoch [190/200], Loss: 649.4030\n",
      "Epoch [200/200], Loss: 695.4129\n",
      "Epoch [10/200], Loss: 4680.9742\n",
      "Epoch [20/200], Loss: 4640.5568\n",
      "Epoch [30/200], Loss: 3787.6837\n",
      "Epoch [40/200], Loss: 3483.9127\n",
      "Epoch [50/200], Loss: 3272.0214\n",
      "Epoch [60/200], Loss: 2327.1232\n",
      "Epoch [70/200], Loss: 1197.2239\n",
      "Epoch [80/200], Loss: 985.7178\n",
      "Epoch [90/200], Loss: 684.7827\n",
      "Epoch [100/200], Loss: 723.1837\n",
      "Epoch [110/200], Loss: 546.0730\n",
      "Epoch [120/200], Loss: 591.0912\n",
      "Epoch [130/200], Loss: 633.0606\n",
      "Epoch [140/200], Loss: 667.8242\n",
      "Epoch [150/200], Loss: 575.7330\n",
      "Epoch [160/200], Loss: 584.1387\n",
      "Epoch [170/200], Loss: 552.7139\n",
      "Epoch [180/200], Loss: 643.5019\n",
      "Epoch [190/200], Loss: 537.8097\n",
      "Epoch [200/200], Loss: 659.6200\n",
      "Epoch [10/200], Loss: 4543.0159\n",
      "Epoch [20/200], Loss: 4428.1645\n",
      "Epoch [30/200], Loss: 4364.3995\n",
      "Epoch [40/200], Loss: 3573.4690\n",
      "Epoch [50/200], Loss: 2918.4705\n",
      "Epoch [60/200], Loss: 1576.6856\n",
      "Epoch [70/200], Loss: 1086.7786\n",
      "Epoch [80/200], Loss: 686.1987\n",
      "Epoch [90/200], Loss: 605.0955\n",
      "Epoch [100/200], Loss: 602.1968\n",
      "Epoch [110/200], Loss: 653.6006\n",
      "Epoch [120/200], Loss: 588.4948\n",
      "Epoch [130/200], Loss: 660.3053\n",
      "Epoch [140/200], Loss: 694.5740\n",
      "Epoch [150/200], Loss: 610.8846\n",
      "Epoch [160/200], Loss: 554.9594\n",
      "Epoch [170/200], Loss: 680.4625\n",
      "Epoch [180/200], Loss: 519.3105\n",
      "Epoch [190/200], Loss: 506.8778\n",
      "Epoch [200/200], Loss: 619.0988\n",
      "Epoch [10/200], Loss: 4432.5492\n",
      "Epoch [20/200], Loss: 4342.0317\n",
      "Epoch [30/200], Loss: 4516.8570\n",
      "Epoch [40/200], Loss: 3777.5441\n",
      "Epoch [50/200], Loss: 2944.0699\n",
      "Epoch [60/200], Loss: 2307.6205\n",
      "Epoch [70/200], Loss: 1497.3177\n",
      "Epoch [80/200], Loss: 930.0259\n",
      "Epoch [90/200], Loss: 660.0421\n",
      "Epoch [100/200], Loss: 757.0394\n",
      "Epoch [110/200], Loss: 577.0783\n",
      "Epoch [120/200], Loss: 662.2234\n",
      "Epoch [130/200], Loss: 699.2618\n",
      "Epoch [140/200], Loss: 625.1280\n",
      "Epoch [150/200], Loss: 655.0819\n",
      "Epoch [160/200], Loss: 644.4379\n",
      "Epoch [170/200], Loss: 679.5125\n",
      "Epoch [180/200], Loss: 577.0570\n",
      "Epoch [190/200], Loss: 698.1273\n",
      "Epoch [200/200], Loss: 643.6432\n",
      "Epoch [10/200], Loss: 4498.2039\n",
      "Epoch [20/200], Loss: 4156.4187\n",
      "Epoch [30/200], Loss: 3965.6468\n",
      "Epoch [40/200], Loss: 3821.2166\n",
      "Epoch [50/200], Loss: 3058.2981\n",
      "Epoch [60/200], Loss: 2291.4840\n",
      "Epoch [70/200], Loss: 1784.3155\n",
      "Epoch [80/200], Loss: 1171.4016\n",
      "Epoch [90/200], Loss: 905.8561\n",
      "Epoch [100/200], Loss: 714.0795\n",
      "Epoch [110/200], Loss: 675.9287\n",
      "Epoch [120/200], Loss: 611.2697\n",
      "Epoch [130/200], Loss: 610.9075\n",
      "Epoch [140/200], Loss: 575.3949\n",
      "Epoch [150/200], Loss: 637.0764\n",
      "Epoch [160/200], Loss: 670.9093\n",
      "Epoch [170/200], Loss: 587.7243\n",
      "Epoch [180/200], Loss: 712.5150\n",
      "Epoch [190/200], Loss: 598.7224\n",
      "Epoch [200/200], Loss: 631.1953\n",
      "Epoch [10/200], Loss: 4278.8832\n",
      "Epoch [20/200], Loss: 4113.1113\n",
      "Epoch [30/200], Loss: 4227.0977\n",
      "Epoch [40/200], Loss: 3614.3040\n",
      "Epoch [50/200], Loss: 2848.1483\n",
      "Epoch [60/200], Loss: 2434.7631\n",
      "Epoch [70/200], Loss: 1915.0912\n",
      "Epoch [80/200], Loss: 1058.8974\n",
      "Epoch [90/200], Loss: 728.3912\n",
      "Epoch [100/200], Loss: 570.6864\n",
      "Epoch [110/200], Loss: 698.9438\n",
      "Epoch [120/200], Loss: 558.4508\n",
      "Epoch [130/200], Loss: 646.4251\n",
      "Epoch [140/200], Loss: 701.6513\n",
      "Epoch [150/200], Loss: 597.4581\n",
      "Epoch [160/200], Loss: 576.2303\n",
      "Epoch [170/200], Loss: 578.8990\n",
      "Epoch [180/200], Loss: 609.4913\n",
      "Epoch [190/200], Loss: 557.1221\n",
      "Epoch [200/200], Loss: 655.0160\n",
      "Epoch [10/200], Loss: 4744.6436\n",
      "Epoch [20/200], Loss: 4657.0950\n",
      "Epoch [30/200], Loss: 4397.6647\n",
      "Epoch [40/200], Loss: 4487.9400\n",
      "Epoch [50/200], Loss: 3782.1882\n",
      "Epoch [60/200], Loss: 3817.5773\n",
      "Epoch [70/200], Loss: 2729.8788\n",
      "Epoch [80/200], Loss: 2046.8971\n",
      "Epoch [90/200], Loss: 1504.4487\n",
      "Epoch [100/200], Loss: 1084.4798\n",
      "Epoch [110/200], Loss: 721.1303\n",
      "Epoch [120/200], Loss: 604.0031\n",
      "Epoch [130/200], Loss: 703.3676\n",
      "Epoch [140/200], Loss: 580.9443\n",
      "Epoch [150/200], Loss: 550.9003\n",
      "Epoch [160/200], Loss: 586.3974\n",
      "Epoch [170/200], Loss: 839.3251\n",
      "Epoch [180/200], Loss: 743.4683\n",
      "Epoch [190/200], Loss: 571.8699\n",
      "Epoch [200/200], Loss: 628.1823\n",
      "Epoch [10/200], Loss: 3845.2789\n",
      "Epoch [20/200], Loss: 5127.9927\n",
      "Epoch [30/200], Loss: 4305.2281\n",
      "Epoch [40/200], Loss: 3697.9673\n",
      "Epoch [50/200], Loss: 3620.5300\n",
      "Epoch [60/200], Loss: 2424.2055\n",
      "Epoch [70/200], Loss: 1604.0241\n",
      "Epoch [80/200], Loss: 1418.9923\n",
      "Epoch [90/200], Loss: 705.8564\n",
      "Epoch [100/200], Loss: 795.3691\n",
      "Epoch [110/200], Loss: 571.5788\n",
      "Epoch [120/200], Loss: 651.1144\n",
      "Epoch [130/200], Loss: 590.0560\n",
      "Epoch [140/200], Loss: 663.3888\n",
      "Epoch [150/200], Loss: 534.1884\n",
      "Epoch [160/200], Loss: 564.4653\n",
      "Epoch [170/200], Loss: 639.9800\n",
      "Epoch [180/200], Loss: 550.4165\n",
      "Epoch [190/200], Loss: 636.6178\n",
      "Epoch [200/200], Loss: 579.9360\n",
      "Epoch [10/200], Loss: 4786.8822\n",
      "Epoch [20/200], Loss: 4697.7673\n",
      "Epoch [30/200], Loss: 4106.5410\n",
      "Epoch [40/200], Loss: 4187.8490\n",
      "Epoch [50/200], Loss: 3112.2125\n",
      "Epoch [60/200], Loss: 1836.1113\n",
      "Epoch [70/200], Loss: 1194.1621\n",
      "Epoch [80/200], Loss: 700.2129\n",
      "Epoch [90/200], Loss: 665.7281\n",
      "Epoch [100/200], Loss: 742.5243\n",
      "Epoch [110/200], Loss: 582.3454\n",
      "Epoch [120/200], Loss: 697.8580\n",
      "Epoch [130/200], Loss: 615.1804\n",
      "Epoch [140/200], Loss: 610.5273\n",
      "Epoch [150/200], Loss: 699.1939\n",
      "Epoch [160/200], Loss: 707.5824\n",
      "Epoch [170/200], Loss: 632.6613\n",
      "Epoch [180/200], Loss: 547.9197\n",
      "Epoch [190/200], Loss: 558.1296\n",
      "Epoch [200/200], Loss: 633.2966\n",
      "Epoch [10/200], Loss: 4120.8683\n",
      "Epoch [20/200], Loss: 4114.8126\n",
      "Epoch [30/200], Loss: 4199.4473\n",
      "Epoch [40/200], Loss: 3980.0139\n",
      "Epoch [50/200], Loss: 3243.7249\n",
      "Epoch [60/200], Loss: 2352.7146\n",
      "Epoch [70/200], Loss: 1778.9927\n",
      "Epoch [80/200], Loss: 917.7789\n",
      "Epoch [90/200], Loss: 786.8568\n",
      "Epoch [100/200], Loss: 586.1947\n",
      "Epoch [110/200], Loss: 615.5628\n",
      "Epoch [120/200], Loss: 638.9729\n",
      "Epoch [130/200], Loss: 600.3271\n",
      "Epoch [140/200], Loss: 578.4005\n",
      "Epoch [150/200], Loss: 649.9579\n",
      "Epoch [160/200], Loss: 707.2678\n",
      "Epoch [170/200], Loss: 551.8794\n",
      "Epoch [180/200], Loss: 561.5246\n",
      "Epoch [190/200], Loss: 666.2004\n",
      "Epoch [200/200], Loss: 576.7136\n",
      "Epoch [10/200], Loss: 4760.7960\n",
      "Epoch [20/200], Loss: 4385.7411\n",
      "Epoch [30/200], Loss: 4184.1918\n",
      "Epoch [40/200], Loss: 3574.2351\n",
      "Epoch [50/200], Loss: 3125.1183\n",
      "Epoch [60/200], Loss: 2684.9110\n",
      "Epoch [70/200], Loss: 1549.7271\n",
      "Epoch [80/200], Loss: 905.4111\n",
      "Epoch [90/200], Loss: 695.2948\n",
      "Epoch [100/200], Loss: 634.1493\n",
      "Epoch [110/200], Loss: 573.6389\n",
      "Epoch [120/200], Loss: 639.3934\n",
      "Epoch [130/200], Loss: 556.8033\n",
      "Epoch [140/200], Loss: 658.3132\n",
      "Epoch [150/200], Loss: 687.8991\n",
      "Epoch [160/200], Loss: 598.9709\n",
      "Epoch [170/200], Loss: 658.0028\n",
      "Epoch [180/200], Loss: 568.5017\n",
      "Epoch [190/200], Loss: 538.1691\n",
      "Epoch [200/200], Loss: 563.6638\n",
      "Epoch [10/200], Loss: 4348.3330\n",
      "Epoch [20/200], Loss: 4005.4677\n",
      "Epoch [30/200], Loss: 4138.8416\n",
      "Epoch [40/200], Loss: 3985.6118\n",
      "Epoch [50/200], Loss: 2948.8605\n",
      "Epoch [60/200], Loss: 2291.1196\n",
      "Epoch [70/200], Loss: 1510.3452\n",
      "Epoch [80/200], Loss: 1019.4867\n",
      "Epoch [90/200], Loss: 653.8132\n",
      "Epoch [100/200], Loss: 712.1814\n",
      "Epoch [110/200], Loss: 593.8440\n",
      "Epoch [120/200], Loss: 664.4360\n",
      "Epoch [130/200], Loss: 579.2333\n",
      "Epoch [140/200], Loss: 561.5638\n",
      "Epoch [150/200], Loss: 569.9793\n",
      "Epoch [160/200], Loss: 524.8420\n",
      "Epoch [170/200], Loss: 646.4305\n",
      "Epoch [180/200], Loss: 607.1955\n",
      "Epoch [190/200], Loss: 558.1790\n",
      "Epoch [200/200], Loss: 694.4255\n",
      "Epoch [10/200], Loss: 4835.7843\n",
      "Epoch [20/200], Loss: 4485.1348\n",
      "Epoch [30/200], Loss: 3984.0083\n",
      "Epoch [40/200], Loss: 3562.2681\n",
      "Epoch [50/200], Loss: 4091.6301\n",
      "Epoch [60/200], Loss: 2256.5775\n",
      "Epoch [70/200], Loss: 1705.8109\n",
      "Epoch [80/200], Loss: 1111.6879\n",
      "Epoch [90/200], Loss: 759.4459\n",
      "Epoch [100/200], Loss: 641.7167\n",
      "Epoch [110/200], Loss: 656.2472\n",
      "Epoch [120/200], Loss: 600.3100\n",
      "Epoch [130/200], Loss: 495.5621\n",
      "Epoch [140/200], Loss: 629.2813\n",
      "Epoch [150/200], Loss: 571.7340\n",
      "Epoch [160/200], Loss: 520.7225\n",
      "Epoch [170/200], Loss: 613.9756\n",
      "Epoch [180/200], Loss: 622.8692\n",
      "Epoch [190/200], Loss: 499.8022\n",
      "Epoch [200/200], Loss: 624.2828\n",
      "Epoch [10/200], Loss: 4501.9816\n",
      "Epoch [20/200], Loss: 4239.6451\n",
      "Epoch [30/200], Loss: 4254.0471\n",
      "Epoch [40/200], Loss: 3550.9565\n",
      "Epoch [50/200], Loss: 3220.0608\n",
      "Epoch [60/200], Loss: 2492.3883\n",
      "Epoch [70/200], Loss: 1619.6844\n",
      "Epoch [80/200], Loss: 1062.8708\n",
      "Epoch [90/200], Loss: 666.4701\n",
      "Epoch [100/200], Loss: 651.7539\n",
      "Epoch [110/200], Loss: 700.7615\n",
      "Epoch [120/200], Loss: 566.8227\n",
      "Epoch [130/200], Loss: 546.0536\n",
      "Epoch [140/200], Loss: 620.7443\n",
      "Epoch [150/200], Loss: 725.3887\n",
      "Epoch [160/200], Loss: 645.4899\n",
      "Epoch [170/200], Loss: 628.3162\n",
      "Epoch [180/200], Loss: 513.7842\n",
      "Epoch [190/200], Loss: 542.9040\n",
      "Epoch [200/200], Loss: 642.9861\n",
      "Epoch [10/200], Loss: 4813.7570\n",
      "Epoch [20/200], Loss: 4439.0813\n",
      "Epoch [30/200], Loss: 4156.0370\n",
      "Epoch [40/200], Loss: 3951.9698\n",
      "Epoch [50/200], Loss: 3731.6008\n",
      "Epoch [60/200], Loss: 2961.7231\n",
      "Epoch [70/200], Loss: 2270.1797\n",
      "Epoch [80/200], Loss: 1575.9857\n",
      "Epoch [90/200], Loss: 1049.0014\n",
      "Epoch [100/200], Loss: 781.0237\n",
      "Epoch [110/200], Loss: 628.0506\n",
      "Epoch [120/200], Loss: 668.0838\n",
      "Epoch [130/200], Loss: 646.3739\n",
      "Epoch [140/200], Loss: 547.3348\n",
      "Epoch [150/200], Loss: 529.3593\n",
      "Epoch [160/200], Loss: 599.1079\n",
      "Epoch [170/200], Loss: 727.5887\n",
      "Epoch [180/200], Loss: 606.1064\n",
      "Epoch [190/200], Loss: 591.2059\n",
      "Epoch [200/200], Loss: 665.0721\n",
      "Epoch [10/200], Loss: 4391.3439\n",
      "Epoch [20/200], Loss: 4456.6229\n",
      "Epoch [30/200], Loss: 3682.4156\n",
      "Epoch [40/200], Loss: 3588.3671\n",
      "Epoch [50/200], Loss: 2738.9979\n",
      "Epoch [60/200], Loss: 2285.9936\n",
      "Epoch [70/200], Loss: 1146.8997\n",
      "Epoch [80/200], Loss: 747.2292\n",
      "Epoch [90/200], Loss: 580.0703\n",
      "Epoch [100/200], Loss: 590.9896\n",
      "Epoch [110/200], Loss: 618.5091\n",
      "Epoch [120/200], Loss: 627.1504\n",
      "Epoch [130/200], Loss: 533.6639\n",
      "Epoch [140/200], Loss: 654.4087\n",
      "Epoch [150/200], Loss: 588.9979\n",
      "Epoch [160/200], Loss: 556.6809\n",
      "Epoch [170/200], Loss: 598.5629\n",
      "Epoch [180/200], Loss: 604.1717\n",
      "Epoch [190/200], Loss: 625.4971\n",
      "Epoch [200/200], Loss: 672.4334\n",
      "Epoch [10/200], Loss: 4287.2675\n",
      "Epoch [20/200], Loss: 4309.3300\n",
      "Epoch [30/200], Loss: 4020.5153\n",
      "Epoch [40/200], Loss: 3682.8509\n",
      "Epoch [50/200], Loss: 2779.2531\n",
      "Epoch [60/200], Loss: 1816.2770\n",
      "Epoch [70/200], Loss: 1079.8641\n",
      "Epoch [80/200], Loss: 601.3977\n",
      "Epoch [90/200], Loss: 706.6664\n",
      "Epoch [100/200], Loss: 655.2805\n",
      "Epoch [110/200], Loss: 575.1985\n",
      "Epoch [120/200], Loss: 614.3065\n",
      "Epoch [130/200], Loss: 707.8904\n",
      "Epoch [140/200], Loss: 582.4127\n",
      "Epoch [150/200], Loss: 660.6088\n",
      "Epoch [160/200], Loss: 672.1290\n",
      "Epoch [170/200], Loss: 640.5569\n",
      "Epoch [180/200], Loss: 705.2790\n",
      "Epoch [190/200], Loss: 618.7626\n",
      "Epoch [200/200], Loss: 585.1976\n",
      "Epoch [10/200], Loss: 4561.7290\n",
      "Epoch [20/200], Loss: 4611.1926\n",
      "Epoch [30/200], Loss: 4218.2416\n",
      "Epoch [40/200], Loss: 4256.8701\n",
      "Epoch [50/200], Loss: 4400.1288\n",
      "Epoch [60/200], Loss: 3381.4382\n",
      "Epoch [70/200], Loss: 3035.8391\n",
      "Epoch [80/200], Loss: 1754.3445\n",
      "Epoch [90/200], Loss: 1226.9030\n",
      "Epoch [100/200], Loss: 845.5822\n",
      "Epoch [110/200], Loss: 707.5338\n",
      "Epoch [120/200], Loss: 577.9067\n",
      "Epoch [130/200], Loss: 535.5960\n",
      "Epoch [140/200], Loss: 571.1736\n",
      "Epoch [150/200], Loss: 571.2263\n",
      "Epoch [160/200], Loss: 604.2196\n",
      "Epoch [170/200], Loss: 530.0899\n",
      "Epoch [180/200], Loss: 614.1043\n",
      "Epoch [190/200], Loss: 596.3025\n",
      "Epoch [200/200], Loss: 607.9061\n",
      "Epoch [10/200], Loss: 4405.5599\n",
      "Epoch [20/200], Loss: 4453.5411\n",
      "Epoch [30/200], Loss: 5057.9838\n",
      "Epoch [40/200], Loss: 5003.7101\n",
      "Epoch [50/200], Loss: 4769.0900\n",
      "Epoch [60/200], Loss: 3966.6245\n",
      "Epoch [70/200], Loss: 4043.4097\n",
      "Epoch [80/200], Loss: 4520.6941\n",
      "Epoch [90/200], Loss: 4999.0555\n",
      "Epoch [100/200], Loss: 4554.6085\n",
      "Epoch [110/200], Loss: 4215.2738\n",
      "Epoch [120/200], Loss: 4726.5427\n",
      "Epoch [130/200], Loss: 5000.4563\n",
      "Epoch [140/200], Loss: 3821.1773\n",
      "Epoch [150/200], Loss: 4814.1516\n",
      "Epoch [160/200], Loss: 4917.7078\n",
      "Epoch [170/200], Loss: 4349.3009\n",
      "Epoch [180/200], Loss: 4505.4470\n",
      "Epoch [190/200], Loss: 4707.5024\n",
      "Epoch [200/200], Loss: 4823.7343\n",
      "Epoch [10/200], Loss: 4745.2245\n",
      "Epoch [20/200], Loss: 4740.1448\n",
      "Epoch [30/200], Loss: 4681.7445\n",
      "Epoch [40/200], Loss: 3394.1594\n",
      "Epoch [50/200], Loss: 2942.0594\n",
      "Epoch [60/200], Loss: 2361.6005\n",
      "Epoch [70/200], Loss: 1347.9614\n",
      "Epoch [80/200], Loss: 854.8760\n",
      "Epoch [90/200], Loss: 690.0645\n",
      "Epoch [100/200], Loss: 529.8111\n",
      "Epoch [110/200], Loss: 720.1664\n",
      "Epoch [120/200], Loss: 535.7222\n",
      "Epoch [130/200], Loss: 666.3213\n",
      "Epoch [140/200], Loss: 682.8610\n",
      "Epoch [150/200], Loss: 543.7577\n",
      "Epoch [160/200], Loss: 651.8072\n",
      "Epoch [170/200], Loss: 674.3757\n",
      "Epoch [180/200], Loss: 678.5651\n",
      "Epoch [190/200], Loss: 598.9588\n",
      "Epoch [200/200], Loss: 538.5047\n",
      "Epoch [10/200], Loss: 4619.8748\n",
      "Epoch [20/200], Loss: 4066.9596\n",
      "Epoch [30/200], Loss: 4272.7645\n",
      "Epoch [40/200], Loss: 3501.7144\n",
      "Epoch [50/200], Loss: 3316.1838\n",
      "Epoch [60/200], Loss: 3032.9984\n",
      "Epoch [70/200], Loss: 1465.1767\n",
      "Epoch [80/200], Loss: 1059.6803\n",
      "Epoch [90/200], Loss: 754.7039\n",
      "Epoch [100/200], Loss: 679.6951\n",
      "Epoch [110/200], Loss: 613.7229\n",
      "Epoch [120/200], Loss: 634.0941\n",
      "Epoch [130/200], Loss: 538.4521\n",
      "Epoch [140/200], Loss: 738.4907\n",
      "Epoch [150/200], Loss: 646.9655\n",
      "Epoch [160/200], Loss: 602.4240\n",
      "Epoch [170/200], Loss: 557.1380\n",
      "Epoch [180/200], Loss: 538.1642\n",
      "Epoch [190/200], Loss: 648.9621\n",
      "Epoch [200/200], Loss: 571.2023\n",
      "Epoch [10/200], Loss: 4859.5656\n",
      "Epoch [20/200], Loss: 4092.7627\n",
      "Epoch [30/200], Loss: 4178.6286\n",
      "Epoch [40/200], Loss: 3984.2785\n",
      "Epoch [50/200], Loss: 3754.6832\n",
      "Epoch [60/200], Loss: 2954.6617\n",
      "Epoch [70/200], Loss: 1884.7690\n",
      "Epoch [80/200], Loss: 1258.0351\n",
      "Epoch [90/200], Loss: 897.9130\n",
      "Epoch [100/200], Loss: 682.4126\n",
      "Epoch [110/200], Loss: 588.2785\n",
      "Epoch [120/200], Loss: 565.5940\n",
      "Epoch [130/200], Loss: 554.3913\n",
      "Epoch [140/200], Loss: 680.6190\n",
      "Epoch [150/200], Loss: 628.6843\n",
      "Epoch [160/200], Loss: 565.8855\n",
      "Epoch [170/200], Loss: 613.9056\n",
      "Epoch [180/200], Loss: 668.2941\n",
      "Epoch [190/200], Loss: 610.3668\n",
      "Epoch [200/200], Loss: 601.2398\n",
      "Epoch [10/200], Loss: 4721.3202\n",
      "Epoch [20/200], Loss: 4616.8958\n",
      "Epoch [30/200], Loss: 3952.0928\n",
      "Epoch [40/200], Loss: 3205.0227\n",
      "Epoch [50/200], Loss: 2947.5233\n",
      "Epoch [60/200], Loss: 1915.8982\n",
      "Epoch [70/200], Loss: 1172.9051\n",
      "Epoch [80/200], Loss: 891.4462\n",
      "Epoch [90/200], Loss: 619.2852\n",
      "Epoch [100/200], Loss: 606.4695\n",
      "Epoch [110/200], Loss: 600.6484\n",
      "Epoch [120/200], Loss: 676.4222\n",
      "Epoch [130/200], Loss: 632.5117\n",
      "Epoch [140/200], Loss: 708.8210\n",
      "Epoch [150/200], Loss: 726.5902\n",
      "Epoch [160/200], Loss: 620.4751\n",
      "Epoch [170/200], Loss: 579.7693\n",
      "Epoch [180/200], Loss: 612.3434\n",
      "Epoch [190/200], Loss: 570.6498\n",
      "Epoch [200/200], Loss: 591.4914\n",
      "Epoch [10/200], Loss: 4751.0403\n",
      "Epoch [20/200], Loss: 4506.2251\n",
      "Epoch [30/200], Loss: 4064.0172\n",
      "Epoch [40/200], Loss: 3380.8510\n",
      "Epoch [50/200], Loss: 2708.4103\n",
      "Epoch [60/200], Loss: 1714.8122\n",
      "Epoch [70/200], Loss: 1152.9015\n",
      "Epoch [80/200], Loss: 774.5403\n",
      "Epoch [90/200], Loss: 715.4411\n",
      "Epoch [100/200], Loss: 555.8543\n",
      "Epoch [110/200], Loss: 623.2309\n",
      "Epoch [120/200], Loss: 584.3422\n",
      "Epoch [130/200], Loss: 715.6512\n",
      "Epoch [140/200], Loss: 783.1435\n",
      "Epoch [150/200], Loss: 545.6688\n",
      "Epoch [160/200], Loss: 594.6710\n",
      "Epoch [170/200], Loss: 518.7579\n",
      "Epoch [180/200], Loss: 566.8865\n",
      "Epoch [190/200], Loss: 572.7145\n",
      "Epoch [200/200], Loss: 634.6221\n",
      "Epoch [10/200], Loss: 4672.9702\n",
      "Epoch [20/200], Loss: 4377.0370\n",
      "Epoch [30/200], Loss: 4341.0551\n",
      "Epoch [40/200], Loss: 3735.5000\n",
      "Epoch [50/200], Loss: 3277.0743\n",
      "Epoch [60/200], Loss: 2383.4884\n",
      "Epoch [70/200], Loss: 1203.6993\n",
      "Epoch [80/200], Loss: 1097.6109\n",
      "Epoch [90/200], Loss: 756.7709\n",
      "Epoch [100/200], Loss: 621.2297\n",
      "Epoch [110/200], Loss: 600.7991\n",
      "Epoch [120/200], Loss: 710.7727\n",
      "Epoch [130/200], Loss: 597.5519\n",
      "Epoch [140/200], Loss: 572.1673\n",
      "Epoch [150/200], Loss: 572.2879\n",
      "Epoch [160/200], Loss: 611.3862\n",
      "Epoch [170/200], Loss: 636.9588\n",
      "Epoch [180/200], Loss: 754.2784\n",
      "Epoch [190/200], Loss: 653.8800\n",
      "Epoch [200/200], Loss: 565.0162\n",
      "Epoch [10/200], Loss: 4595.8798\n",
      "Epoch [20/200], Loss: 4576.6910\n",
      "Epoch [30/200], Loss: 4352.4891\n",
      "Epoch [40/200], Loss: 3828.5031\n",
      "Epoch [50/200], Loss: 3322.7473\n",
      "Epoch [60/200], Loss: 3385.7578\n",
      "Epoch [70/200], Loss: 2298.5865\n",
      "Epoch [80/200], Loss: 1319.5021\n",
      "Epoch [90/200], Loss: 1129.3614\n",
      "Epoch [100/200], Loss: 788.5249\n",
      "Epoch [110/200], Loss: 624.9527\n",
      "Epoch [120/200], Loss: 691.9235\n",
      "Epoch [130/200], Loss: 591.3892\n",
      "Epoch [140/200], Loss: 605.4038\n",
      "Epoch [150/200], Loss: 645.1941\n",
      "Epoch [160/200], Loss: 527.3826\n",
      "Epoch [170/200], Loss: 535.9747\n",
      "Epoch [180/200], Loss: 576.7990\n",
      "Epoch [190/200], Loss: 519.2244\n",
      "Epoch [200/200], Loss: 639.4294\n",
      "Epoch [10/200], Loss: 5280.5118\n",
      "Epoch [20/200], Loss: 4408.4292\n",
      "Epoch [30/200], Loss: 4085.6694\n",
      "Epoch [40/200], Loss: 3273.2787\n",
      "Epoch [50/200], Loss: 2987.8497\n",
      "Epoch [60/200], Loss: 2541.5117\n",
      "Epoch [70/200], Loss: 1399.4277\n",
      "Epoch [80/200], Loss: 951.5332\n",
      "Epoch [90/200], Loss: 744.1134\n",
      "Epoch [100/200], Loss: 708.2724\n",
      "Epoch [110/200], Loss: 653.0648\n",
      "Epoch [120/200], Loss: 653.3989\n",
      "Epoch [130/200], Loss: 594.3435\n",
      "Epoch [140/200], Loss: 618.0327\n",
      "Epoch [150/200], Loss: 633.3334\n",
      "Epoch [160/200], Loss: 598.5811\n",
      "Epoch [170/200], Loss: 567.6403\n",
      "Epoch [180/200], Loss: 707.7973\n",
      "Epoch [190/200], Loss: 591.8295\n",
      "Epoch [200/200], Loss: 646.2147\n",
      "Epoch [10/200], Loss: 4585.7208\n",
      "Epoch [20/200], Loss: 4170.8530\n",
      "Epoch [30/200], Loss: 4554.4828\n",
      "Epoch [40/200], Loss: 3960.3182\n",
      "Epoch [50/200], Loss: 3071.7363\n",
      "Epoch [60/200], Loss: 2201.4168\n",
      "Epoch [70/200], Loss: 1444.8284\n",
      "Epoch [80/200], Loss: 982.2730\n",
      "Epoch [90/200], Loss: 832.1827\n",
      "Epoch [100/200], Loss: 649.9471\n",
      "Epoch [110/200], Loss: 598.3978\n",
      "Epoch [120/200], Loss: 613.7970\n",
      "Epoch [130/200], Loss: 636.4161\n",
      "Epoch [140/200], Loss: 634.9773\n",
      "Epoch [150/200], Loss: 623.5967\n",
      "Epoch [160/200], Loss: 513.3270\n",
      "Epoch [170/200], Loss: 723.5792\n",
      "Epoch [180/200], Loss: 756.3624\n",
      "Epoch [190/200], Loss: 640.3125\n",
      "Epoch [200/200], Loss: 676.1096\n",
      "Epoch [10/200], Loss: 4713.9744\n",
      "Epoch [20/200], Loss: 4853.5613\n",
      "Epoch [30/200], Loss: 4783.9702\n",
      "Epoch [40/200], Loss: 3384.4991\n",
      "Epoch [50/200], Loss: 2877.8497\n",
      "Epoch [60/200], Loss: 2416.7629\n",
      "Epoch [70/200], Loss: 1688.5371\n",
      "Epoch [80/200], Loss: 812.2347\n",
      "Epoch [90/200], Loss: 803.6600\n",
      "Epoch [100/200], Loss: 571.6104\n",
      "Epoch [110/200], Loss: 580.7040\n",
      "Epoch [120/200], Loss: 530.9960\n",
      "Epoch [130/200], Loss: 624.2532\n",
      "Epoch [140/200], Loss: 590.3060\n",
      "Epoch [150/200], Loss: 565.2135\n",
      "Epoch [160/200], Loss: 548.0895\n",
      "Epoch [170/200], Loss: 676.7860\n",
      "Epoch [180/200], Loss: 621.2831\n",
      "Epoch [190/200], Loss: 656.6441\n",
      "Epoch [200/200], Loss: 592.1058\n",
      "Epoch [10/200], Loss: 4892.1743\n",
      "Epoch [20/200], Loss: 4197.0354\n",
      "Epoch [30/200], Loss: 4157.2049\n",
      "Epoch [40/200], Loss: 3981.5048\n",
      "Epoch [50/200], Loss: 2902.7673\n",
      "Epoch [60/200], Loss: 3203.4671\n",
      "Epoch [70/200], Loss: 2066.8269\n",
      "Epoch [80/200], Loss: 1336.5734\n",
      "Epoch [90/200], Loss: 728.1852\n",
      "Epoch [100/200], Loss: 748.3148\n",
      "Epoch [110/200], Loss: 611.3227\n",
      "Epoch [120/200], Loss: 590.3399\n",
      "Epoch [130/200], Loss: 508.3344\n",
      "Epoch [140/200], Loss: 639.8326\n",
      "Epoch [150/200], Loss: 637.9655\n",
      "Epoch [160/200], Loss: 632.1594\n",
      "Epoch [170/200], Loss: 682.7209\n",
      "Epoch [180/200], Loss: 650.4016\n",
      "Epoch [190/200], Loss: 629.6894\n",
      "Epoch [200/200], Loss: 689.9314\n",
      "Epoch [10/200], Loss: 4661.7114\n",
      "Epoch [20/200], Loss: 5159.2712\n",
      "Epoch [30/200], Loss: 4223.1719\n",
      "Epoch [40/200], Loss: 3626.9402\n",
      "Epoch [50/200], Loss: 2843.6516\n",
      "Epoch [60/200], Loss: 2100.4342\n",
      "Epoch [70/200], Loss: 1616.7266\n",
      "Epoch [80/200], Loss: 992.0002\n",
      "Epoch [90/200], Loss: 661.2828\n",
      "Epoch [100/200], Loss: 625.4124\n",
      "Epoch [110/200], Loss: 571.3531\n",
      "Epoch [120/200], Loss: 542.3159\n",
      "Epoch [130/200], Loss: 720.4968\n",
      "Epoch [140/200], Loss: 535.1243\n",
      "Epoch [150/200], Loss: 724.3832\n",
      "Epoch [160/200], Loss: 651.0540\n",
      "Epoch [170/200], Loss: 567.9501\n",
      "Epoch [180/200], Loss: 549.0172\n",
      "Epoch [190/200], Loss: 694.4120\n",
      "Epoch [200/200], Loss: 558.0595\n",
      "Epoch [10/200], Loss: 4581.5406\n",
      "Epoch [20/200], Loss: 4463.2643\n",
      "Epoch [30/200], Loss: 4270.2238\n",
      "Epoch [40/200], Loss: 3725.7895\n",
      "Epoch [50/200], Loss: 2460.9797\n",
      "Epoch [60/200], Loss: 1909.0697\n",
      "Epoch [70/200], Loss: 1221.5969\n",
      "Epoch [80/200], Loss: 766.8270\n",
      "Epoch [90/200], Loss: 696.8123\n",
      "Epoch [100/200], Loss: 619.6319\n",
      "Epoch [110/200], Loss: 695.8761\n",
      "Epoch [120/200], Loss: 638.6276\n",
      "Epoch [130/200], Loss: 621.2385\n",
      "Epoch [140/200], Loss: 551.0912\n",
      "Epoch [150/200], Loss: 588.7480\n",
      "Epoch [160/200], Loss: 677.6256\n",
      "Epoch [170/200], Loss: 547.7579\n",
      "Epoch [180/200], Loss: 555.1099\n",
      "Epoch [190/200], Loss: 599.9307\n",
      "Epoch [200/200], Loss: 634.4485\n",
      "Epoch [10/200], Loss: 4303.7163\n",
      "Epoch [20/200], Loss: 4182.4503\n",
      "Epoch [30/200], Loss: 3973.3742\n",
      "Epoch [40/200], Loss: 3755.2634\n",
      "Epoch [50/200], Loss: 3241.1320\n",
      "Epoch [60/200], Loss: 1890.4441\n",
      "Epoch [70/200], Loss: 1436.7292\n",
      "Epoch [80/200], Loss: 756.9293\n",
      "Epoch [90/200], Loss: 681.8533\n",
      "Epoch [100/200], Loss: 596.6292\n",
      "Epoch [110/200], Loss: 601.7132\n",
      "Epoch [120/200], Loss: 730.3340\n",
      "Epoch [130/200], Loss: 552.0729\n",
      "Epoch [140/200], Loss: 668.6490\n",
      "Epoch [150/200], Loss: 625.1965\n",
      "Epoch [160/200], Loss: 644.0013\n",
      "Epoch [170/200], Loss: 582.7114\n",
      "Epoch [180/200], Loss: 723.9252\n",
      "Epoch [190/200], Loss: 566.4455\n",
      "Epoch [200/200], Loss: 542.8585\n",
      "Epoch [10/200], Loss: 4279.2794\n",
      "Epoch [20/200], Loss: 4198.7119\n",
      "Epoch [30/200], Loss: 4082.7402\n",
      "Epoch [40/200], Loss: 3390.0588\n",
      "Epoch [50/200], Loss: 2775.9346\n",
      "Epoch [60/200], Loss: 1926.0243\n",
      "Epoch [70/200], Loss: 917.6069\n",
      "Epoch [80/200], Loss: 688.1191\n",
      "Epoch [90/200], Loss: 690.6822\n",
      "Epoch [100/200], Loss: 627.8815\n",
      "Epoch [110/200], Loss: 759.8199\n",
      "Epoch [120/200], Loss: 688.1263\n",
      "Epoch [130/200], Loss: 658.2940\n",
      "Epoch [140/200], Loss: 633.4017\n",
      "Epoch [150/200], Loss: 617.0531\n",
      "Epoch [160/200], Loss: 633.8225\n",
      "Epoch [170/200], Loss: 576.4816\n",
      "Epoch [180/200], Loss: 562.5868\n",
      "Epoch [190/200], Loss: 600.7838\n",
      "Epoch [200/200], Loss: 542.8809\n",
      "Epoch [10/200], Loss: 4430.3553\n",
      "Epoch [20/200], Loss: 4181.2357\n",
      "Epoch [30/200], Loss: 4926.3136\n",
      "Epoch [40/200], Loss: 3843.3781\n",
      "Epoch [50/200], Loss: 3189.8177\n",
      "Epoch [60/200], Loss: 3225.6292\n",
      "Epoch [70/200], Loss: 2037.4278\n",
      "Epoch [80/200], Loss: 1499.5750\n",
      "Epoch [90/200], Loss: 923.7861\n",
      "Epoch [100/200], Loss: 890.3620\n",
      "Epoch [110/200], Loss: 567.7471\n",
      "Epoch [120/200], Loss: 593.5883\n",
      "Epoch [130/200], Loss: 616.9606\n",
      "Epoch [140/200], Loss: 650.1839\n",
      "Epoch [150/200], Loss: 551.0558\n",
      "Epoch [160/200], Loss: 554.8822\n",
      "Epoch [170/200], Loss: 522.3194\n",
      "Epoch [180/200], Loss: 656.2008\n",
      "Epoch [190/200], Loss: 636.8776\n",
      "Epoch [200/200], Loss: 648.4467\n",
      "Epoch [10/200], Loss: 4730.9380\n",
      "Epoch [20/200], Loss: 4271.3140\n",
      "Epoch [30/200], Loss: 5056.1485\n",
      "Epoch [40/200], Loss: 3883.6688\n",
      "Epoch [50/200], Loss: 2774.5499\n",
      "Epoch [60/200], Loss: 2434.0221\n",
      "Epoch [70/200], Loss: 1775.3502\n",
      "Epoch [80/200], Loss: 1226.6114\n",
      "Epoch [90/200], Loss: 875.3174\n",
      "Epoch [100/200], Loss: 639.9810\n",
      "Epoch [110/200], Loss: 564.4924\n",
      "Epoch [120/200], Loss: 549.5943\n",
      "Epoch [130/200], Loss: 586.0336\n",
      "Epoch [140/200], Loss: 527.3323\n",
      "Epoch [150/200], Loss: 597.2012\n",
      "Epoch [160/200], Loss: 519.8696\n",
      "Epoch [170/200], Loss: 561.4977\n",
      "Epoch [180/200], Loss: 583.9326\n",
      "Epoch [190/200], Loss: 662.6150\n",
      "Epoch [200/200], Loss: 700.8549\n",
      "Epoch [10/200], Loss: 4073.0585\n",
      "Epoch [20/200], Loss: 4983.1101\n",
      "Epoch [30/200], Loss: 3937.5898\n",
      "Epoch [40/200], Loss: 4219.3945\n",
      "Epoch [50/200], Loss: 3476.2207\n",
      "Epoch [60/200], Loss: 2120.9609\n",
      "Epoch [70/200], Loss: 1198.5280\n",
      "Epoch [80/200], Loss: 1009.6141\n",
      "Epoch [90/200], Loss: 614.8861\n",
      "Epoch [100/200], Loss: 645.5051\n",
      "Epoch [110/200], Loss: 591.6733\n",
      "Epoch [120/200], Loss: 609.7212\n",
      "Epoch [130/200], Loss: 661.0361\n",
      "Epoch [140/200], Loss: 587.4831\n",
      "Epoch [150/200], Loss: 622.9118\n",
      "Epoch [160/200], Loss: 669.8318\n",
      "Epoch [170/200], Loss: 526.4870\n",
      "Epoch [180/200], Loss: 681.3151\n",
      "Epoch [190/200], Loss: 648.3398\n",
      "Epoch [200/200], Loss: 524.9627\n",
      "Epoch [10/200], Loss: 4217.7645\n",
      "Epoch [20/200], Loss: 4252.5474\n",
      "Epoch [30/200], Loss: 4613.9441\n",
      "Epoch [40/200], Loss: 3742.3646\n",
      "Epoch [50/200], Loss: 2593.1830\n",
      "Epoch [60/200], Loss: 2095.1808\n",
      "Epoch [70/200], Loss: 1403.1549\n",
      "Epoch [80/200], Loss: 755.6650\n",
      "Epoch [90/200], Loss: 639.1024\n",
      "Epoch [100/200], Loss: 730.7949\n",
      "Epoch [110/200], Loss: 642.2447\n",
      "Epoch [120/200], Loss: 652.8219\n",
      "Epoch [130/200], Loss: 528.5114\n",
      "Epoch [140/200], Loss: 668.3751\n",
      "Epoch [150/200], Loss: 641.9377\n",
      "Epoch [160/200], Loss: 684.7271\n",
      "Epoch [170/200], Loss: 558.8151\n",
      "Epoch [180/200], Loss: 518.1541\n",
      "Epoch [190/200], Loss: 651.2760\n",
      "Epoch [200/200], Loss: 601.7353\n",
      "Epoch [10/200], Loss: 5241.7958\n",
      "Epoch [20/200], Loss: 4539.1752\n",
      "Epoch [30/200], Loss: 4318.7769\n",
      "Epoch [40/200], Loss: 3609.7125\n",
      "Epoch [50/200], Loss: 3824.5932\n",
      "Epoch [60/200], Loss: 2814.2725\n",
      "Epoch [70/200], Loss: 2468.9230\n",
      "Epoch [80/200], Loss: 1832.6964\n",
      "Epoch [90/200], Loss: 1141.5257\n",
      "Epoch [100/200], Loss: 1036.5823\n",
      "Epoch [110/200], Loss: 725.3980\n",
      "Epoch [120/200], Loss: 561.0895\n",
      "Epoch [130/200], Loss: 801.6076\n",
      "Epoch [140/200], Loss: 639.7602\n",
      "Epoch [150/200], Loss: 620.5812\n",
      "Epoch [160/200], Loss: 536.2641\n",
      "Epoch [170/200], Loss: 549.9904\n",
      "Epoch [180/200], Loss: 622.4175\n",
      "Epoch [190/200], Loss: 633.0023\n",
      "Epoch [200/200], Loss: 710.7398\n",
      "Epoch [10/200], Loss: 4751.6963\n",
      "Epoch [20/200], Loss: 4490.4268\n",
      "Epoch [30/200], Loss: 3454.7456\n",
      "Epoch [40/200], Loss: 3538.8388\n",
      "Epoch [50/200], Loss: 2240.2823\n",
      "Epoch [60/200], Loss: 1174.9797\n",
      "Epoch [70/200], Loss: 821.7321\n",
      "Epoch [80/200], Loss: 653.4910\n",
      "Epoch [90/200], Loss: 607.6263\n",
      "Epoch [100/200], Loss: 626.9098\n",
      "Epoch [110/200], Loss: 742.1451\n",
      "Epoch [120/200], Loss: 539.2297\n",
      "Epoch [130/200], Loss: 495.8403\n",
      "Epoch [140/200], Loss: 519.3890\n",
      "Epoch [150/200], Loss: 578.5660\n",
      "Epoch [160/200], Loss: 559.4958\n",
      "Epoch [170/200], Loss: 633.2028\n",
      "Epoch [180/200], Loss: 654.8550\n",
      "Epoch [190/200], Loss: 608.1642\n",
      "Epoch [200/200], Loss: 571.9775\n",
      "Epoch [10/200], Loss: 5148.0576\n",
      "Epoch [20/200], Loss: 3875.7595\n",
      "Epoch [30/200], Loss: 4322.3240\n",
      "Epoch [40/200], Loss: 3353.9070\n",
      "Epoch [50/200], Loss: 3052.7946\n",
      "Epoch [60/200], Loss: 2147.5089\n",
      "Epoch [70/200], Loss: 1324.6602\n",
      "Epoch [80/200], Loss: 989.0634\n",
      "Epoch [90/200], Loss: 802.0320\n",
      "Epoch [100/200], Loss: 630.8915\n",
      "Epoch [110/200], Loss: 665.1726\n",
      "Epoch [120/200], Loss: 660.5878\n",
      "Epoch [130/200], Loss: 611.7952\n",
      "Epoch [140/200], Loss: 652.8887\n",
      "Epoch [150/200], Loss: 613.0199\n",
      "Epoch [160/200], Loss: 535.0296\n",
      "Epoch [170/200], Loss: 570.7226\n",
      "Epoch [180/200], Loss: 718.0244\n",
      "Epoch [190/200], Loss: 579.4644\n",
      "Epoch [200/200], Loss: 695.1073\n",
      "Epoch [10/200], Loss: 4962.6926\n",
      "Epoch [20/200], Loss: 3807.7719\n",
      "Epoch [30/200], Loss: 4358.4325\n",
      "Epoch [40/200], Loss: 3258.2477\n",
      "Epoch [50/200], Loss: 2549.9676\n",
      "Epoch [60/200], Loss: 1878.8146\n",
      "Epoch [70/200], Loss: 1001.6088\n",
      "Epoch [80/200], Loss: 625.7610\n",
      "Epoch [90/200], Loss: 699.0959\n",
      "Epoch [100/200], Loss: 576.5733\n",
      "Epoch [110/200], Loss: 590.8507\n",
      "Epoch [120/200], Loss: 529.6534\n",
      "Epoch [130/200], Loss: 624.5529\n",
      "Epoch [140/200], Loss: 671.8620\n",
      "Epoch [150/200], Loss: 614.3089\n",
      "Epoch [160/200], Loss: 606.6824\n",
      "Epoch [170/200], Loss: 691.0293\n",
      "Epoch [180/200], Loss: 557.5930\n",
      "Epoch [190/200], Loss: 529.6892\n",
      "Epoch [200/200], Loss: 637.0745\n",
      "Epoch [10/200], Loss: 4305.9651\n",
      "Epoch [20/200], Loss: 3986.8771\n",
      "Epoch [30/200], Loss: 4459.5086\n",
      "Epoch [40/200], Loss: 3800.3898\n",
      "Epoch [50/200], Loss: 3105.8793\n",
      "Epoch [60/200], Loss: 2456.7440\n",
      "Epoch [70/200], Loss: 1845.7493\n",
      "Epoch [80/200], Loss: 1339.2369\n",
      "Epoch [90/200], Loss: 654.0237\n",
      "Epoch [100/200], Loss: 757.8776\n",
      "Epoch [110/200], Loss: 570.4439\n",
      "Epoch [120/200], Loss: 669.3294\n",
      "Epoch [130/200], Loss: 560.7500\n",
      "Epoch [140/200], Loss: 598.0363\n",
      "Epoch [150/200], Loss: 596.9544\n",
      "Epoch [160/200], Loss: 587.7456\n",
      "Epoch [170/200], Loss: 722.7311\n",
      "Epoch [180/200], Loss: 585.1319\n",
      "Epoch [190/200], Loss: 598.7217\n",
      "Epoch [200/200], Loss: 691.1520\n",
      "Epoch [10/200], Loss: 4473.0634\n",
      "Epoch [20/200], Loss: 4199.0607\n",
      "Epoch [30/200], Loss: 4335.2709\n",
      "Epoch [40/200], Loss: 3958.5364\n",
      "Epoch [50/200], Loss: 3450.7332\n",
      "Epoch [60/200], Loss: 2564.1890\n",
      "Epoch [70/200], Loss: 1547.5862\n",
      "Epoch [80/200], Loss: 865.3549\n",
      "Epoch [90/200], Loss: 654.6667\n",
      "Epoch [100/200], Loss: 692.6128\n",
      "Epoch [110/200], Loss: 655.0485\n",
      "Epoch [120/200], Loss: 647.9724\n",
      "Epoch [130/200], Loss: 657.6224\n",
      "Epoch [140/200], Loss: 624.2322\n",
      "Epoch [150/200], Loss: 564.5983\n",
      "Epoch [160/200], Loss: 502.0095\n",
      "Epoch [170/200], Loss: 721.9188\n",
      "Epoch [180/200], Loss: 649.9688\n",
      "Epoch [190/200], Loss: 609.3044\n",
      "Epoch [200/200], Loss: 696.0839\n",
      "Epoch [10/200], Loss: 4700.3398\n",
      "Epoch [20/200], Loss: 4348.3698\n",
      "Epoch [30/200], Loss: 3828.0566\n",
      "Epoch [40/200], Loss: 3994.9172\n",
      "Epoch [50/200], Loss: 2993.1687\n",
      "Epoch [60/200], Loss: 2625.5865\n",
      "Epoch [70/200], Loss: 1268.0583\n",
      "Epoch [80/200], Loss: 974.7981\n",
      "Epoch [90/200], Loss: 732.6368\n",
      "Epoch [100/200], Loss: 624.8854\n",
      "Epoch [110/200], Loss: 536.2155\n",
      "Epoch [120/200], Loss: 593.1274\n",
      "Epoch [130/200], Loss: 602.0546\n",
      "Epoch [140/200], Loss: 609.3629\n",
      "Epoch [150/200], Loss: 625.4147\n",
      "Epoch [160/200], Loss: 567.6425\n",
      "Epoch [170/200], Loss: 615.5350\n",
      "Epoch [180/200], Loss: 543.5613\n",
      "Epoch [190/200], Loss: 623.0853\n",
      "Epoch [200/200], Loss: 638.4718\n",
      "Epoch [10/200], Loss: 4742.2521\n",
      "Epoch [20/200], Loss: 4562.8561\n",
      "Epoch [30/200], Loss: 4651.7905\n",
      "Epoch [40/200], Loss: 3378.9971\n",
      "Epoch [50/200], Loss: 2436.4093\n",
      "Epoch [60/200], Loss: 1999.3639\n",
      "Epoch [70/200], Loss: 1130.6469\n",
      "Epoch [80/200], Loss: 836.7516\n",
      "Epoch [90/200], Loss: 598.6988\n",
      "Epoch [100/200], Loss: 650.0378\n",
      "Epoch [110/200], Loss: 589.6702\n",
      "Epoch [120/200], Loss: 491.2623\n",
      "Epoch [130/200], Loss: 705.1947\n",
      "Epoch [140/200], Loss: 586.1678\n",
      "Epoch [150/200], Loss: 696.8392\n",
      "Epoch [160/200], Loss: 618.8231\n",
      "Epoch [170/200], Loss: 679.1506\n",
      "Epoch [180/200], Loss: 697.6402\n",
      "Epoch [190/200], Loss: 558.2188\n",
      "Epoch [200/200], Loss: 681.7948\n",
      "Epoch [10/200], Loss: 4562.8137\n",
      "Epoch [20/200], Loss: 4985.7501\n",
      "Epoch [30/200], Loss: 3880.3578\n",
      "Epoch [40/200], Loss: 3487.3236\n",
      "Epoch [50/200], Loss: 3183.0471\n",
      "Epoch [60/200], Loss: 2669.6405\n",
      "Epoch [70/200], Loss: 1592.4610\n",
      "Epoch [80/200], Loss: 1123.7738\n",
      "Epoch [90/200], Loss: 817.6745\n",
      "Epoch [100/200], Loss: 590.7478\n",
      "Epoch [110/200], Loss: 677.7079\n",
      "Epoch [120/200], Loss: 624.8905\n",
      "Epoch [130/200], Loss: 630.2647\n",
      "Epoch [140/200], Loss: 780.5166\n",
      "Epoch [150/200], Loss: 593.4182\n",
      "Epoch [160/200], Loss: 585.4062\n",
      "Epoch [170/200], Loss: 635.4500\n",
      "Epoch [180/200], Loss: 549.4571\n",
      "Epoch [190/200], Loss: 581.8473\n",
      "Epoch [200/200], Loss: 590.3595\n",
      "Epoch [10/200], Loss: 4534.2114\n",
      "Epoch [20/200], Loss: 4725.8423\n",
      "Epoch [30/200], Loss: 5137.7489\n",
      "Epoch [40/200], Loss: 4470.9716\n",
      "Epoch [50/200], Loss: 4145.0440\n",
      "Epoch [60/200], Loss: 4063.4772\n",
      "Epoch [70/200], Loss: 3149.3484\n",
      "Epoch [80/200], Loss: 3169.8173\n",
      "Epoch [90/200], Loss: 2264.9219\n",
      "Epoch [100/200], Loss: 1875.8761\n",
      "Epoch [110/200], Loss: 1874.2674\n",
      "Epoch [120/200], Loss: 1293.0788\n",
      "Epoch [130/200], Loss: 1177.1396\n",
      "Epoch [140/200], Loss: 870.5652\n",
      "Epoch [150/200], Loss: 751.0043\n",
      "Epoch [160/200], Loss: 689.0672\n",
      "Epoch [170/200], Loss: 648.4101\n",
      "Epoch [180/200], Loss: 559.1385\n",
      "Epoch [190/200], Loss: 589.6362\n",
      "Epoch [200/200], Loss: 606.8717\n",
      "Epoch [10/200], Loss: 4417.0101\n",
      "Epoch [20/200], Loss: 4627.8318\n",
      "Epoch [30/200], Loss: 4393.6671\n",
      "Epoch [40/200], Loss: 4139.2684\n",
      "Epoch [50/200], Loss: 3567.2914\n",
      "Epoch [60/200], Loss: 2700.3207\n",
      "Epoch [70/200], Loss: 2076.8116\n",
      "Epoch [80/200], Loss: 1278.4843\n",
      "Epoch [90/200], Loss: 827.2251\n",
      "Epoch [100/200], Loss: 650.7170\n",
      "Epoch [110/200], Loss: 548.2125\n",
      "Epoch [120/200], Loss: 626.7898\n",
      "Epoch [130/200], Loss: 662.2663\n",
      "Epoch [140/200], Loss: 600.5272\n",
      "Epoch [150/200], Loss: 589.6367\n",
      "Epoch [160/200], Loss: 578.8155\n",
      "Epoch [170/200], Loss: 634.9377\n",
      "Epoch [180/200], Loss: 625.3477\n",
      "Epoch [190/200], Loss: 629.1372\n",
      "Epoch [200/200], Loss: 583.0906\n",
      "Epoch [10/200], Loss: 4654.2634\n",
      "Epoch [20/200], Loss: 4368.0794\n",
      "Epoch [30/200], Loss: 4600.8247\n",
      "Epoch [40/200], Loss: 3248.9418\n",
      "Epoch [50/200], Loss: 2677.2628\n",
      "Epoch [60/200], Loss: 2201.3555\n",
      "Epoch [70/200], Loss: 1293.2982\n",
      "Epoch [80/200], Loss: 828.6720\n",
      "Epoch [90/200], Loss: 746.1385\n",
      "Epoch [100/200], Loss: 595.6887\n",
      "Epoch [110/200], Loss: 582.4240\n",
      "Epoch [120/200], Loss: 783.1913\n",
      "Epoch [130/200], Loss: 695.2320\n",
      "Epoch [140/200], Loss: 685.0218\n",
      "Epoch [150/200], Loss: 682.0243\n",
      "Epoch [160/200], Loss: 647.4635\n",
      "Epoch [170/200], Loss: 634.9982\n",
      "Epoch [180/200], Loss: 673.2080\n",
      "Epoch [190/200], Loss: 616.2393\n",
      "Epoch [200/200], Loss: 652.6936\n",
      "Epoch [10/200], Loss: 4802.2024\n",
      "Epoch [20/200], Loss: 4439.9893\n",
      "Epoch [30/200], Loss: 4280.1808\n",
      "Epoch [40/200], Loss: 3771.7650\n",
      "Epoch [50/200], Loss: 2410.8265\n",
      "Epoch [60/200], Loss: 1717.8943\n",
      "Epoch [70/200], Loss: 975.1391\n",
      "Epoch [80/200], Loss: 676.7834\n",
      "Epoch [90/200], Loss: 662.0206\n",
      "Epoch [100/200], Loss: 679.2449\n",
      "Epoch [110/200], Loss: 577.2791\n",
      "Epoch [120/200], Loss: 577.7424\n",
      "Epoch [130/200], Loss: 731.8812\n",
      "Epoch [140/200], Loss: 611.7272\n",
      "Epoch [150/200], Loss: 604.7216\n",
      "Epoch [160/200], Loss: 578.5335\n",
      "Epoch [170/200], Loss: 621.3085\n",
      "Epoch [180/200], Loss: 519.0245\n",
      "Epoch [190/200], Loss: 650.2388\n",
      "Epoch [200/200], Loss: 669.2870\n",
      "Epoch [10/200], Loss: 3988.8412\n",
      "Epoch [20/200], Loss: 4406.1281\n",
      "Epoch [30/200], Loss: 4144.0729\n",
      "Epoch [40/200], Loss: 2873.2989\n",
      "Epoch [50/200], Loss: 2012.9181\n",
      "Epoch [60/200], Loss: 1179.2572\n",
      "Epoch [70/200], Loss: 734.7450\n",
      "Epoch [80/200], Loss: 609.8749\n",
      "Epoch [90/200], Loss: 563.3906\n",
      "Epoch [100/200], Loss: 630.9863\n",
      "Epoch [110/200], Loss: 547.9004\n",
      "Epoch [120/200], Loss: 622.6263\n",
      "Epoch [130/200], Loss: 658.6863\n",
      "Epoch [140/200], Loss: 602.9797\n",
      "Epoch [150/200], Loss: 612.7802\n",
      "Epoch [160/200], Loss: 641.6492\n",
      "Epoch [170/200], Loss: 573.6642\n",
      "Epoch [180/200], Loss: 504.9224\n",
      "Epoch [190/200], Loss: 691.0492\n",
      "Epoch [200/200], Loss: 589.2438\n",
      "Epoch [10/200], Loss: 4532.8956\n",
      "Epoch [20/200], Loss: 4030.2704\n",
      "Epoch [30/200], Loss: 4160.1920\n",
      "Epoch [40/200], Loss: 4132.1413\n",
      "Epoch [50/200], Loss: 2858.9634\n",
      "Epoch [60/200], Loss: 2378.0731\n",
      "Epoch [70/200], Loss: 1598.0445\n",
      "Epoch [80/200], Loss: 982.1739\n",
      "Epoch [90/200], Loss: 695.1783\n",
      "Epoch [100/200], Loss: 605.2379\n",
      "Epoch [110/200], Loss: 584.5873\n",
      "Epoch [120/200], Loss: 580.5442\n",
      "Epoch [130/200], Loss: 631.7016\n",
      "Epoch [140/200], Loss: 636.0829\n",
      "Epoch [150/200], Loss: 557.6772\n",
      "Epoch [160/200], Loss: 555.2014\n",
      "Epoch [170/200], Loss: 593.4548\n",
      "Epoch [180/200], Loss: 626.7314\n",
      "Epoch [190/200], Loss: 598.3399\n",
      "Epoch [200/200], Loss: 637.2886\n",
      "Epoch [10/200], Loss: 4950.3331\n",
      "Epoch [20/200], Loss: 4430.9189\n",
      "Epoch [30/200], Loss: 4400.1702\n",
      "Epoch [40/200], Loss: 4215.3550\n",
      "Epoch [50/200], Loss: 3629.2551\n",
      "Epoch [60/200], Loss: 2729.7678\n",
      "Epoch [70/200], Loss: 1931.3211\n",
      "Epoch [80/200], Loss: 1451.4594\n",
      "Epoch [90/200], Loss: 902.6356\n",
      "Epoch [100/200], Loss: 678.5665\n",
      "Epoch [110/200], Loss: 587.4302\n",
      "Epoch [120/200], Loss: 632.4535\n",
      "Epoch [130/200], Loss: 648.1677\n",
      "Epoch [140/200], Loss: 606.8813\n",
      "Epoch [150/200], Loss: 631.5789\n",
      "Epoch [160/200], Loss: 689.1031\n",
      "Epoch [170/200], Loss: 569.6713\n",
      "Epoch [180/200], Loss: 667.6296\n",
      "Epoch [190/200], Loss: 576.1290\n",
      "Epoch [200/200], Loss: 574.7871\n",
      "Epoch [10/200], Loss: 4667.5820\n",
      "Epoch [20/200], Loss: 4707.0591\n",
      "Epoch [30/200], Loss: 4994.0000\n",
      "Epoch [40/200], Loss: 4396.4261\n",
      "Epoch [50/200], Loss: 4386.7522\n",
      "Epoch [60/200], Loss: 2562.8212\n",
      "Epoch [70/200], Loss: 2186.5034\n",
      "Epoch [80/200], Loss: 1562.7962\n",
      "Epoch [90/200], Loss: 1037.0800\n",
      "Epoch [100/200], Loss: 794.1345\n",
      "Epoch [110/200], Loss: 624.3001\n",
      "Epoch [120/200], Loss: 583.3100\n",
      "Epoch [130/200], Loss: 575.1287\n",
      "Epoch [140/200], Loss: 648.7193\n",
      "Epoch [150/200], Loss: 552.0353\n",
      "Epoch [160/200], Loss: 623.9831\n",
      "Epoch [170/200], Loss: 634.3075\n",
      "Epoch [180/200], Loss: 641.5004\n",
      "Epoch [190/200], Loss: 679.3172\n",
      "Epoch [200/200], Loss: 597.0688\n",
      "Epoch [10/200], Loss: 4340.3779\n",
      "Epoch [20/200], Loss: 4334.4510\n",
      "Epoch [30/200], Loss: 4100.5266\n",
      "Epoch [40/200], Loss: 3908.4244\n",
      "Epoch [50/200], Loss: 3522.4798\n",
      "Epoch [60/200], Loss: 2969.9966\n",
      "Epoch [70/200], Loss: 1967.4528\n",
      "Epoch [80/200], Loss: 1263.5153\n",
      "Epoch [90/200], Loss: 781.6918\n",
      "Epoch [100/200], Loss: 799.9618\n",
      "Epoch [110/200], Loss: 660.2351\n",
      "Epoch [120/200], Loss: 740.6570\n",
      "Epoch [130/200], Loss: 677.4297\n",
      "Epoch [140/200], Loss: 633.2037\n",
      "Epoch [150/200], Loss: 537.7628\n",
      "Epoch [160/200], Loss: 607.5008\n",
      "Epoch [170/200], Loss: 551.4699\n",
      "Epoch [180/200], Loss: 669.5226\n",
      "Epoch [190/200], Loss: 655.5719\n",
      "Epoch [200/200], Loss: 639.7913\n",
      "Epoch [10/200], Loss: 4084.8871\n",
      "Epoch [20/200], Loss: 4528.9044\n",
      "Epoch [30/200], Loss: 4173.5106\n",
      "Epoch [40/200], Loss: 3468.5865\n",
      "Epoch [50/200], Loss: 2884.1915\n",
      "Epoch [60/200], Loss: 2159.0410\n",
      "Epoch [70/200], Loss: 1368.1747\n",
      "Epoch [80/200], Loss: 855.1074\n",
      "Epoch [90/200], Loss: 545.1009\n",
      "Epoch [100/200], Loss: 539.4040\n",
      "Epoch [110/200], Loss: 575.1746\n",
      "Epoch [120/200], Loss: 630.3514\n",
      "Epoch [130/200], Loss: 649.1886\n",
      "Epoch [140/200], Loss: 540.4225\n",
      "Epoch [150/200], Loss: 703.2895\n",
      "Epoch [160/200], Loss: 586.2391\n",
      "Epoch [170/200], Loss: 598.9489\n",
      "Epoch [180/200], Loss: 617.9424\n",
      "Epoch [190/200], Loss: 658.6876\n",
      "Epoch [200/200], Loss: 658.2233\n",
      "Epoch [10/200], Loss: 4841.7175\n",
      "Epoch [20/200], Loss: 4281.9617\n",
      "Epoch [30/200], Loss: 4027.7876\n",
      "Epoch [40/200], Loss: 3646.7161\n",
      "Epoch [50/200], Loss: 3189.3082\n",
      "Epoch [60/200], Loss: 1849.6546\n",
      "Epoch [70/200], Loss: 1230.9753\n",
      "Epoch [80/200], Loss: 707.3283\n",
      "Epoch [90/200], Loss: 589.5102\n",
      "Epoch [100/200], Loss: 708.7583\n",
      "Epoch [110/200], Loss: 611.0108\n",
      "Epoch [120/200], Loss: 586.2991\n",
      "Epoch [130/200], Loss: 625.5046\n",
      "Epoch [140/200], Loss: 644.2085\n",
      "Epoch [150/200], Loss: 681.4393\n",
      "Epoch [160/200], Loss: 691.6561\n",
      "Epoch [170/200], Loss: 582.7600\n",
      "Epoch [180/200], Loss: 554.3563\n",
      "Epoch [190/200], Loss: 515.7677\n",
      "Epoch [200/200], Loss: 543.7387\n",
      "Epoch [10/200], Loss: 4570.7402\n",
      "Epoch [20/200], Loss: 4716.2001\n",
      "Epoch [30/200], Loss: 4152.9333\n",
      "Epoch [40/200], Loss: 4161.0588\n",
      "Epoch [50/200], Loss: 3693.0551\n",
      "Epoch [60/200], Loss: 2739.7311\n",
      "Epoch [70/200], Loss: 2065.6380\n",
      "Epoch [80/200], Loss: 1282.7035\n",
      "Epoch [90/200], Loss: 1274.7313\n",
      "Epoch [100/200], Loss: 709.7473\n",
      "Epoch [110/200], Loss: 693.9406\n",
      "Epoch [120/200], Loss: 721.0417\n",
      "Epoch [130/200], Loss: 554.1231\n",
      "Epoch [140/200], Loss: 625.9912\n",
      "Epoch [150/200], Loss: 610.8580\n",
      "Epoch [160/200], Loss: 696.5126\n",
      "Epoch [170/200], Loss: 591.1313\n",
      "Epoch [180/200], Loss: 569.2319\n",
      "Epoch [190/200], Loss: 547.8411\n",
      "Epoch [200/200], Loss: 536.2282\n",
      "Epoch [10/200], Loss: 4349.3001\n",
      "Epoch [20/200], Loss: 4299.0445\n",
      "Epoch [30/200], Loss: 3359.4710\n",
      "Epoch [40/200], Loss: 3649.0690\n",
      "Epoch [50/200], Loss: 2590.5921\n",
      "Epoch [60/200], Loss: 1518.7418\n",
      "Epoch [70/200], Loss: 1044.7083\n",
      "Epoch [80/200], Loss: 751.9158\n",
      "Epoch [90/200], Loss: 555.1583\n",
      "Epoch [100/200], Loss: 602.1644\n",
      "Epoch [110/200], Loss: 517.8100\n",
      "Epoch [120/200], Loss: 594.3206\n",
      "Epoch [130/200], Loss: 625.8712\n",
      "Epoch [140/200], Loss: 627.6111\n",
      "Epoch [150/200], Loss: 637.3390\n",
      "Epoch [160/200], Loss: 579.5110\n",
      "Epoch [170/200], Loss: 675.0719\n",
      "Epoch [180/200], Loss: 585.7227\n",
      "Epoch [190/200], Loss: 711.6660\n",
      "Epoch [200/200], Loss: 578.6660\n",
      "Epoch [10/200], Loss: 4521.6377\n",
      "Epoch [20/200], Loss: 4347.5513\n",
      "Epoch [30/200], Loss: 3985.5417\n",
      "Epoch [40/200], Loss: 3236.3761\n",
      "Epoch [50/200], Loss: 3369.1226\n",
      "Epoch [60/200], Loss: 2190.3947\n",
      "Epoch [70/200], Loss: 1169.9080\n",
      "Epoch [80/200], Loss: 839.6113\n",
      "Epoch [90/200], Loss: 745.2390\n",
      "Epoch [100/200], Loss: 663.2156\n",
      "Epoch [110/200], Loss: 576.5722\n",
      "Epoch [120/200], Loss: 642.5738\n",
      "Epoch [130/200], Loss: 521.7320\n",
      "Epoch [140/200], Loss: 648.7421\n",
      "Epoch [150/200], Loss: 608.3081\n",
      "Epoch [160/200], Loss: 664.9646\n",
      "Epoch [170/200], Loss: 565.9725\n",
      "Epoch [180/200], Loss: 548.5457\n",
      "Epoch [190/200], Loss: 605.2771\n",
      "Epoch [200/200], Loss: 687.7784\n",
      "Epoch [10/200], Loss: 4644.4851\n",
      "Epoch [20/200], Loss: 3910.2682\n",
      "Epoch [30/200], Loss: 4188.7177\n",
      "Epoch [40/200], Loss: 3334.4284\n",
      "Epoch [50/200], Loss: 2374.6006\n",
      "Epoch [60/200], Loss: 1500.0590\n",
      "Epoch [70/200], Loss: 864.7144\n",
      "Epoch [80/200], Loss: 559.2440\n",
      "Epoch [90/200], Loss: 670.5136\n",
      "Epoch [100/200], Loss: 574.8724\n",
      "Epoch [110/200], Loss: 635.7937\n",
      "Epoch [120/200], Loss: 584.8712\n",
      "Epoch [130/200], Loss: 526.4787\n",
      "Epoch [140/200], Loss: 647.7760\n",
      "Epoch [150/200], Loss: 576.9928\n",
      "Epoch [160/200], Loss: 604.5628\n",
      "Epoch [170/200], Loss: 609.9128\n",
      "Epoch [180/200], Loss: 588.0414\n",
      "Epoch [190/200], Loss: 488.6835\n",
      "Epoch [200/200], Loss: 542.1849\n",
      "Epoch [10/200], Loss: 4242.9551\n",
      "Epoch [20/200], Loss: 3946.5778\n",
      "Epoch [30/200], Loss: 3977.9863\n",
      "Epoch [40/200], Loss: 4348.3074\n",
      "Epoch [50/200], Loss: 3603.1020\n",
      "Epoch [60/200], Loss: 2920.4105\n",
      "Epoch [70/200], Loss: 2233.1294\n",
      "Epoch [80/200], Loss: 1393.0615\n",
      "Epoch [90/200], Loss: 1260.1736\n",
      "Epoch [100/200], Loss: 929.2704\n",
      "Epoch [110/200], Loss: 617.9168\n",
      "Epoch [120/200], Loss: 648.0619\n",
      "Epoch [130/200], Loss: 596.3026\n",
      "Epoch [140/200], Loss: 627.4219\n",
      "Epoch [150/200], Loss: 560.4692\n",
      "Epoch [160/200], Loss: 589.5491\n",
      "Epoch [170/200], Loss: 589.7090\n",
      "Epoch [180/200], Loss: 654.8341\n",
      "Epoch [190/200], Loss: 627.7273\n",
      "Epoch [200/200], Loss: 668.7771\n",
      "Epoch [10/200], Loss: 4493.2909\n",
      "Epoch [20/200], Loss: 4217.5286\n",
      "Epoch [30/200], Loss: 4363.1686\n",
      "Epoch [40/200], Loss: 3502.6302\n",
      "Epoch [50/200], Loss: 3180.9218\n",
      "Epoch [60/200], Loss: 1927.2794\n",
      "Epoch [70/200], Loss: 1122.8271\n",
      "Epoch [80/200], Loss: 781.8334\n",
      "Epoch [90/200], Loss: 518.0831\n",
      "Epoch [100/200], Loss: 620.9232\n",
      "Epoch [110/200], Loss: 540.3893\n",
      "Epoch [120/200], Loss: 586.0965\n",
      "Epoch [130/200], Loss: 538.6611\n",
      "Epoch [140/200], Loss: 556.1697\n",
      "Epoch [150/200], Loss: 578.8425\n",
      "Epoch [160/200], Loss: 608.1273\n",
      "Epoch [170/200], Loss: 606.2719\n",
      "Epoch [180/200], Loss: 563.3198\n",
      "Epoch [190/200], Loss: 550.0546\n",
      "Epoch [200/200], Loss: 687.5301\n",
      "Epoch [10/200], Loss: 4416.3553\n",
      "Epoch [20/200], Loss: 4246.5909\n",
      "Epoch [30/200], Loss: 4261.2870\n",
      "Epoch [40/200], Loss: 3673.5220\n",
      "Epoch [50/200], Loss: 3725.7938\n",
      "Epoch [60/200], Loss: 2330.3192\n",
      "Epoch [70/200], Loss: 1521.7067\n",
      "Epoch [80/200], Loss: 1026.8893\n",
      "Epoch [90/200], Loss: 626.4811\n",
      "Epoch [100/200], Loss: 702.2679\n",
      "Epoch [110/200], Loss: 642.9547\n",
      "Epoch [120/200], Loss: 625.1949\n",
      "Epoch [130/200], Loss: 598.2068\n",
      "Epoch [140/200], Loss: 611.9889\n",
      "Epoch [150/200], Loss: 667.7279\n",
      "Epoch [160/200], Loss: 671.0886\n",
      "Epoch [170/200], Loss: 680.0485\n",
      "Epoch [180/200], Loss: 570.7997\n",
      "Epoch [190/200], Loss: 521.9455\n",
      "Epoch [200/200], Loss: 552.1985\n",
      "Epoch [10/200], Loss: 4904.8763\n",
      "Epoch [20/200], Loss: 4890.5878\n",
      "Epoch [30/200], Loss: 4013.3021\n",
      "Epoch [40/200], Loss: 3577.3061\n",
      "Epoch [50/200], Loss: 3381.6545\n",
      "Epoch [60/200], Loss: 2284.1645\n",
      "Epoch [70/200], Loss: 1473.3047\n",
      "Epoch [80/200], Loss: 1011.2308\n",
      "Epoch [90/200], Loss: 780.9651\n",
      "Epoch [100/200], Loss: 644.8220\n",
      "Epoch [110/200], Loss: 555.0071\n",
      "Epoch [120/200], Loss: 686.0281\n",
      "Epoch [130/200], Loss: 680.6269\n",
      "Epoch [140/200], Loss: 655.0465\n",
      "Epoch [150/200], Loss: 647.3696\n",
      "Epoch [160/200], Loss: 592.9175\n",
      "Epoch [170/200], Loss: 505.6967\n",
      "Epoch [180/200], Loss: 678.7426\n",
      "Epoch [190/200], Loss: 560.8396\n",
      "Epoch [200/200], Loss: 660.0538\n",
      "Epoch [10/200], Loss: 5014.6086\n",
      "Epoch [20/200], Loss: 4134.0178\n",
      "Epoch [30/200], Loss: 3443.4009\n",
      "Epoch [40/200], Loss: 3716.7200\n",
      "Epoch [50/200], Loss: 3187.3666\n",
      "Epoch [60/200], Loss: 2090.3222\n",
      "Epoch [70/200], Loss: 1469.9192\n",
      "Epoch [80/200], Loss: 808.6868\n",
      "Epoch [90/200], Loss: 667.0835\n",
      "Epoch [100/200], Loss: 642.9464\n",
      "Epoch [110/200], Loss: 554.6126\n",
      "Epoch [120/200], Loss: 560.9189\n",
      "Epoch [130/200], Loss: 568.2574\n",
      "Epoch [140/200], Loss: 691.2652\n",
      "Epoch [150/200], Loss: 676.8957\n",
      "Epoch [160/200], Loss: 675.0041\n",
      "Epoch [170/200], Loss: 630.7847\n",
      "Epoch [180/200], Loss: 713.6796\n",
      "Epoch [190/200], Loss: 757.3868\n",
      "Epoch [200/200], Loss: 653.8599\n",
      "Epoch [10/200], Loss: 4629.8811\n",
      "Epoch [20/200], Loss: 4525.5129\n",
      "Epoch [30/200], Loss: 3410.6551\n",
      "Epoch [40/200], Loss: 3380.7521\n",
      "Epoch [50/200], Loss: 2767.5944\n",
      "Epoch [60/200], Loss: 1148.0834\n",
      "Epoch [70/200], Loss: 736.3055\n",
      "Epoch [80/200], Loss: 706.3317\n",
      "Epoch [90/200], Loss: 574.9548\n",
      "Epoch [100/200], Loss: 574.7375\n",
      "Epoch [110/200], Loss: 587.2686\n",
      "Epoch [120/200], Loss: 622.5018\n",
      "Epoch [130/200], Loss: 486.9656\n",
      "Epoch [140/200], Loss: 754.7468\n",
      "Epoch [150/200], Loss: 584.9896\n",
      "Epoch [160/200], Loss: 787.2219\n",
      "Epoch [170/200], Loss: 717.5933\n",
      "Epoch [180/200], Loss: 498.0993\n",
      "Epoch [190/200], Loss: 610.6260\n",
      "Epoch [200/200], Loss: 707.9025\n",
      "Epoch [10/200], Loss: 3857.3510\n",
      "Epoch [20/200], Loss: 4203.0579\n",
      "Epoch [30/200], Loss: 4014.2752\n",
      "Epoch [40/200], Loss: 3853.2252\n",
      "Epoch [50/200], Loss: 3902.3689\n",
      "Epoch [60/200], Loss: 2921.6691\n",
      "Epoch [70/200], Loss: 2133.1307\n",
      "Epoch [80/200], Loss: 1147.4598\n",
      "Epoch [90/200], Loss: 895.6374\n",
      "Epoch [100/200], Loss: 642.8819\n",
      "Epoch [110/200], Loss: 584.4182\n",
      "Epoch [120/200], Loss: 529.4442\n",
      "Epoch [130/200], Loss: 627.0161\n",
      "Epoch [140/200], Loss: 696.0703\n",
      "Epoch [150/200], Loss: 578.7752\n",
      "Epoch [160/200], Loss: 683.4882\n",
      "Epoch [170/200], Loss: 616.9763\n",
      "Epoch [180/200], Loss: 501.0503\n",
      "Epoch [190/200], Loss: 654.8849\n",
      "Epoch [200/200], Loss: 545.4710\n",
      "Epoch [10/200], Loss: 4680.0822\n",
      "Epoch [20/200], Loss: 4398.8207\n",
      "Epoch [30/200], Loss: 4285.3241\n",
      "Epoch [40/200], Loss: 4490.4243\n",
      "Epoch [50/200], Loss: 3821.0410\n",
      "Epoch [60/200], Loss: 2747.7073\n",
      "Epoch [70/200], Loss: 2008.6409\n",
      "Epoch [80/200], Loss: 1461.2371\n",
      "Epoch [90/200], Loss: 985.7999\n",
      "Epoch [100/200], Loss: 725.0815\n",
      "Epoch [110/200], Loss: 716.5818\n",
      "Epoch [120/200], Loss: 687.8965\n",
      "Epoch [130/200], Loss: 571.5383\n",
      "Epoch [140/200], Loss: 559.5870\n",
      "Epoch [150/200], Loss: 568.0795\n",
      "Epoch [160/200], Loss: 544.9166\n",
      "Epoch [170/200], Loss: 618.6333\n",
      "Epoch [180/200], Loss: 579.0739\n",
      "Epoch [190/200], Loss: 601.4224\n",
      "Epoch [200/200], Loss: 649.0537\n",
      "Epoch [10/200], Loss: 4415.7606\n",
      "Epoch [20/200], Loss: 4262.1643\n",
      "Epoch [30/200], Loss: 4292.0922\n",
      "Epoch [40/200], Loss: 3752.1765\n",
      "Epoch [50/200], Loss: 2691.5657\n",
      "Epoch [60/200], Loss: 1410.2116\n",
      "Epoch [70/200], Loss: 988.7088\n",
      "Epoch [80/200], Loss: 648.0334\n",
      "Epoch [90/200], Loss: 597.4150\n",
      "Epoch [100/200], Loss: 608.8748\n",
      "Epoch [110/200], Loss: 662.9817\n",
      "Epoch [120/200], Loss: 590.3065\n",
      "Epoch [130/200], Loss: 588.0362\n",
      "Epoch [140/200], Loss: 494.4752\n",
      "Epoch [150/200], Loss: 593.6862\n",
      "Epoch [160/200], Loss: 620.8562\n",
      "Epoch [170/200], Loss: 685.2707\n",
      "Epoch [180/200], Loss: 562.2375\n",
      "Epoch [190/200], Loss: 574.4085\n",
      "Epoch [200/200], Loss: 584.0125\n",
      "Epoch [10/200], Loss: 3933.7626\n",
      "Epoch [20/200], Loss: 4220.6426\n",
      "Epoch [30/200], Loss: 3794.6833\n",
      "Epoch [40/200], Loss: 4099.7576\n",
      "Epoch [50/200], Loss: 3033.1674\n",
      "Epoch [60/200], Loss: 2476.1417\n",
      "Epoch [70/200], Loss: 1825.4623\n",
      "Epoch [80/200], Loss: 1360.0461\n",
      "Epoch [90/200], Loss: 739.1728\n",
      "Epoch [100/200], Loss: 821.9973\n",
      "Epoch [110/200], Loss: 620.4525\n",
      "Epoch [120/200], Loss: 530.0940\n",
      "Epoch [130/200], Loss: 777.7323\n",
      "Epoch [140/200], Loss: 614.2449\n",
      "Epoch [150/200], Loss: 576.8940\n",
      "Epoch [160/200], Loss: 582.1866\n",
      "Epoch [170/200], Loss: 750.6365\n",
      "Epoch [180/200], Loss: 585.5134\n",
      "Epoch [190/200], Loss: 601.9498\n",
      "Epoch [200/200], Loss: 672.0396\n",
      "Epoch [10/200], Loss: 4449.9160\n",
      "Epoch [20/200], Loss: 4574.4487\n",
      "Epoch [30/200], Loss: 4022.8701\n",
      "Epoch [40/200], Loss: 3963.8145\n",
      "Epoch [50/200], Loss: 2534.0757\n",
      "Epoch [60/200], Loss: 2005.2721\n",
      "Epoch [70/200], Loss: 1564.5241\n",
      "Epoch [80/200], Loss: 694.3465\n",
      "Epoch [90/200], Loss: 591.9605\n",
      "Epoch [100/200], Loss: 637.2536\n",
      "Epoch [110/200], Loss: 638.1783\n",
      "Epoch [120/200], Loss: 635.1859\n",
      "Epoch [130/200], Loss: 590.7803\n",
      "Epoch [140/200], Loss: 551.0061\n",
      "Epoch [150/200], Loss: 611.8143\n",
      "Epoch [160/200], Loss: 684.9116\n",
      "Epoch [170/200], Loss: 705.9107\n",
      "Epoch [180/200], Loss: 639.6673\n",
      "Epoch [190/200], Loss: 637.5289\n",
      "Epoch [200/200], Loss: 617.5423\n",
      "Epoch [10/200], Loss: 4494.1026\n",
      "Epoch [20/200], Loss: 4627.4427\n",
      "Epoch [30/200], Loss: 4335.3646\n",
      "Epoch [40/200], Loss: 3783.1976\n",
      "Epoch [50/200], Loss: 2747.0245\n",
      "Epoch [60/200], Loss: 2453.1583\n",
      "Epoch [70/200], Loss: 1940.4863\n",
      "Epoch [80/200], Loss: 873.6298\n",
      "Epoch [90/200], Loss: 800.6056\n",
      "Epoch [100/200], Loss: 609.4648\n",
      "Epoch [110/200], Loss: 577.5978\n",
      "Epoch [120/200], Loss: 692.4029\n",
      "Epoch [130/200], Loss: 608.5709\n",
      "Epoch [140/200], Loss: 612.2501\n",
      "Epoch [150/200], Loss: 639.9494\n",
      "Epoch [160/200], Loss: 605.6700\n",
      "Epoch [170/200], Loss: 553.6030\n",
      "Epoch [180/200], Loss: 570.1688\n",
      "Epoch [190/200], Loss: 631.7883\n",
      "Epoch [200/200], Loss: 653.1211\n",
      "Epoch [10/200], Loss: 4828.8878\n",
      "Epoch [20/200], Loss: 4234.4732\n",
      "Epoch [30/200], Loss: 3490.2235\n",
      "Epoch [40/200], Loss: 3458.2526\n",
      "Epoch [50/200], Loss: 2762.8436\n",
      "Epoch [60/200], Loss: 2472.8457\n",
      "Epoch [70/200], Loss: 1836.0224\n",
      "Epoch [80/200], Loss: 939.2782\n",
      "Epoch [90/200], Loss: 706.0893\n",
      "Epoch [100/200], Loss: 566.2778\n",
      "Epoch [110/200], Loss: 581.5528\n",
      "Epoch [120/200], Loss: 617.8660\n",
      "Epoch [130/200], Loss: 716.4209\n",
      "Epoch [140/200], Loss: 562.7839\n",
      "Epoch [150/200], Loss: 602.1812\n",
      "Epoch [160/200], Loss: 550.1174\n",
      "Epoch [170/200], Loss: 766.2599\n",
      "Epoch [180/200], Loss: 584.6865\n",
      "Epoch [190/200], Loss: 517.9632\n",
      "Epoch [200/200], Loss: 558.6702\n",
      "Epoch [10/200], Loss: 4423.4319\n",
      "Epoch [20/200], Loss: 4576.0470\n",
      "Epoch [30/200], Loss: 3854.7159\n",
      "Epoch [40/200], Loss: 4037.4688\n",
      "Epoch [50/200], Loss: 3718.1057\n",
      "Epoch [60/200], Loss: 3274.8809\n",
      "Epoch [70/200], Loss: 2266.8759\n",
      "Epoch [80/200], Loss: 1546.5955\n",
      "Epoch [90/200], Loss: 1000.0947\n",
      "Epoch [100/200], Loss: 753.6256\n",
      "Epoch [110/200], Loss: 640.8111\n",
      "Epoch [120/200], Loss: 621.5628\n",
      "Epoch [130/200], Loss: 679.3988\n",
      "Epoch [140/200], Loss: 615.6290\n",
      "Epoch [150/200], Loss: 620.0400\n",
      "Epoch [160/200], Loss: 573.5824\n",
      "Epoch [170/200], Loss: 543.8988\n",
      "Epoch [180/200], Loss: 545.7604\n",
      "Epoch [190/200], Loss: 614.0883\n",
      "Epoch [200/200], Loss: 600.2565\n",
      "Epoch [10/200], Loss: 4428.9136\n",
      "Epoch [20/200], Loss: 4166.0148\n",
      "Epoch [30/200], Loss: 4602.6245\n",
      "Epoch [40/200], Loss: 3996.6215\n",
      "Epoch [50/200], Loss: 3716.8550\n",
      "Epoch [60/200], Loss: 2766.2764\n",
      "Epoch [70/200], Loss: 2129.4995\n",
      "Epoch [80/200], Loss: 1225.1189\n",
      "Epoch [90/200], Loss: 899.1916\n",
      "Epoch [100/200], Loss: 713.3026\n",
      "Epoch [110/200], Loss: 609.3465\n",
      "Epoch [120/200], Loss: 692.7046\n",
      "Epoch [130/200], Loss: 556.8784\n",
      "Epoch [140/200], Loss: 624.9548\n",
      "Epoch [150/200], Loss: 549.7587\n",
      "Epoch [160/200], Loss: 746.6436\n",
      "Epoch [170/200], Loss: 670.1159\n",
      "Epoch [180/200], Loss: 652.3100\n",
      "Epoch [190/200], Loss: 739.7805\n",
      "Epoch [200/200], Loss: 552.1296\n",
      "Epoch [10/200], Loss: 4448.1362\n",
      "Epoch [20/200], Loss: 4815.2517\n",
      "Epoch [30/200], Loss: 4226.2087\n",
      "Epoch [40/200], Loss: 3617.5843\n",
      "Epoch [50/200], Loss: 2612.4797\n",
      "Epoch [60/200], Loss: 2206.5375\n",
      "Epoch [70/200], Loss: 1219.4845\n",
      "Epoch [80/200], Loss: 853.7923\n",
      "Epoch [90/200], Loss: 571.6363\n",
      "Epoch [100/200], Loss: 658.9333\n",
      "Epoch [110/200], Loss: 616.5942\n",
      "Epoch [120/200], Loss: 558.9935\n",
      "Epoch [130/200], Loss: 692.2244\n",
      "Epoch [140/200], Loss: 597.6291\n",
      "Epoch [150/200], Loss: 688.2206\n",
      "Epoch [160/200], Loss: 642.6076\n",
      "Epoch [170/200], Loss: 661.3127\n",
      "Epoch [180/200], Loss: 650.9373\n",
      "Epoch [190/200], Loss: 636.7835\n",
      "Epoch [200/200], Loss: 628.1370\n",
      "Epoch [10/200], Loss: 4425.5453\n",
      "Epoch [20/200], Loss: 4452.2310\n",
      "Epoch [30/200], Loss: 5276.7227\n",
      "Epoch [40/200], Loss: 4299.8245\n",
      "Epoch [50/200], Loss: 3624.4510\n",
      "Epoch [60/200], Loss: 2784.1408\n",
      "Epoch [70/200], Loss: 2113.8319\n",
      "Epoch [80/200], Loss: 1622.3730\n",
      "Epoch [90/200], Loss: 993.5244\n",
      "Epoch [100/200], Loss: 718.2386\n",
      "Epoch [110/200], Loss: 556.1628\n",
      "Epoch [120/200], Loss: 528.7088\n",
      "Epoch [130/200], Loss: 636.6086\n",
      "Epoch [140/200], Loss: 721.4495\n",
      "Epoch [150/200], Loss: 755.0412\n",
      "Epoch [160/200], Loss: 563.2791\n",
      "Epoch [170/200], Loss: 584.0423\n",
      "Epoch [180/200], Loss: 548.3929\n",
      "Epoch [190/200], Loss: 579.4430\n",
      "Epoch [200/200], Loss: 596.0820\n",
      "Epoch [10/200], Loss: 4472.9033\n",
      "Epoch [20/200], Loss: 4706.0562\n",
      "Epoch [30/200], Loss: 4904.2382\n",
      "Epoch [40/200], Loss: 4204.4866\n",
      "Epoch [50/200], Loss: 3371.6875\n",
      "Epoch [60/200], Loss: 2824.8051\n",
      "Epoch [70/200], Loss: 1784.5417\n",
      "Epoch [80/200], Loss: 1296.7440\n",
      "Epoch [90/200], Loss: 773.0408\n",
      "Epoch [100/200], Loss: 755.9120\n",
      "Epoch [110/200], Loss: 663.6230\n",
      "Epoch [120/200], Loss: 609.6525\n",
      "Epoch [130/200], Loss: 609.6529\n",
      "Epoch [140/200], Loss: 602.9522\n",
      "Epoch [150/200], Loss: 597.1639\n",
      "Epoch [160/200], Loss: 541.2879\n",
      "Epoch [170/200], Loss: 653.4163\n",
      "Epoch [180/200], Loss: 601.7673\n",
      "Epoch [190/200], Loss: 610.1012\n",
      "Epoch [200/200], Loss: 508.6040\n",
      "Epoch [10/200], Loss: 4848.8948\n",
      "Epoch [20/200], Loss: 4568.9636\n",
      "Epoch [30/200], Loss: 4297.8600\n",
      "Epoch [40/200], Loss: 3820.2593\n",
      "Epoch [50/200], Loss: 2961.4930\n",
      "Epoch [60/200], Loss: 2367.2999\n",
      "Epoch [70/200], Loss: 1276.5765\n",
      "Epoch [80/200], Loss: 943.4271\n",
      "Epoch [90/200], Loss: 629.9336\n",
      "Epoch [100/200], Loss: 591.8815\n",
      "Epoch [110/200], Loss: 616.1708\n",
      "Epoch [120/200], Loss: 624.5702\n",
      "Epoch [130/200], Loss: 675.2559\n",
      "Epoch [140/200], Loss: 674.9301\n",
      "Epoch [150/200], Loss: 557.0472\n",
      "Epoch [160/200], Loss: 593.9108\n",
      "Epoch [170/200], Loss: 678.9184\n",
      "Epoch [180/200], Loss: 673.1702\n",
      "Epoch [190/200], Loss: 570.7830\n",
      "Epoch [200/200], Loss: 520.5284\n",
      "\n",
      "Model: GCN | Test RMSE: 25.7950 | Test R value (correlation): -0.0466\n",
      "Predictions for GCN saved to ../Results/GCN_predictions.csv\n",
      "Metrics for GCN saved to ../Results/GCN_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:41:24.723728Z",
     "start_time": "2024-10-23T18:41:24.720018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n"
   ],
   "id": "39c4646732b90dd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Number of GPUs:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:41:25.622910Z",
     "start_time": "2024-10-23T18:41:25.015868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from PIL import Image\n",
    "\n",
    "def merge_metrics(directory):\n",
    "    metrics_list = []\n",
    "\n",
    "    # Loop through the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('metrics.csv'):\n",
    "            # Read each metrics CSV file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Add a column for the model name (extracted from the filename)\n",
    "            model_name = filename.replace('_metrics.csv', '')\n",
    "\n",
    "            # Extract correlation and RMSE values\n",
    "            r_corr = df.loc[df['Metric'] == 'Correlation', 'Value'].values[0]\n",
    "            rmse = df.loc[df['Metric'] == 'RMSE', 'Value'].values[0]\n",
    "\n",
    "            # Append the model name, correlation, and RMSE to the list\n",
    "            metrics_list.append({'Model': model_name, 'R corr': r_corr, 'RMSE': rmse})\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    all_metrics = pd.DataFrame(metrics_list)\n",
    "\n",
    "    # Save the merged metrics to a new CSV file\n",
    "    output_file = os.path.join(directory, 'merged_metric.csv')\n",
    "    all_metrics.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Merged metrics saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def plot_predictions(directory):\n",
    "    # Loop through the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('predictions.csv'):\n",
    "            # Read each predictions CSV file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract true and predicted values\n",
    "            true_values = df['True Label'].values\n",
    "            predicted_values = df['Predicted Label'].values\n",
    "\n",
    "            # Calculate RMSE and Pearson correlation coefficient (R-value)\n",
    "            rmse = mean_squared_error(true_values, predicted_values, squared=False)\n",
    "            r_corr, _ = pearsonr(true_values, predicted_values)\n",
    "\n",
    "            # Extract model name from the filename\n",
    "            model_name = filename.replace('_predictions.csv', '')\n",
    "\n",
    "            # Create the plot\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(true_values, predicted_values, label=f'R = {r_corr:.4f}\\nRMSE = {rmse:.4f}', alpha=0.6)\n",
    "            plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], color='red', linestyle='--')  # Diagonal line\n",
    "\n",
    "            # Set title and labels\n",
    "            plt.title(f'{model_name}')\n",
    "            plt.xlabel('True')\n",
    "            plt.ylabel('Predicted')\n",
    "            \n",
    "            # Show the legend with R and RMSE\n",
    "            plt.legend(loc='lower right')\n",
    "\n",
    "            # Show the plot\n",
    "            plt.grid(True)            \n",
    "            # Save the plot as a PNG file\n",
    "            plot_filename = f\"{model_name}_plot.png\"\n",
    "            plot_path = os.path.join(directory, plot_filename)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "def combine_plots_to_pdf(directory):\n",
    "    \"\"\"\n",
    "    Searches the specified directory for PNG files and combines them into a single PDF file.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the PNG plot files.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store paths of PNG files\n",
    "    image_paths = []\n",
    "\n",
    "    # Search for PNG files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png'):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    # Combine the images into a single PDF\n",
    "    if image_paths:\n",
    "        images = []\n",
    "        for image_path in image_paths:\n",
    "            img = Image.open(image_path)\n",
    "\n",
    "            # Convert image to 'RGB' mode if not already\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            images.append(img)\n",
    "\n",
    "        # Save the first image and append the rest into a PDF\n",
    "        pdf_path = os.path.join(directory, 'all_plots.pdf')\n",
    "        images[0].save(pdf_path, save_all=True, append_images=images[1:])\n",
    "        print(f\"All plots combined and saved to {pdf_path}\")\n",
    "    else:\n",
    "        print(\"No PNG plot files found.\")\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == '__main__':\n",
    "    directory_path = '../Results' \n",
    "\n",
    "    # Merge the metrics and save to CSV\n",
    "    merge_metrics(directory_path)\n",
    "\n",
    "    # Plot all prediction files and save them as PNG\n",
    "    plot_predictions(directory_path)\n",
    "\n",
    "    # Combine all the PNG plots into a single PDF\n",
    "    combine_plots_to_pdf(directory_path)"
   ],
   "id": "e3a2b4e698dcafa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged metrics saved to ../Results\\merged_metric.csv\n",
      "Plot saved to ../Results\\CNN_2d_plot.png\n",
      "Plot saved to ../Results\\CNN_plot.png\n",
      "Plot saved to ../Results\\DeepNeuralNetMLP_plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../Results\\GCN_plot.png\n",
      "Plot saved to ../Results\\Linear_Regression_plot.png\n",
      "Plot saved to ../Results\\NeuralNetwork_MLP_plot.png\n",
      "Plot saved to ../Results\\Random Forest_plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shark\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../Results\\SVR_plot.png\n",
      "All plots combined and saved to ../Results\\all_plots.pdf\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c997d1ffab5fce4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
