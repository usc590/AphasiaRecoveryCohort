{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T00:34:25.066470Z",
     "start_time": "2024-10-15T00:34:25.057562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Get a count of participants of file ending number\n",
    "def calc_num_files_ending_with(folder_path: str, ending: int) -> list[int]:\n",
    "    count: int = 0\n",
    "    count_list: list[int] = []\n",
    "    # Loop through folder to find csv file\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Files look like this 'M10011_rest_jhu.csv', extract the number\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) > 0:\n",
    "                # print(parts[0])\n",
    "                num_str = ''.join(filter(str.isdigit, parts[0]))\n",
    "                # print(num_str)\n",
    "                if (len(num_str) == 5 and num_str.endswith(str(ending))):\n",
    "                    # count_list.append(int(num_str[:-1]))\n",
    "                    count_list.append(int(num_str))\n",
    "                    count += 1\n",
    " \n",
    "    print(count)                \n",
    "    return count_list\n",
    "\n",
    "# Save a list to csv\n",
    "def save_list_to_csv(input_list, output_folder: str, filename: str) -> None:\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    with open(f\"{output_folder}/{filename}\", \"w\", newline='\\n') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Participant ID'])\n",
    "        for participant in input_list:\n",
    "            writer.writerow([participant])\n",
    "    return\n",
    "        \n",
    "# Check if two csv is identical\n",
    "def is_csv_identical(csv1: str, csv2: str) -> bool:\n",
    "    # Open both csv\n",
    "    with open(csv1, 'r') as f1, open(csv2, 'r') as f2:\n",
    "        reader1 = csv.reader(f1)\n",
    "        reader2 = csv.reader(f2)\n",
    "        \n",
    "        for row1, row2 in zip(reader1, reader2):\n",
    "            if row1 != row2:\n",
    "                return False\n",
    "        # Check if there are more lines\n",
    "        try:\n",
    "            next(reader1)\n",
    "            return False  \n",
    "        except StopIteration:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            next(reader2)\n",
    "            return False  \n",
    "        except StopIteration:\n",
    "            pass  \n",
    "    print('True')\n",
    "    return True\n",
    "\n",
    "# Produce the connectome matrix\n",
    "def produce_connectome_csv(participant_csv: str, input_folder: str, output_folder: str, filename: str) -> None:\n",
    "    # The brain regions responsible for language and speech in the JHU atlas. Started with 1 index\n",
    "    speech_regions = [1, 2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 25, 26, 27, 28, 29, 30, 31,\n",
    "                    32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 49, 50, 69, 70, 71, 72, 184,\n",
    "                    185, 186, 187]\n",
    "    # There are 40 brain regions. Each region is connected to another region.\n",
    "    # So that gives 40 choose 2 = 780 features\n",
    "    # Calculate the combinations of the brain regions\n",
    "    region_combination = list(combinations(speech_regions, 2))\n",
    "    # Convert combination tuples to column names\n",
    "    column_names = [f\"{x}_{y}\" for x, y in region_combination]\n",
    "    # Initialize the table to store all the rest state fmri features\n",
    "    result_df = pd.DataFrame(columns = column_names)\n",
    "    # Use the participant_csv to open the mri data\n",
    "    participant_df = pd.read_csv(participant_csv, index_col=False)\n",
    "    for index, row in participant_df.iterrows():\n",
    "       # Files look like this 'M10011_rest_jhu.csv', recreate it\n",
    "        participant_id = row['Participant ID']\n",
    "        mri_pattern = f\"{input_folder}/M{participant_id}*.csv\"\n",
    "        matching_files = glob.glob(mri_pattern)\n",
    "        mri_csv = matching_files[0]\n",
    "        mri_df = pd.read_csv(mri_csv, index_col=False, header=None)\n",
    "       \n",
    "        # Create a numpy array to hold 780 features \n",
    "        features_array = []\n",
    "         \n",
    "        # Loop through all the brain region combination\n",
    "        for row_ , col in region_combination:\n",
    "            # Append the value to array. Make it 0 indexed.\n",
    "            features_array.append(mri_df.iloc[row_ - 1, col - 1])\n",
    "        # Convert the array to dataframe\n",
    "        features_array_df = pd.DataFrame([features_array], columns=result_df.columns, index=[participant_id])\n",
    "        result_df = pd.concat([result_df,features_array_df])\n",
    "    result_df.to_csv(f\"{output_folder}/{filename}.csv\")\n",
    "\n",
    "# Produce the prediction target csv\n",
    "def produce_polar_csv(participant_csv: str, polar_csv: str, output_folder: str, filename: str) -> None:\n",
    "    participant_df = pd.read_csv(participant_csv, index_col=False)\n",
    "    polar_df = pd.read_csv(polar_csv, index_col='POLAR ID Number')\n",
    "    \n",
    "    print(participant_df.shape)\n",
    "    result_df = pd.DataFrame()\n",
    "    column_names = [\n",
    "        \"information content\",\n",
    "        \"fluency rating\",\n",
    "        \"spontaneous speech rating\",\n",
    "        \"comprehension yes/no questions\",\n",
    "        \"comprehension auditory words\",\n",
    "        \"comprehension sequential commands\",\n",
    "        \"comprehension subscore\",\n",
    "        \"repetition subscore\",\n",
    "        \"object naming\",\n",
    "        \"word fluency\",\n",
    "        \"sentence completion\",\n",
    "        \"responsive speech\",\n",
    "        \"naming subscore\",\n",
    "        \"Aphasia quotient\",\n",
    "        \"Aphasia Type from WAB\"\n",
    "    ]\n",
    "    df_list = []\n",
    "    for index, row in participant_df.iterrows():\n",
    "        participant_id = int(str(row['Participant ID'])[:-1])\n",
    "        # df_list.append(polar_df.loc[[participant_id], column_names])\n",
    "        result_df = pd.concat([result_df, polar_df.loc[[participant_id], column_names]])\n",
    "    result_df.to_csv(f\"{output_folder}/{filename}.csv\")\n",
    "    print(result_df.shape)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T23:00:27.609977Z",
     "start_time": "2024-10-14T23:00:25.023709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# produce_connectome_csv(r\"..\\Processed Data\\rest_participants_first.csv\",\n",
    "#                        r\"..\\ConnectivityMatrix\\rest_jhu\",\n",
    "#                        r\"..\\Processed Data\",\n",
    "#                        \"rest_101_participants_40_regions\")\n",
    "# produce_connectome_csv(r\"..\\Processed Data\\dti_participants_first.csv\",\n",
    "#                        r\"..\\ConnectivityMatrix\\dti_jhu\",\n",
    "#                        r\"..\\Processed Data\",\n",
    "#                        \"dti_101_participants_40_regions\")\n",
    "# produce_connectome_csv(r\"..\\Processed Data\\rest_participants_fourth.csv\",\n",
    "#                        r\"..\\ConnectivityMatrix\\rest_jhu\",\n",
    "#                        r\"..\\Processed Data\",\n",
    "#                        \"rest_61_participants_40_regions_fourth_vist\")\n",
    "# produce_connectome_csv(r\"..\\Processed Data\\dti_participants_fourth.csv\",\n",
    "#                        r\"..\\ConnectivityMatrix\\dti_jhu\",\n",
    "#                        r\"..\\Processed Data\",\n",
    "#                        \"dti_61_participants_40_regions_fourth_vist\")\n",
    "produce_connectome_csv(r\"..\\Processed Data\\rest_participants_61_first_visit.csv\",\n",
    "                       r\"..\\ConnectivityMatrix\\rest_jhu\",\n",
    "                       r\"..\\Processed Data\",\n",
    "                       \"rest_61_participants_40_regions_first_vist\")\n",
    "produce_connectome_csv(r\"..\\Processed Data\\dti_participants_61_first_visit.csv\",\n",
    "                       r\"..\\ConnectivityMatrix\\dti_jhu\",\n",
    "                       r\"..\\Processed Data\",\n",
    "                       \"dti_61_participants_40_regions_first_vist\")"
   ],
   "id": "e85c35cfcf55fbeb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shark\\AppData\\Local\\Temp\\ipykernel_18940\\1566471448.py:94: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df,features_array_df])\n",
      "C:\\Users\\shark\\AppData\\Local\\Temp\\ipykernel_18940\\1566471448.py:94: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df,features_array_df])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T21:12:51.611444Z",
     "start_time": "2024-10-14T21:12:51.598713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "participant_list = calc_num_files_ending_with(r\"..\\ConnectivityMatrix\\rest_jhu\", 1)\n",
    "save_list_to_csv(participant_list, r\"..\\Processed Data\", 'rest_participants_first.csv') \n",
    "participant_list = calc_num_files_ending_with(r\"..\\ConnectivityMatrix\\rest_jhu\", 4)\n",
    "save_list_to_csv(participant_list, r\"..\\Processed Data\", 'rest_participants_fourth.csv')\n",
    "participant_list = calc_num_files_ending_with(r\"..\\ConnectivityMatrix\\dti_jhu\", 1)\n",
    "save_list_to_csv(participant_list, r\"..\\Processed Data\", 'dti_participants_first.csv') \n",
    "participant_list = calc_num_files_ending_with(r\"..\\ConnectivityMatrix\\dti_jhu\", 4)\n",
    "save_list_to_csv(participant_list, r\"..\\Processed Data\", 'dti_participants_fourth.csv') "
   ],
   "id": "beac1284abe05962",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "61\n",
      "101\n",
      "61\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T21:28:26.171588Z",
     "start_time": "2024-10-14T21:28:26.162598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_csv_identical(r\"..\\Processed Data\\rest_participants_first.csv\", \n",
    "                 r\"..\\Processed Data\\dti_participants_first.csv\")\n",
    "is_csv_identical(r\"..\\Processed Data\\rest_participants_fourth.csv\", \n",
    "                 r\"..\\Processed Data\\dti_participants_fourth.csv\")"
   ],
   "id": "5d28f4dcc0d15868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 1)\n",
      "(103, 15)\n"
     ]
    }
   ],
   "execution_count": 67,
   "source": [
    "produce_polar_csv(r\"..\\Processed Data\\rest_participants_first.csv\",\n",
    "                r\"..\\ConnectivityMatrix\\POLAR_measures.csv\",\n",
    "                r\"..\\Processed Data\",\n",
    "                '101_participants_40_regions_target_variables')"
   ],
   "id": "48f022c326d660df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c93917d11ea4fc46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
